\input{common/header.tex}
\inbpdocument

\chapter{Gaussian Conditionals}
\label{ch:appendix-gaussians}

%\section{Formula for Gaussian Conditionals}

A standard result shows how to condition on knowing a subset of the dimensions $\vy_B$ of a vector $\vy$ having a multivariate Gaussian distribution.
If
%
\begin{align}
\vy = \colvec{\vy_A}{\vy_B} \sim \Nt{\colvec{\vmu_A}{\vmu_B}}{\left[ \begin{array}{cc}\vSigma_{AA} & \vSigma_{AB} \\ \vSigma_{BA} & \vSigma_{BB} \end{array} \right]}
%\left[ \begin{array} \vx_A \sim \Nt{\vmu_A}{
%| \vx_B \sim \Nt{\vmu_A + \vSigma_{AB} \vSigma_{BB}\inv \left( \vx_B - \vmu_B \right) }
%{\vSigma_{AA} - \vSigma_{AB} \vSigma_{BB}\inv \vSigma_{BA} }
%\label{eq:gauss_conditional}
\end{align}
%
then
%
\begin{align}
\vy_A | \vy_B \sim \Nt{\vmu_A + \vSigma_{AB} \vSigma_{BB}\inv \left( \vx_B - \vmu_B \right) }
{\vSigma_{AA} - \vSigma_{AB} \vSigma_{BB}\inv \vSigma_{BA} }.
\label{eq:gauss_conditional}
\end{align}

This result can be used in the context of Gaussian process regression, where $\vy_B = [f(\vx_1), f(\vx_2), \dots, f(\vx_N)]$ represents a set of function values observed at some subset of locations $[\vx_1, \vx_2, \dots, \vx_N]$, while $\vy_A = [f(\vx_1\star), f(\vx_2\star), \ldots, f(\vx_N\star)]$ represents test points whose predictive distribution we'd like to know.
In this case, the necessary covariance matrices are given by:
%
\begin{align}
\vSigma_{AA} & = k(\vX, \vX) \\
\vSigma_{AB} & = k(\vX, \vX^\star) \\
\vSigma_{BA} & = k(\vX^\star, \vX) \\
\vSigma_{BB} & = k(\vX^\star, \vX^\star)
\end{align}
and similarily for the mean vectors.






\chapter{Kernel Definitions}
\label{ch:appendix-kernels}
\label{sec:kernel-definitions}

%\newcommand{\scalefactor}{\sigma_f^2}
\newcommand{\scalefactor}{}

%\subsection{Base kernels}

%For scalar-valued inputs, the white noise ($\kWN$), constant ($\kC$), linear ($\kLin$), squared exponential ($\kSE$), and periodic kernels ($\kPer$) are defined as follows:
Here we give the formulas for all one-dimensional base kernels mentioned in the thesis.
Each of these formulas is multiplied by a scale factor $\sigma_f^2$, which we omit for clarity.
\begin{align}
\kC(\inputVar, \inputVar') & = \scalefactor 1 \\
\kWN(\inputVar, \inputVar') & = \scalefactor \delta(\inputVar - \inputVar') \label{eq:appendix-wn}\\
\kLin(\inputVar, \inputVar') & = \scalefactor (\inputVar - c)(\inputVar' - c)  \label{eq:appendix-lin} \\
\kSE(\inputVar, \inputVar') & = \scalefactor \exp\left(-\frac{(\inputVar - \inputVar')^2}{2\ell^2}\right)  \label{eq:appendix-se}\\
\kRQ(x, x') & = \scalefactor \left( 1 + \frac{(\inputVar - \inputVar')^2}{2\alpha\ell^2}\right)^{-\alpha}  \label{eq:appendix-rq}\\
\kPer(\inputVar, \inputVar') & =  \sigma_f^2 \frac{\exp\left(\frac{1}{\ell^2}\cos 2 \pi  \frac{(\inputVar - \inputVar')}{p}\right) - I_0\left(\frac{1}{\ell^2}\right)}{\exp\left(\frac{1}{\ell^2}\right) - I_0\left(\frac{1}{\ell^2}\right)} \label{eq:generalized-periodic} \\
%\kPer(\inputVar, \inputVar') & =  \scalefactor \exp\left(\frac{1}{\ell^2}\cos 2 \pi  \frac{(\inputVar - \inputVar')}{p}\right) - I_0\left(\frac{1}{\ell^2}\right) \\
\cos(x, x') & = \scalefactor \cos\left(\frac{2 \pi (x - x')}{p}\right) \label{eq:appendix-cos} \\
\kCP(\kernel_1, \kernel_2)(x, x') & = \scalefactor \sigma(x) k_1(x,x')\sigma(x') + (1-\sigma(x)) k_2(x,x')(1-\sigma(x')) \\
\boldsymbol\sigma & = \scalefactor \sigma(x)\sigma(x') \\
\boldsymbol{\bar\sigma} & = \scalefactor (1-\sigma(x))(1-\sigma(x'))
\end{align}
where $\delta_{\inputVar, \inputVar'}$ is the Kronecker delta function, $I_0$ is the modified Bessel function of the first kind of order zero, and other symbols are kernel parameters.
\Cref{eq:appendix-se,eq:generalized-periodic,eq:appendix-lin} are plotted in \cref{fig:basic_kernels}, and \cref{eq:appendix-wn,eq:appendix-rq,eq:appendix-cos} are plotted in \cref{fig:basic_kernels_two}.
Draws from \gp{} priors with changepoint kernels are shown in \cref{fig:changepoint_examples}.



\subsubsection{The Generalized Periodic Kernel}

\citet{lloyd-periodic} showed that the standard periodic kernel due to \citet{mackay1998introduction} can be decomposed into a periodic and a constant component.
He derived the equivalent periodic kernel without any constant component, shown in \cref{eq:generalized-periodic}.
He further showed that its limit as the lengthscale grows is the cosine kernel:
\begin{equation}
\lim_{\ell \to \infty} \kPer(x, x') = \cos\left(\frac{2 \pi (x - x')}{p}\right).
\end{equation}

Separating out the constant component allows us to express negative prior covariance, as well as increasing the interpretability of the resulting models.






\chapter{Search Operators}
\label{ch:appendix-search}
\label{sec:search-operators}

%\subsection{Overview}

The model construction phase of \procedurename{} starts with the noise kernel, $\kWN$.
New kernel expressions are generated by applying search operators to the current kernel, which replace some part of the existing kernel expression with a new kernel expression.
%When new base kernels are proposed by the search operators, their parameters are randomly initialised with several restarts.
%Parameters are then optimized by conjugate gradients to maximise the likelihood of the data conditioned on the kernel parameters.
%The kernels are then scored by the Bayesian information criterion and the top scoring kernel is selected as the new kernel.
%The search then proceeds by applying the search operators to the new kernel \ie this is a greedy search algorithm.

%In all experiments, 10 random restarts were used for parameter initialisation and the search was run to a depth of 10.

%\subsection{Search operators}

The search used in the multidimensional regression experiments in \cref{sec:synthetic,sec:additive-experiments} used only the following search operators:
%
\begin{eqnarray}
\mathcal{S} &\to& \mathcal{S} + \mathcal{B} \\
\mathcal{S} &\to& \mathcal{S} \times \mathcal{B} \label{eq:search-multiply}\\
\mathcal{B} &\to& \mathcal{B'}
\end{eqnarray}
%
where $\mathcal{S}$ represents any kernel subexpression and $\mathcal{B}$ is any base kernel within a kernel expression.
These search operators represent addition, multiplication and replacement.
When the multiplication operator is applied to a subexpression which includes a sum of subexpressions, parentheses () are introduced.
For instance, if rule \eqref{eq:search-multiply} is applied to the subexpression $k_1 + k_2$, the resulting expression is $(k_1 \kernplus k_2) \kerntimes \mathcal{B}$.

Afterwards, we added several more search operators in order to speed up the search.
These new operators do not change the set of possible models.

To accommodate changepoints and changewindows, we introduced the following additional operators to our search:
%
\begin{eqnarray}
\mathcal{S} &\to& \kCP(\mathcal{S},\mathcal{S}) \\
\mathcal{S} &\to& \kCW(\mathcal{S},\mathcal{S}) \\
\mathcal{S} &\to& \kCW(\mathcal{S},\kC) \\
\mathcal{S} &\to& \kCW(\kC,\mathcal{S})
\end{eqnarray}
%
where $\kC$ is the constant kernel.
The last two operators result in a kernel only applying outside, or within, a certain region.

To allow the search to simplify existing expressions, we introduced the following operators:
%
\begin{eqnarray}
\mathcal{S} &\to& \mathcal{B}\\
\mathcal{S} + \mathcal{S'} &\to& \mathcal{S}\\
\mathcal{S} \times \mathcal{S'} &\to& \mathcal{S}
\end{eqnarray}
%
where $\mathcal{S'}$ represents any other kernel expression.
%Their introduction is currently not rigorously justified.
We also introduced the operator
%
\begin{eqnarray}
\mathcal{S} &\to& \mathcal{S} \times (\mathcal{B} + \kC)
\end{eqnarray}
%
Which allows a new base kernel to be added along with the constant kernel, for cases when multiplying by a base kernel by itself would restrict the model too much.





\chapter{Example Automatically-Generated Report}
\label{ch:example-solar}

The following pages of this appendix contain an entire automatically-generated report, run on a dataset measuring annual solar irradiation data from 1610 to 2011.
This dataset was previously analyzed by \citet{lean1995reconstruction}.

The structure search was run using the \procedurename-interpretable variant, with base kernels $\kSE$, $\kLin$, $\kC$, $\kPer$, $\vsigma$, and $\kWN$.

Other example reports can be found at \url{mlg.eng.cam.ac.uk/Lloyd/abcdoutput/}, including anlyses of wheat prices, temperature records, call centre volumes, radio interference, gas production, unemployment, number of births, and wages over time.

\newcommand{\solarreportpage}[1]{\includegraphics[width=\columnwidth]{figures/solarpages/02-solar-seperate-pages-#1}}

\clearpage

%\solarreportpage{1}

\solarreportpage{2}
\\ \vspace{1cm} \\
\solarreportpage{3}
\\ \vspace{1cm} \\
\solarreportpage{4}
\\ \vspace{1cm} \\
\solarreportpage{5}
\\ \vspace{1cm} \\
\solarreportpage{6}
\\ \vspace{1cm} \\
\solarreportpage{7}
\\ \vspace{1cm} \\
\solarreportpage{8}
\\ \vspace{1cm} \\
\solarreportpage{9}
\\ \vspace{1cm} \\
\solarreportpage{10}
\\ \vspace{1cm} \\
\solarreportpage{11}
\\ \vspace{1cm} \\
\solarreportpage{12}
\\ \vspace{1cm} \\
\solarreportpage{13}

%\solarreportpage{14}

%\solarreportpage{15}

%\solarreportpage{16}

%\solarreportpage{17}

%\solarreportpage{18}

%\solarreportpage{19}

%\solarreportpage{20}

%\solarreportpage{21}

%\solarreportpage{22}


\chapter{Inference in the Warped Mixture Model}
\label{ch:warped-appendix}

\subsubsection{Detailed Definition of Model}

To ensure a conjugate likelihood, we place Gaussian-Wishart priors on the Gaussian parameters
$\{\bm{\mu}_{c},\vR_{c}\}$,
\begin{align}
p(\bm{\mu}_{c},\vR_{c})
= {\cal N}(\bm{\mu}_{c}|\vu,(r\vR_{c})\inv)
{\cal W}(\vR_{c}|\vS\inv,\nu),
\end{align}
%
where $\vu$ is the mean of $\vmu_c$, $r$ is the relative precision of $\vmu_{c}$, $\vS\inv$ is the scale matrix for $\vR_c$, and $\nu$ is the number of degrees of freedom for $\vR_c$.

The Wishart distribution is defined as follows:

\begin{align}
{\cal W}(\vR|\vS\inv,\nu)=\frac{1}{G}|\vR|^{\frac{\nu-Q-1}{2}}\exp\left(-\frac{1}{2}{\rm tr}(\vS\vR)\right),
\end{align}

where $G$ is the normalizing constant.
Because we use conjugate Gaussian-Wishart priors for the parameters of the Gaussian mixture components, we can analytically integrate out those parameters, given the assignments of points to components.
Let $z_{n}$ be the latent assignment of the $n^{\text{\tiny th}}$ point.
The probability of latent coordinates $\vX$ given latent assignments $\vZ=(z_1, z_2, \dots, z_N)$ is obtained by integrating out the Gaussian parameters $\{\vmu_c,\vR_c\}$ as follows:
%
\begin{align}
p(\vX|\vZ,\vS,\nu,r) &= \prod_{c=1}^{\infty}
\pi^{-\frac{N_{c}Q}{2}}\frac{r^{Q/2}|\vS|^{\nu/2}}{r_{c}^{Q/2}|\vS_{c}|^{\nu_{c}/2}}
\times \prod_{q=1}^{Q}\frac{\Gamma(\frac{\nu_{c}+1-q}{2})}{\Gamma(\frac{\nu+1-q}{2})},
\label{eq:px_z}
\end{align}
%
where
$N_c$ is the number of data points assigned to the $c^{\text{\tiny th}}$ component,
$\Gamma(\cdot)$ is the Gamma function, and
%
\begin{align}
r_{c}=r+N_{c}, \hspace{2em}
\nu_{c}=\nu+N_{c}, 
\quad
%\nonumber
%\end{align}
%\begin{align}
\vu_{c}=\frac{r\vu+\sum_{n:z_{n}=c}\vx_{n}}{r+N_{c}}, 
\nonumber
\end{align}
%
\begin{align}
\vS_{c}=\vS+\sum_{n:z_{n}=c}\vx_{n}\vx_{n}\tra + r\vu\vu\tra
 - r_{c}\vu_{c}\vu_{c}\tra,
\end{align}
%
are the posterior Gaussian-Wishart parameters of the $c^{\text{\tiny th}}$ component.
We use a Dirichlet process with concentration parameter $\eta$ for infinite mixture modeling~\citep{maceachern1998estimating} in the latent space.
Then, the probability of $\vZ$ is given as follows:
%
\begin{align}
p(\vZ|\eta) = 
\frac{\eta^{C}\prod_{c=1}^{C}(N_{c}-1)!}
{\eta(\eta+1)\cdots(\eta+N-1)},
\label{eq:pz}
\end{align}
%
where $C$ is the number of components for which $N_{c}>0$.
The joint distribution is given by
%
\begin{align}
p(\vY,\vX,\vZ|\bm{\theta},\bm{S},\nu,\vu,r,\eta)
 = p(\vY|\vX,\bm{\theta})
 p(\vX|\vZ,\bm{S},\nu,\vu,r)p(\vZ|\eta),
\label{eq:joint}
\end{align}
%
where factors in the right hand side can be calculated by (\ref{eq:py_x}), (\ref{eq:px_z}) and (\ref{eq:pz}), respectively.

\subsubsection{Generative Model}

In summary, the infinite warped mixture model generates observations $\vY$ according to the following generative process:
%
\begin{enumerate}
\item Draw mixture weights $\bm{\lambda} \sim \GEM(\eta)$
\item For each component $c=1, 2, \dots, \infty$
\begin{enumerate}
\item Draw precision $\vR_c \sim {\mathcal W}(\vS\inv, \nu)$
\item Draw mean $\vmu_c \sim {\mathcal N}(\vu,(r\vR_c)\inv)$
\end{enumerate}
\item For each observed dimension $d=1, 2, \dots, D$
\begin{enumerate}
\item Draw function $f_{d}(\vx) \sim {\rm GP}(m(\vx),k(\vx,\vx'))$
\end{enumerate}
\item For each observation $n=1, 2, \dots,N$
\begin{enumerate}
\item Draw latent assignment $z_n \sim {\rm Mult}(\bm{\lambda})$
\item Draw latent coordinates $\vx_n \sim {\mathcal N}(\vmu_{z_n},\vR_{z_n}\inv)$
\item For each observed dimension $d=1, 2, \dots, D$
\begin{enumerate}
\item Draw feature $y_{nd} \sim {\cal N}(f_d(\vx_n), \beta\inv)$
\end{enumerate}
\end{enumerate}
\end{enumerate}
%
Here, $\GEM(\eta)$ is the stick-breaking process \citep{sethuraman94} that generates mixture weights for a Dirichlet process with parameter $\eta$, %${\rm Mult}(\cdot)$ represents a multinomial distribution,
${\rm Mult}(\bm{\lambda})$ represents a multinomial distribution with parameter $\bm{\lambda}$,
$m(\vx)$ is the mean function of the Gaussian process, and $\vx,\vx'\in{\mathbb R}^{Q}$.



\subsubsection{Details of Inference}
\label{sec:iwmm-inference-details}

First, by placing conjugate Gaussian-Wishart priors on the parameters of the Gaussian mixture components, we can analytically integrate out those parameters given the assignments of points to clusters.
The only remaining variables to infer will be the latent points $\vX$, the cluster assignments $Z$, and the kernel parameters $\vtheta$.

We can then infer the posterior distribution of the latent coordinates $\vX$ and cluster assignments $\vZ$ using Markov chain Monte Carlo (\mcmc{}).
In particular, we'll alternate collapsed Gibbs sampling of $\vZ$ and Hamiltonian Monte Carlo sampling of $\vX$.
Given $\vX$,
%the proposed model is simply a standard infinite mixture model with conjugate priors. Therefore, 
we can efficiently sample $\vZ$ using collapsed Gibbs sampling, integrating out the mixture parameters.
Given $\vZ$, we can calculate the gradient of the un-normalized posterior distribution of $\vX$, integrating over warping functions.
This gradient allows us to sample $\vX$ using Hamiltonian Monte Carlo.

First, we explain collapsed Gibbs sampling for $\vZ$.
Given a sample of $\vX$, $p(\vZ | \vX, \vS, \nu, \vu, r, \eta)$ does not depend on $\vY$.
This lets us resample cluster assignments, integrating out the \iGMM{} likelihood in closed form.
Given the current state of all but one latent component $z_n$, a new value for $z_n$ is sampled with the following probability:
%
\begin{align}
p(z_{n}=c|\vX,\vZ_{\setminus n},\bm{S},\nu,\vu,r,\eta)
 & \propto\!
\left\{
\begin{array}{ll}
\!\!N_{c\setminus n}\cdot p(\vx_{n}|\vX_{c\setminus n},\bm{S},\nu,\vu,r) & \text{{\small existing components}}\\
\!\!\eta\cdot p(\vx_{n}|\bm{S},\nu,\vu,r) & \text{{\small a new component}}
\end{array}
\right.
\label{eq:gibbs}
\end{align}
%
where $\vX_{c}=\{\vx_{n}|z_{n}=c\}$ 
is the set of latent coordinates assigned to the $c^{\text{\tiny th}}$ component,
and $\setminus n$ represents the value or set
when excluding the $n^{\text{\tiny th}}$ data point.
We can analytically calculate $p(\vx_{n}|\vX_{c\setminus n},\bm{S},\nu,\vu,r)$
as follows:
%
\begin{align}
p(\vx_{n}|\vX_{c\setminus n},\bm{S},\nu,\vu,r)
 & =\pi^{-\frac{N_{c\setminus n}Q}{2}}\frac{r_{c\setminus n}^{Q/2}|\vS_{c\setminus n}|^{\nu_{c\setminus n}/2}}{r_{c\setminus n}'^{Q/2}|\vS_{c\setminus n}'|^{\nu_{c\setminus n}'/2}}\prod_{d=1}^{Q}\frac{\Gamma(\frac{\nu_{c\setminus n}'+1-d}{2})}{\Gamma(\frac{\nu_{c\setminus n}+1-d}{2})},
\end{align}
%
where $r_{c}'$, $\nu_{c}'$, $\vu_{c}'$ and $\vS_{c}'$ represent the posterior Gaussian-Wishart parameters of the $c^{\text{\tiny th}}$ component when the $n^{\text{\tiny th}}$ data point is assigned to the $c^{\text{\tiny th}}$ component.
We can efficiently calculate the determinant by using the rank one Cholesky update.
In the same way, we can analytically calculate the likelihood for a new component $p(\vx_{n}|\bm{S},\nu,\vu,r)$.

\subsubsection{Gradients for Hamiltonian Monte Carlo}

Hamiltonian Monte Carlo (\HMC{}) sampling of $\vX$ from posterior ${p(\vX|\vZ,\vY,\bm{\theta},\bm{S},\nu,\vu,r)}$, requires computing the gradient of the log of the unnormalized posterior
%
\begin{align}
\log p(\vY|\vX,\bm{\theta}) + \log p(\vX|\vZ,\bm{S},\nu,\vu,r)
\end{align}
%
The first term of the gradient can be calculated by
%
\begin{align}
\pderiv{\log p(\vY | \vX,\bm{\theta})}{\vX} =  \pderiv{\log p(\vY | \vX,\bm{\theta})}{\vK} \pderiv{\vK}{\vX} = \left[ -\frac{1}{2}D\vK\inv+\frac{1}{2}\vK\inv \vY \vY^{T} \vK\inv \right]\left[\pderiv{\vK}{\vX}\right], 
\end{align}
%
where for an $\kSE + \kWN$ kernel with the same lengthscale $\ell$ on all dimensions,
%
\begin{align}
\pderiv{k(\vx_{n},\vx_{m})}{\vx_n}
 & = -\frac{\sigma^2_f}{\ell^2} \exp \left( - \frac{1}{2 \ell^2} (\vx_n - \vx_m)\tra (\vx_n - \vx_m) \right) (\vx_n - \vx_m).
\end{align}
%
%using the chain rule.
The second term can be calculated as follows:
\begin{align}
\frac{\partial \log p(\vX|\vZ,\bm{S},\nu,\vu,r)}{\partial \vx_{n}} 
= -\nu_{z_{n}}\bm{S}_{z_{n}}\inv(\vx_{n}-\vu_{z_{n}}).
\end{align}
We also infer kernel parameters $\vtheta$ via \HMC{}, using the gradient of the log unnormalized posterior with respect to the kernel parameters, and an improper uniform prior.




\subsubsection{Posterior Predictive Density}
\label{sec:iwmm-predictive-density}

In the \gplvm{}, the predictive density of at test point $\vy_\star$ is usually computed by finding the point $\vx_\star$ which which is most likely to be mapped to $\vy_\star$, then using the density of $p(\vx_\star)$ and the Jacobian of the warping at that point to approximately compute the density at $\vy_\star$.
When inference is done by simply optimizing the location of the latent points, this estimation method simply requires solving a single optimization for each $\vy_\star$.  

For our model, we use approximate integration to estimate $p(\vy_\star)$.
This is done for two reasons:
First, multiple latent points (possibly from different clusters) can map to the same observed point, meaning the standard method can underestimate $p(\vy_\star)$.
Second, because we do not optimize the latent coordinates but rather sample them, we would need to perform optimizations for each $p(\vy_\star)$ seperately for each sample.
Our method gives estimates for all $p(\vy_\star)$ at once, but may not be accurate in very high dimensions.

The posterior density in the observed space given the training data is simply:
\begin{align}
p(\vy_\star | \vY)
& = \int \!\!\! \int p(\vy_\star,\vx_\star, \vX | \vY)d\vx_\star d\vX \nonumber\\
& = \int\!\!\! \int p(\vy_\star | \vx_\star, \vX, \vY)p(\vx_\star|\vX,\vY)p(\vX|\vY)d\vx_\star d\vX.
\label{eq:density}
\end{align}
We approximate $p(\vX | \vY)$ using the samples from the Gibbs and Hamiltonian Monte Carlo samplers.
We approximate $p(\vx_\star | \vX, \vY)$ by sampling points from the latent mixture and warping them, using the following procedure:
\begin{enumerate}
\item Draw latent assignments
$z_\star \sim {\rm Mult}(\frac{N_{1}}{N+\eta},\cdots,\frac{N_{C}}{N+\eta},\frac{\eta}{N+\eta})$
\item Draw precision matrix
$\vR_\star \sim {\cal W}(\vS\inv_{z_\star},\nu_{z_\star})$
\item Draw mean
$\vmu_\star \sim {\cal N}(\vu_{z_\star},(r_{z_\star}\vR_\star)\inv)$
\item Draw latent coordinates
$\vx_\star \sim {\cal N}(\vmu_\star,\vR_\star\inv)$
\end{enumerate}
%
When a new component $C+1$ is assigned to $z_\star$, the prior Gaussian-Wishart distribution is used for sampling in steps 2 and 3.
The first factor of (\ref{eq:density}) can be calculated by
%
\begin{align}
p(\vy_\star | \vx_\star, \vX, \vY)
& = {\cal N}(\vk_\star\tra \vK\inv \vY, k(\vx_\star,\vx_\star)-\vk_\star\tra \vK\inv \vk_\star),
\end{align}
%
where
%
$\vk_\star=( k(\vx_\star, \vx_1), \cdots, k(\vx_\star, \vx_N))\tra$.
%  
Each step of this sampling procedure draws from the exact conditional distribution, so the Monte Carlo estimate of the predictive density $p(\vy_\star | \vX, \vY)$ will converge to the true marginal distribution.
Since the observations $\vy_\star$ are conditionally normally distributed, each one adds a smooth contribution to the empirical Monte Carlo estimate of the posterior density, as opposed to a collection of point masses.


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


\iffalse

\section{Guide to the automatically generated reports}

Additional supplementary material to this paper is 13 reports automatically generated by \procedurename{}.
A link to these reports will be maintained at \url{http://mlg.eng.cam.ac.uk/lloyd/}.
We recommend that you read the report for `01-airline' first and review the reports that follow afterwards more briefly.
`02-solar' is discussed in the main text.
`03-mauna' analyses a dataset mentioned in the related work.
`04-wheat' demonstrates changepoints being used to capture heteroscedasticity.
`05-temperature' extracts an exactly periodic pattern from noisy data.
`07-call-centre' demonstrates a large discontinuity being modeled by a changepoint.
`10-sulphuric' combines many changepoints to create a highly structured model of the data.
`12-births' discovers multiple periodic components.

\fi


\iffalse


\section{Comparison of Predictive Accuracy}
\label{sec:accuracy-appendix}


\subsection{Interpolation}
\label{sec:interpolation-appendix}

To test the ability of the methods to interpolate, we randomly divided each data set into equal amounts of training data and testing data.
We trained each algorithm on the training half of the data, produced predictions for the remaining half and then computed the root mean squared error (\RMSE{}).
The values of the \RMSE{}s are then standardised by dividing by the smallest RMSE for each data set \ie the best performance on each data set will have a value of 1.

\Cref{fig:box_interp} shows the standardised \RMSE{}s for the different algorithms.
The box plots show that all quartiles of the distribution of standardised RMSEs are lower for both versions of \procedurename{}.
The median for \procedurename{}-accuracy is 1; it is the best performing algorithm on 7 datasets.
The largest outliers of \procedurename{} and spectral kernels are similar in value.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{\grammarfiguresdir/comparison/box_interp}
\caption[Comparision of interpolation error of all methods on 13 time-series datasets.]
{Box plot of standardised \RMSE{} (best performance = 1) on 13 interpolation tasks.}
\label{fig:box_interp}
\end{figure*}

Changepoints performs slightly worse than \MKL{} despite being strictly more general than Changepoints.
The introduction of changepoints allows for more structured models, but it introduces parametric forms into the regression models (\ie the sigmoids expressing the changepoints).
This results in worse interpolations at the locations of the change points, suggesting that a more robust modeling language would require a more flexible class of changepoint shapes or improved inference (\eg fully Bayesian inference over the location and shape of the changepoint).

Eureqa is not suited to this task and performs poorly.
The models learned by Eureqa tend to capture only broad trends of the data since the fine details are not well explained by parametric forms.

\subsection{Tabels of standardised RMSEs}

See table~\ref{table:interp} for raw interpolation results and table~\ref{table:extrap} for raw extrapolation results. 
The rows follow the order of the datasets in the rest of the supplementary material.
The following abbreviations are used: \procedurename{}-accuracy (\procedurename{}-acc), \procedurename{}-interpretability ((\procedurename{}-int), Spectral kernels (SP), Trend-cyclical-irregular (TCI), Bayesian MKL (MKL), Eureqa (EL), Changepoints (CP), Squared exponential (SE) and Linear regression (Lin).

\begin{table*}[ht]
\center
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\procedurename{}-acc & \procedurename{}-int & SP & TCI & MKL & EL & CP & SE & Lin \\
\hline
1.04 & 1.00 & 2.09 & 1.32 & 3.20 & 5.30 & 3.25 & 4.87 & 5.01\\
1.00 & 1.27 & 1.09 & 1.50 & 1.50 & 3.22 & 1.75 & 2.75 & 3.26\\
1.00 & 1.00 & 1.09 & 1.00 & 2.69 & 26.20 & 2.69 & 7.93 & 10.74\\
1.09 & 1.04 & 1.00 & 1.00 & 1.00 & 1.59 & 1.37 & 1.33 & 1.55\\
1.00 & 1.06 & 1.08 & 1.06 & 1.01 & 1.49 & 1.01 & 1.07 & 1.58\\
1.50 & 1.00 & 2.19 & 1.37 & 2.09 & 7.88 & 2.23 & 6.19 & 7.36\\
1.55 & 1.50 & 1.02 & 1.00 & 1.00 & 2.40 & 1.52 & 1.22 & 6.28\\
1.00 & 1.30 & 1.26 & 1.24 & 1.49 & 2.43 & 1.49 & 2.30 & 3.20\\
1.00 & 1.09 & 1.08 & 1.06 & 1.30 & 2.84 & 1.29 & 2.81 & 3.79\\
1.08 & 1.00 & 1.15 & 1.19 & 1.23 & 42.56 & 1.38 & 1.45 & 2.70\\
1.13 & 1.00 & 1.42 & 1.05 & 2.44 & 3.29 & 2.96 & 2.97 & 3.40\\
1.00 & 1.15 & 1.76 & 1.20 & 1.79 & 1.93 & 1.79 & 1.81 & 1.87\\
1.00 & 1.10 & 1.03 & 1.03 & 1.03 & 2.24 & 1.02 & 1.77 & 9.97\\
\hline
\end{tabular}
\caption[Interpolation error]{Interpolation standardised RMSEs}
\label{table:interp}
\end{table*}

\begin{table*}[ht]
\center
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\procedurename{}-acc & \procedurename{}-int & SP & TCI & MKL & EL & CP & SE & Lin \\
\hline
1.14 & 2.10 & 1.00 & 1.44 & 4.73 & 3.24 & 4.80 & 32.21 & 4.94\\
1.00 & 1.26 & 1.21 & 1.03 & 1.00 & 2.64 & 1.03 & 1.61 & 1.07\\
1.40 & 1.00 & 1.32 & 1.29 & 1.74 & 2.54 & 1.74 & 1.85 & 3.19\\
1.07 & 1.18 & 3.00 & 3.00 & 3.00 & 1.31 & 1.00 & 3.03 & 1.02\\
1.00 & 1.00 & 1.03 & 1.00 & 1.35 & 1.28 & 1.35 & 2.72 & 1.51\\
1.00 & 2.03 & 3.38 & 2.14 & 4.09 & 6.26 & 4.17 & 4.13 & 4.93\\
2.98 & 1.00 & 11.04 & 1.80 & 1.80 & 493.30 & 3.54 & 22.63 & 28.76\\
3.10 & 1.88 & 1.00 & 2.31 & 3.13 & 1.41 & 3.13 & 8.46 & 4.31\\
1.00 & 2.05 & 1.61 & 1.52 & 2.90 & 2.73 & 3.14 & 2.85 & 2.64\\
1.00 & 1.45 & 1.43 & 1.80 & 1.61 & 1.97 & 2.25 & 1.08 & 3.52\\
2.16 & 2.03 & 3.57 & 2.23 & 1.71 & 2.23 & 1.66 & 1.89 & 1.00\\
1.06 & 1.00 & 1.54 & 1.56 & 1.85 & 1.93 & 1.84 & 1.66 & 1.96\\
3.03 & 4.00 & 3.63 & 3.12 & 3.16 & 1.00 & 5.83 & 5.35 & 4.25\\
\hline
\end{tabular}
\caption[Extrapolation error]{Extrapolation standardised RMSEs}
\label{table:extrap}
\end{table*}

\fi


\iffalse

\subsection{Comparison to Equation Learning}
\label{sec:eqn-learning-comp}

We now compare the descriptions generated by \procedurename{} to parametric functions produced by an equation learning system.
We show equations produced by Eureqa \citep{Eureqa} for the data sets shown above, using the default mean absolute error performance metric.

The learned function for the solar irradiance data is
\begin{align*}
\textrm{Irradiance($t$)} = 1361 + \alpha\sin(\beta + \gamma t)\sin(\delta + \epsilon t^2 - \zeta t)
\end{align*}
where $t$ is time and constants are replaced with symbols for brevity.
This equation captures the constant offset of the data, and models the long-term trend with a product of sinusoids, but fails to capture the solar cycle or the Maunder minimum.

The learned function for the airline passenger data is
\begin{align*}
\textrm{Passengers($t$)} = \alpha t + \beta\cos(\gamma - \delta t)\textrm{logistic}(\epsilon t - \zeta) - \eta
\end{align*}
which captures the approximately linear trend, and the periodic component with approximately linearly (logistic) increasing amplitude.
However, the annual cycle is heavily approximated by a sinusoid and the model does not capture heteroscedasticity.

\fi





