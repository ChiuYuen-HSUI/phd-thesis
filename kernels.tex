\input{common/header.tex}
\inbpdocument

\chapter{Expressing Structure with Kernels}
\label{ch:kernels}

In this chapter, we'll show how to use kernels (also called \emph{covariance functions}) to build many different kinds of models of functions.
By combining a few simple kernels through addition and multiplication, we'll be able to express many different kinds of structure: additivity, symmetry, periodicity, interactions between variables, and many types of invariances.
Combining kernels in a few simple ways gives us a rich language with which to build appropriate models.


\section{Definition}

Since we'll be discussing covariance functions at length, we now give a precise definition.
A kernel $\kernel(x, x') : \InputSpace \times \InputSpace \to \Reals$ is any positive-definite function between two points $\inputVar, \inputVar'$ in some space $\InputSpace$.
In this chapter, $\InputSpace$ is usually taken to be Euclidian space, but it doesn't have to be.
As we shall see, $\InputSpace$ can also be the space of images, documents, categories or points on a sphere, for example.

%\fTBD{Define $\GP\sim\dots$ here?}
Gaussian process models use a kernel to define the prior covariance between any two function values:
%${\textrm{Cov}(\outputVar, \outputVar') = \kernel(\inputVar,\inputVar')}$
%
\begin{align}
\textrm{Cov}\left(f(\inputVar), f(\inputVar') \right) = \kernel(\inputVar,\inputVar')
\end{align}
%
Colloquially, kernels are often said to specify the similarity between two objects.
This is slightly misleading, since what is actually being specified is the similarity between the values of a \emph{function} of two objects.
The kernel specifies which structures are likely under the \gp{} prior, which in turn determines the generalization properties of the model.

%The kernel, $\kernel$, must define a valid covariance function\fTBD{expand me}; when this is the case $\kernel$ is said to be positive semi-definite (PSD).

%In this section, we review the ways in which kernel families\footnotemark can be composed to express diverse priors over functions. 
%\footnotetext{When unclear from context, we use `kernel family' to refer to the parametric forms of the functions given in the appendix. A kernel is a kernel family with all of the parameters specified.}  





\section{Basic Kernels}
Commonly used kernels families include the squared exponential (\kSE), periodic (\kPer), linear (\kLin), and rational quadratic (\kRQ).
These kernels are defined in figure~\ref{fig:basic_kernels}.
%
\newcommand{\fhbig}{2cm}
\newcommand{\fwbig}{3cm}
\newcommand{\kernpic}[1]{\includegraphics[height=\fhbig,width=\fwbig]{\grammarfiguresdir/structure_examples/#1}}
\newcommand{\kernpicr}[1]{\rotatebox{90}{\includegraphics[height=\fwbig,width=\fhbig]{\grammarfiguresdir/structure_examples/#1}}}
\newcommand{\addkernpic}[1]{{\includegraphics[height=\fhbig,width=\fwbig]{\grammarfiguresdir/additive_multi_d/#1}}}
\newcommand{\largeplus}{\tabbox{{\Large+}}}
\newcommand{\largeeq}{\tabbox{{\Large=}}}
\newcommand{\largetimes}{\tabbox{{\Large$\times$}}}
\newcommand{\fixedx}{$x$ with $(x' = 1)$}
%
\begin{figure}[h]
\centering
\begin{tabular}{cccc}
Squared-exp (\kSE)  & Periodic (\kPer) & Linear (\kLin) & Rational quadratic (\kRQ) \\[6pt]
$\exp\left(-\frac{(\inputVar - \inputVar')^2}{2\ell^2}\right)$ &
$\exp\left(-\frac{2}{\ell^2} \sin^2 \left( \pi \frac{\inputVar - \inputVar'}{p} \right)\right)$ &
$(\inputVar - c)(\inputVar' - c)$ &
$\left( 1 + \frac{(\inputVar - \inputVar')^2}{2 \alpha \ell^2} \right)^{-\alpha}$ \\[14pt]
\kernpic{se_kernel} & \kernpic{per_kernel} & \kernpic{lin_kernel} & \kernpic{rq_kernel}\\
$x -x'$ & $x -x'$ & \fixedx & $x -x'$ \\
%& & & \\
\large $\downarrow$ & \large $\downarrow$ & \large $\downarrow$ & \large $\downarrow$  \\
\kernpic{se_kernel_draws} & \kernpic{per_kernel_draws_s2} & \kernpic{lin_kernel_draws} & \kernpic{rq_kernel_draws}\\
local variation & repeating structure & linear functions & multi-scale variation \\[10pt]
\end{tabular}
\caption[Examples of structures expressible by base kernels]
{Examples of structures expressible by base kernels.
Left and third columns: base kernels $k(\cdot,0)$.
Second and fourth columns: draws from a \sgp{} with each repective kernel.
The x-axis has the same range on all plots.}
\label{fig:basic_kernels}
\end{figure}
%
There is nothing special about these kernels in particular, except that they represent a diverse set.

Each covariance function corresponds to a different set of assumptions made about the function we wish to model.
For example, using a squared-exp ($\kSE$) kernel implies that the function we are modeling has infinitely many derivatives.



\section{Combining Kernels}

The rest of this chapter will explore ways in which kernels can be combined to create new ones with different properties.
Using a few simple base kernels and operations for combining them will give us a rich language with which to build models with diverse sorts of properties.
%For an overview, see \cite[Chapter~4]{rasmussen38gaussian}.

%In this chapter, we'll focus on two methods for combining kernels: addition and multiplication.
%\begin{align}
%k_1 + k_2 =& \,\, k_1(x,x') + k_2(x,x')\\
%k_1 \times k_2 =& \,\, k_1(x,x') \times k_2(x,x')
%\end{align}

%Combining kernels using these operations can yield kernels encoding for richer structures than are encoded in the original kernels.
% such as approximate periodicity ($\kSE \times \kPer$) or smooth functions with linear trends ($\kSE + \kLin$).


%This allows one to create richly structured and interpretable kernels from well-understood base components.
%Figure \ref{fig:kernels} shows several examples of structured kernels that can be constructed by adding or multiplying standard base kernels.

\subsection{Notation}

In this chapter, we'll focus on two ways of combining kernels: addition and multiplication, which we'll often write in shorthand without arguments:
\begin{align}
k_1 + k_2 =& \,\, k_1(x,x') + k_2(x,x')\\
k_1 \times k_2 =& \,\, k_1(x,x') \times k_2(x,x')
\end{align}


All of the base kernels we'll consider in this chapter are one-dimensional.
Kernels over multidimensional inputs are constructed by adding and multiplying kernels over individual dimensions.
These dimensions are represented using subscripts, e.g. $\SE_2$ represents an \kSE{} kernel over the second dimension of $\inputVar$.
%
\begin{figure}[h]
\centering
\begin{tabular}{cccc}
$\kLin \times \kLin$ & $\kSE \times \kPer$ & $\kLin \times \kSE$ & $\kLin \times \kPer$ \\
\kernpic{lin_times_lin} & \kernpic{se_times_per} & \kernpic{se_times_lin} & \kernpic{lin_times_per}\\
\fixedx & $x -x'$ & \fixedx & \fixedx\\
%& & & \\
\large $\downarrow$ & \large $\downarrow$ & \large $\downarrow$ & \large $\downarrow$  \\
\kernpic{lin_times_lin_draws}  & \kernpic{se_times_per_draws_s7} & \kernpic{se_times_lin_draws_s2} & \kernpic{lin_times_per_draws_s2} \\
quadratic functions & locally \newline periodic & increasing variation  & growing amplitude \\[10pt]
\end{tabular}
\caption[Examples of one-dimensional structures expressible by multiplying kernels]
{ Examples of one-dimensional structures expressible by multiplying kernels.  
%The x-axis has the same scale for all plots.
Plots have same meaning as in figure \ref{fig:basic_kernels}.}
\label{fig:kernels_times}
\end{figure}





\begin{figure}[h]
\centering
\begin{tabular}{cccc}
%Composite & Draws from \gp{} & \gp{} posterior \\ \toprule
$\kLin + \kPer$ & $\kSE + \kPer$ & $\kSE + \kLin$ & $\kSE^{(\textnormal{long})} + \kSE^{(\textnormal{short})}$ \\
\kernpic{lin_plus_per} & \kernpic{se_plus_per} & \kernpic{se_plus_lin} & \kernpic{longse_plus_se}\\
\fixedx & $x -x'$ & \fixedx & $x -x'$\\
%& & & \\
\large $\downarrow$ & \large $\downarrow$ & \large $\downarrow$ & \large $\downarrow$  \\
\kernpic{lin_plus_per_draws} & \kernpic{se_plus_per_draws_s7} & \kernpic{se_plus_lin_draws_s5} & \kernpic{longse_plus_se_draws_s7}\\
periodic plus trend & periodic plus noise & linear plus variation & slow \& fast variation \\[10pt]
\end{tabular}
\caption[Examples of one-dimensional structures expressible by adding kernels]
{ Examples of one-dimensional structures expressible by adding kernels.  
%The x-axis has the same scale for all plots.
Plots have same meaning as in figure \ref{fig:basic_kernels}.}
\label{fig:kernels_plus}
\end{figure}





\section{Expressing Additivity}

Additivity is a very useful modeling assumption in a wide variety of contexts.  It often aids in building interpretable models, as well as enabling extrapolation in high-dimensional settings.
Fortunately, this assumption is easy to encode in \gp{} models.
By summing kernels, we can model the data as a sum of independent functions, each possibly representing a different type of structure.
Suppose functions ${\function_1, \function_2}$ are drawn independently fpr \gp{} priors:
%
\begin{align}
\function_1 \dist \GP(\mu_1, \kernel_1)\\
\function_2 \dist \GP(\mu_2, \kernel_2)
\end{align}
%
Then we can model the sum of those functions through another \gp{}:
%
\begin{align}
%\function := 
\function_1 + \function_2 \dist \GP(\mu_1 + \mu_2, \kernel_1 + \kernel_2).
\end{align}

%In time-series models, sums of kernels can express superposition of different processes, possibly operating at different scales.

%A theme throughout this thesis is exploring the idea that a lot of the expressivity of \gp{} models comes from the fact that these models can be combined and decomposed additively.



\subsection{Additivity Across Multiple Dimensions}

In multiple dimensions, summing kernels can give rise to additive structure over different dimensions.
If the kernels being added together are functions only of a subset of input dimensions, then the implied prior over functions decomposes in the same way.
For example, if
%
\begin{align}
%\function := 
f(x_1, x_2) \dist \GP(\vzero, \kernel_1(x_1, x_1') + \kernel_2(x_2, x_2'))
\end{align}
%
Then this is equivalent to the model
%
\begin{align}
\function_1(x_1) & \dist \GP(\vzero, \kernel_1(x_1, x_1'))\\
\function_2(x_2) & \dist \GP(\vzero, \kernel_2(x_2, x_2'))\\
f(x_1, x_2) & = f_1(x_1) + f_2(x_2)
\end{align}
%
%Then we can model the sum of those functions through another \gp{}:
%

%\subsection{Additivity in a GP}
\begin{figure}
\centering
\begin{tabular}{ccccc|c}
\multicolumn{5}{c|}{Kernels constructed through addition} & \multicolumn{1}{c}{Mutliplicative} \\
\multicolumn{5}{c|}{of orthogonal one-dimensional components} & \multicolumn{1}{c}{kernels} \\
%\hline \\
\hspace{-0.2cm}\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/additive_kernel_sum_p2} 
& \hspace{-0.4cm} \raisebox{1cm}{+} \hspace{-0.4cm} & 
\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/additive_kernel_sum_p1} 
& \hspace{-0.4cm} \raisebox{1cm}{=} \hspace{-0.4cm} & 
\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/additive_kernel} &
\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/sqexp_kernel} \\
$k_1(x_1, x_1')$ & & $k_2(x_2, x_2')$ & & $k_1(x_1,x_1') + k_2(x_2,x_2')$ &$k_1(x_1,x_1')k_2(x_2,x_2')$ \\[1em]
%1D kernel & & 1D kernel & & 1st order kernel & 2nd order kernel \\ 
%& & & & & \\
%(Second Order) & & & & & Additive Kernel \\
%&&&&& \\
\large $\downarrow$ & & \large $\downarrow$ & & \large $\downarrow$ & \large $\downarrow$  \\[-0.2em]
\hspace{-0.2cm}\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/additive_kernel_draw_sum_p1}
& \hspace{-0.4cm} \raisebox{1cm}{+} \hspace{-0.4cm} & 
\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/additive_kernel_draw_sum_p2}
& \hspace{-0.4cm} \raisebox{1cm}{=} \hspace{-0.4cm} &
\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/additive_kernel_draw_sum} &
\includegraphics[width=0.2\textwidth]{\additivefigsdir/2d-kernel/sqexp_draw} \\
$f_1 \sim \GP\left(0, k_1\right)$ & & $f_2 \sim \GP\left(0, k_2\right)$ & & $f_1(x_1) + f_2(x_2)$ & $f(x_1, x_2)$ \\
%draw from & & draw from & & draw from & draw from\\
%1D GP prior & & 1D GP prior & & 1st order GP prior & 2nd order GP prior\\
%Draw from product kernel GP prior & Draw from additive kernel GP prior\\
\end{tabular}
\caption[Additive kernels correspond to additive functions]{Top left: An additive kernel is simply a sum of kernels.  Bottom left:  A draw from an additive kernel corresponds to a sum of draws from \gp{}s with the corresponding kernels.  Top right: a product kernel is a product of kernels.  Bottom right:  A draw from a product kernel does not correspond to a product of draws from the corresponding kernels.
%In this example, both kernels are composed of one dimensional squared-exponential kernels, but this need not be the case in general.
}
\label{fig:kernels}
\end{figure}

Figure \ref{fig:kernels} illustrates this decomposition.%, for two-dimensional functions, a first-order additive kernel with a second-order kernel.
Note that the product of two kernels does not have an analogous interpretation as the product of two functions.

\subsection{Long-range Extrapolation Through Additivity}

Because additive kernels can discover non-local structure in data, they are exceptionally well-suited to problems where local interpolation fails.  
%[Would this make them good for nonstationary noise?]
\begin{figure}[h]
\centering
\begin{tabular}{ccc}
True function: & Squared-exp \gp{} model: & Additive \gp{} model: \\
\parbox{0.33\columnwidth}{$f(x_1, x_2) = \sin(x_1) + \sin(x_2)$} & $k(x_1, x_1') \times k(x_2, x_2')$ & $k(x_1, x_1') + k(x_2, x_2')$ \\
\hspace{-0.1in}\includegraphics[width=0.33\textwidth]{\additivefigsdir/1st_order_censored_truth} &
\hspace{-0.1in}\includegraphics[width=0.33\textwidth]{\additivefigsdir/1st_order_censored_ard}&
\hspace{-0.1in}\includegraphics[width=0.33\textwidth]{\additivefigsdir/1st_order_censored_add} \\[1em]
\end{tabular}
\caption[Long-range inference in functions with additive structure]
{Inference in functions with additive structure.
When the function we're modeling has additive structure, we can epxloit this fact to extrapolate far from the training data.

%  The additive GP is able to discover the additive pattern, and use it to fill in a distant mode.  The ARD kernel can only interpolate, and thus predicts the mean in locations missing data.
}
\label{fig:synth2d}
\end{figure}
%Figure \ref{fig:synth2d} shows a simple experiment demonstrating the ability of additive kernels to discover low-order structure, and to exploit that structure to make predictions about unseen combinations of inputs.
Figure \ref{fig:synth2d} shows the very different extrapolations made by different \gp{} models.
We constructed a training set by evaluating a sum of two axis-aligned sine functions in a a small, L-shaped area.
%The ability of additive GPs to discover long-range structure suggests that this model may be well-suited to deal with covariate-shift problems.

These types of additive models have been well-explored in the statistics literature.  For example, generalized additive models~\citep{hastie1990generalized} have seen wide adoption.




\subsection{Marginal Variance and Covariance of Components}

In this section, we derive the posterior marginal variance and covariance of the additive components of a \gp{}.  These formulas let us plot the marginal variance of each component separately.  These formulas can also be used to examine the posterior covariance between pairs of components.

%Let us assume that our function $\vf$ is a sum of two functions, $\vf_1$ and $\vf_2$, where $\vf = \vf_1 + \vf_2$.  

Let's examine the joint prior over the sum of two functions.
We'll distinguish between $f(\vx)$ (the function values at the training locations) and  $f(\vx^\star)$ (the function values at some other, query locations) so that it's clear which matrices to use to extrapolate.

If $\vf_1$ and $\vf_2$ are \emph{a priori} independent, and $\vf_1 \sim \gp( \vmu_1, k_1)$ and $\vf_2 \sim \gp( \vmu_2, k_2)$, then
%
\begin{align}
\left[ \begin{array}{l} 
\vf_1(\vx) \\
\vf_1(\vx^\star) \\
\vf_2(\vx) \\
\vf_2(\vx^\star) \\
\vf_1(\vx) + \vf_2(\vx) \\
\vf_1(\vx^\star) + \vf_2(\vx^\star)
\end{array} \right]
\sim
\Nt{
\left[ \begin{array}{c} \vmu_1 \\ \vmu_1^\star \\ \vmu_2 \\ \vmu_2^\star \\ \vmu_1 + \vmu_2 \\ \vmu_1^\star + \vmu_2^\star \end{array} \right]
}
{\left[ \begin{array}{cccccc} 
\vK_1 & \vK_1^\star & 0 & 0 & \vK_1 & \vK_1^\star \\ 
\vK_1^\star & \vK_1^{\star\star} & 0 & 0 & \vK_1^\star & \vK_1^{\star\star} \\
0 & 0 & \vK_2 & \vK_2^\star & \vK_2 & \vK_2^\star \\ 
0 & 0 & \vK_2^\star & \vK_2^{\star\star} & \vK_2^\star & \vK_2^{\star\star} \\
\vK_1 & \vK_1^\star & \vK_2 & \vK_2^\star & \vK_1 + \vK_2 & \vK_1^\star + \vK_2^\star \\ 
\vK_1^\star & \vK_1^{\star\star}  & \vK_2^\star & \vK_2^{\star\star}  & \vK_1^\star + \vK_2^\star & \vK_1^{\star\star} + \vK_2^{\star\star}\\
\end{array} \right]
}
\end{align}
%
where $\vk_1 = k_1( \vX, \vX )$ and $\vk_1^\star = k_1( \vX^\star, \vX )$. 

By the formula for Gaussian conditionals:
\begin{align}
\vx_A | \vx_B \sim \Nt{\vmu_A + \vSigma_{AB} \vSigma_{BB}\inv \left( \vx_B - \vmu_B \right) }
{\vSigma_{AA} - \vSigma_{AB} \vSigma_{BB}\inv \vSigma_{BA} },
\end{align}
we get that the conditional variance of a Gaussian conditioned on its sum with another Gaussian is given by
\begin{align}
\vf_1^\star | \vf \sim \Nt{\vmu_1^\star + \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \left( \vf - \vmu_1 - \vmu_2 \right) } { \vk_1^{\star\star} - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_1^{\star}}
%\vf_1(\vx^\star) | \vf(\vx) \sim \mathcal{N}\big( 
%& \vmu_1(\vx^\star) + \vk_1(\vx^{\star}, \vx)  \left[ \vK_1(\vx, \vx) + \vK_2(\vx, \vx) \right]\inv \left( \vf(\vx) - \vmu_1(\vx) - \vmu_2(\vx) \right) , \nonumber \\
%& \vk_1(\vx^{\star}, \vx^{\star}) - \vk_1(\vx^{\star}, \vx)  \left[ \vK_1(\vx, \vx) + \vK_2(\vx, \vx) \right] \inv \vk_1(\vx, \vx^{\star}) 
%\big).
\end{align}

We can even compute the posterior covariance between two components, conditioned on their sum:
%
\begin{align}
\covargs{ \vf_1^\star}{\vf_2^\star | \vf } 
& = - \vk_1^{\star\tra} (\vK_1 + \vK_2)\inv \vk_2^\star
%\covargs{\vf_1(\vx^\star)}{\vf_2(\vx^\star) | \vf(\vx) }
%& = - \vk_1(\vx^{\star}, \vx)  \left[ \vK_1(\vx, \vx) + \vK_2(\vx, \vx) \right] \inv \vk_1(\vx, \vx^{\star}) 
\label{eq:post-component-cov}
\end{align}
%
\ref{eq:post-component-cov} is not always negative.

These formulae express the posterior model uncertainty about different components of the signal, integrating over the possible configurations of the other components.








\subsection{Interpreting Additive Models in High Dimensions}
%As noted by \citet{plate1999accuracy}, o
One advantage of additive models is their interpretability.
% since a high-dimensional function is decomposed into a series of one- or two-dimensional functions.  
\citet{plate1999accuracy} demonstrated that by allowing high-order interactions as well as low-order interactions, one can trade off interpretability with predictive accuracy.
In the case where the hyperparameters indicate that most of the variance in a function can be explained by low-order interactions, it is informative to plot the corresponding low-order functions, as in Figure \ref{fig:interpretable functions}. 


\newcommand{\concretepic}[1]{\includegraphics[width=0.31\columnwidth]{\decompfigsdir/concrete-component-#1}}
\newcommand{\concretelegend}[0]{\raisebox{5mm}{\includegraphics[trim=56mm 1mm 6mm 33mm, clip, width=0.2\columnwidth]{\decompfigsdir/concrete-component-1-legend}}}

\begin{figure}[h]
\centering
\begin{tabular}{ccc}
%\includegraphics[width=0.3\textwidth]{\additivefigsdir/interpretable_1st_order1.pdf} &
%\includegraphics[width=0.3\textwidth]{\additivefigsdir/interpretable_1st_order2.pdf}& 
Cement & Slag & Fly Ash\\
\concretepic{1} & \concretepic{2} & \concretepic{3} \\
 Water & Plasticizer & Coarse\\
\concretepic{4} & \concretepic{5} & \concretepic{6} \\
 Fine & Age \\
 \concretepic{7} & \concretepic{8} & \concretelegend \\
\end{tabular}
\caption[Decomposition of posterior into interpretable one-dimensional functions]
{By considering only one-dimensional terms of the additive kernel, we recover a form of Generalized Additive Model, and can plot the corresponding 1-dimensional functions.
Blue points indicate the original data, green points are data after the contribution from all other terms has been subtracted.
The vertical axis is the same for all plots.
%The black line is the posterior mean of a \sgp{} with only one term in its kernel.
%Right:  the posterior mean of a \sgp{} with only one second-order term in its kernel.
}
\label{fig:interpretable functions}
\end{figure}

%rs = 1 - var(complete_mean - y)/ var(y)

%R-squared = 0.9094

\begin{figure}[h!]
\centering
%
%
%
\renewcommand{\tabcolsep}{1mm}
\def\incpic#1{\includegraphics[width=0.1\columnwidth]{../figures/decomp/concrete-#1}}
\begin{tabular}{p{2mm}*{6}{c}}
 & {Cement} & {Slag} & {Fly Ash} & {Water} & \parbox{0.1\columnwidth}{Plasticizer} & {Age} \\ 
 \rotatebox{90}{{Cement}} & \incpic{Cement-Cement} & \incpic{Cement-Slag} & \incpic{Cement-Fly-Ash} & \incpic{Cement-Water} & \incpic{Cement-Plasticizer} & \incpic{Cement-Age} \\ 
 \rotatebox{90}{{Slag}} & \incpic{Slag-Cement} & \incpic{Slag-Slag} & \incpic{Slag-Fly-Ash} & \incpic{Slag-Water} & \incpic{Slag-Plasticizer} & \incpic{Slag-Age} \\ 
 \rotatebox{90}{{Fly Ash}} & \incpic{Fly-Ash-Cement} & \incpic{Fly-Ash-Slag} & \incpic{Fly-Ash-Fly-Ash} & \incpic{Fly-Ash-Water} & \incpic{Fly-Ash-Plasticizer} & \incpic{Fly-Ash-Age} \\ 
 \rotatebox{90}{{$\quad$Water}} & \incpic{Water-Cement} & \incpic{Water-Slag} & \incpic{Water-Fly-Ash} & \incpic{Water-Water} & \incpic{Water-Plasticizer} & \incpic{Water-Age} \\ 
 \rotatebox{90}{\parbox{0.1\columnwidth}{Plasticizer}} & \incpic{Plasticizer-Cement} & \incpic{Plasticizer-Slag} & \incpic{Plasticizer-Fly-Ash} & \incpic{Plasticizer-Water} & \incpic{Plasticizer-Plasticizer} & \incpic{Plasticizer-Age} \\ 
 \rotatebox{90}{{Age}} & \incpic{Age-Cement} & \incpic{Age-Slag} & \incpic{Age-Fly-Ash} & \incpic{Age-Water} & \incpic{Age-Plasticizer} & \incpic{Age-Age} \\
 \end{tabular}
 \fbox{
\begin{tabular}{c}
 Correlation \\[1ex]
\includegraphics[width=0.1\columnwidth, clip, trim=6.2cm 0cm 0cm 0cm]{../figures/decomp/concrete-colorbar}
\end{tabular}
}
%
%
%
\caption[Visualizing posterior correlations between components]
{Visualizing posterior correlations between the components explaining the concrete dataset.
%Plots on the diagonal show the posterior correlation of each function.
Each plot shows the posterior correlation between two components.
Within each plot, color indicates the amount of correlation between the function value of the two components.
Red indicates high correlation, teal indicates no correlation, and blue indicates negative correlation.
%Off-diagonal plots show posterior covariance between each pair of functions, as a function of both inputs.
%Because each
We can see that there is negative correlation between the ``Cement'' and ``Slag'' variables.  This reflects a degeneracy in the model.
%TODO: explain the degeneracy.
The `Coarse' and `Fine' dimensions are not shown here because their variance was zero.
}
\label{fig:interpretable interactions}
\end{figure}




\section{Expressing Interactions}

Multiplying kernels allows us to account for interactions between different input dimensions or different notions of similarity. 
For instance, in multidimensional data, the multiplicative kernel $\SE_1 \times \SE_3$ represents a smoothly varying function of dimensions 1 and 3 which is not constrained to be additive.
In univariate data, multiplying a kernel by \kSE{} gives a way of converting global structure to local structure. 
For example, $\Per$ corresponds to globally periodic structure, whereas $\Per \times \SE$ corresponds to locally periodic structure, as shown in row 1 of figure~\ref{fig:kernels}.

Many architectures for learning complex functions, such as convolutional networks \cite{lecun1989backpropagation} and sum-product networks \cite{poon2011sum}, include units which compute AND-like and OR-like operations.
Composite kernels can be viewed in this way too. A sum of kernels can be understood as an OR-like operation: two points are considered similar if either kernel has a high value.
Similarly, multiplying kernels is an AND-like operation, since two points are considered similar only if both kernels have high values.
Since we are applying these operations to the similarity functions rather than the regression functions themselves, compositions of even a few base kernels are able to capture complex relationships in data which do not have a simple parametric form.




\subsection{Expressing Multiplication with a Known Function}

If we wish to model a function that's been multiplied by some fixed, known function $a(x)$, this can be easily achieved by multiplying the kernel by $a(\vx) a(\vx')$.

This is why it is common practice to assume zero mean, since marginalizing over an unknown mean function can be equivalently expressed as a zero-mean \gp{} with a new kernel.





\section{Feature Representation}
%
By Mercer's theorem \citep{mercer1909functions},
%(TODO:cite)
any positive-definite kernel can be represented as the inner product between a fixed set of features, evaluated at $x$ and at $x'$:
%
\begin{align}
k(x,x') = \feat(x)\tra \feat(x')
\end{align}

As a simple example, the squared-exponential kernel ($\kSE$) on the real line has a representation in terms of infinitely many radial-basis functions.
Any stationary kernel (one which only depends on the distance between its inputs) on the real line can be represented by a set of sines and cosines - a Fourier representation \citep{bochner1959lectures}.
In general, the feature representation of a kernel is not unique, and depends on which space $\InputSpace$ is being considered \citep{minh2006mercer}.

In some cases, $\InputSpace$ can even be the infinite-dimensional feature mapping of another kernel.  Composing feature maps in this way leads to \emph{deep kernels}, a topic explored in chapter~\ref{ch:deep-limits}.



\subsection{Relation to Linear Regression}

Suprisingly, \gp{} regression is equivalent to Bayesian linear regression on $\feat(x)$:
%
\begin{align}
f(\iva) = \vw\tra \feat(\iva), \quad \vw \sim \N{\vw}{\vzero}{\Sigma} \quad
\iff
\quad f \sim \GPdist{f}{\vzero}{\feat(\iva) \tra \Sigma \feat(\iva)}
\end{align}
%
The link between Gaussian processes, linear regression, and neural networks is explored further in chapter~\ref{ch:deep-limits}.


\subsection{Feature-space view of combining kernels}

We can also view kernel addition and multiplication as a combination of the features of the original kernels.

Viewing kernel addition from this point of view, if
%
\begin{align}
k_1(\iva, \iva') & = \feat_1(\iva)\tra \feat_1(\iva')\\
k_2(\iva, \iva') & = \feat_2(\iva)\tra \feat_2(\iva')
\end{align}
%
then
%
\begin{align}
k_1(\iva, \iva') + k_2(\iva, \iva)
& = \feat_1(\iva)\tra \feat_2(\iva') + \feat_1(\iva)\tra \feat_2(\iva') \\
& = \colvec{\feat_1(\iva)}{\feat_2(\iva)}^\tra \colvec{\feat_1(\iva')}{\feat_2(\iva')}
\end{align}

In other words, the features of $k_1 + k_2$ are the concatenation of the features of each kernel.

We can examine kernel multiplication in a similar way:
%
\begin{align}
k_1(\iva, \iva') \times k_2(\iva, \iva')
& = \left[ \feat_1(\iva)\tra \feat_1(\iva') \right] \times \left[ \feat_2(\iva)\tra \feat_2(\iva') \right] \\
%& = \left[ \begin{array}{c} \feat_1(\iva) \\ \feat_2(\iva) \end{array} \right]^\tra \left[ \begin{array}{c} \feat_1(\iva') \\ \feat_2(\iva') \end{array} \right]
& = \sum_i h_i(\iva) h_i(\iva') \times \sum_j h_j(\iva) h_j(\iva') \\
& = \sum_i \sum_j h_i(\iva) h_i(\iva') h_j(\iva) h_j(\iva') \\
& = \sum_{i,j} \left[ h_i(\iva) h_j(\iva) \right] \left[ h_i(\iva') h_j(\iva') \right] \\
& = \vecop{ \feat_1(\iva) \otimes \feat_2(\iva') } \tra \vecop{ \feat_1(\iva) \otimes \feat_2(\iva')} 
\end{align}

In other words, the features of $k_1 \times k_2$ are the cartesian product (all possible combinations) of the original sets of features.
For example, one set of features $\feat(\iva)$ corresponding to a one-dimensional $\kSE$ kernel is a set of infinitely many Gaussian bumps spread along the real line.
Multiplied by an $\kSE$ kernel in another dimension, the resulting features are a set of two-dimensional Gaussian bumps, covering the plane.






\section{Expressing Symmetries}

When modeling functions, encoding known symmetries greatly aids learning and prediction.  We demonstrate that in nonparametric regression, many types of symmetry can be enforced through operations on the covariance function.  These symmetries can be composed to produce nonparametric priors on functions whose domains have interesting topological structure such as spheres, torii, and M\"{o}bius strips.

% I would cite Pierre Chiche but he hasn't actually published his damn papers.

\citet{ginsbourger2012argumentwise} and \citet{Invariances13} characterized \gp{} priors on functions invariant to transformations.
They showed that the only way to construct a prior on functions which respect a given invariance, is to construct a kernel which is invariant to the same invariance.
Formally,
given a symmetry $\Phi$, and $f \sim \GPt{0}{k(\vx,\vx')}$, $f$ is invariant under the action of a finite group $\Phi$ if and only if k is argumentwise invariant:
%
\begin{align}
\forall \vx, \forall g \in G, \quad k( g(\vx), x') = k(\vx, \vx')
\end{align}

In this section, we give recipes for expressing several classes of symmetries.  Later, we will show how these can be combined to produce more interesting structures.


\subsection{Periodicity}
Given $D$ dimensions, we can enforce rotational symmetry on any subset of the dimensions:
%
\begin{align}
f(x) = f( x_i + \tau_i)% \quad \forall k \in \mathbb{Z}
\end{align}
% TODO: Generalize this to multiple dimensions.
%
by the applying a kernel between pairs transformed coordinates $\sin(x), \cos(x)$:
%
\begin{align}
k_{\textnormal{periodic}}(x, x') = k(\sin(x), \cos(x), \sin(x'), \cos(x'))
\end{align}
%
We can also apply rotational symmetry repeatedy to a single dimension.


\subsection{Reflective Symmetry}

\subsubsection{Reflective Symmetry Along an Axis}
W
e can enforce the symmetry
%
\begin{align}
f(x) = f( -x)
\end{align}
%
by the kernel transform
%
\begin{align}
k_{\textnormal{symm arg1}}(x, x') & = k(x, x') + k(x, -x') + k(-x, x') + k(-x, -x')
\end{align}
%
Note that, because $k(\vx, \vx') = k(\vx', \vx')$ for any PSD kernel, so we don't need to include the corresponding terms $k(-x, x')$ and $k(-x, -x')$.


\subsubsection{Reflective Symmetry Along a Diagonal}
We can enforce that a function is invariant to the order of two of its arguments:
%symmetry between any two dimensions:
%
\begin{align}
f(x, y) = f( y, x)
\end{align}
%
by two methods:  In the additive method, we transform the kernel by:
%
\begin{align}
k_{\textnormal{reflect add}}(x, y, x', y') 
 = k(x, y, x', y') + k(x, y, y', x')
 + k(y, x, x', y') + k(y, x, y', x')
\end{align}
%
or by
%
\begin{align}
k_{\textnormal{reflect min}}(x, y, x', y') = k( & \min(x, y), \max(x,y), % \nonumber \\ & 
\min(x', y'), \max(x',y') )
\end{align}
however, the second method will in general lead to non-differentiability along $x = y$.  Figure \ref{fig:add_vs_min} shows the difference.

\begin{figure}
\begin{tabular}{ccc}
\includegraphics[width=0.3\columnwidth]{\symmetryfigsdir/symmetric-xy-naive-sample} &
\includegraphics[width=0.3\columnwidth]{\symmetryfigsdir/symmetric-xy-prod-sample} &
\includegraphics[width=0.3\columnwidth]{\symmetryfigsdir/symmetric-xy-projection-sample} \\
Additive method & Product method & Representer method \\[0.5ex]
%$k(x, y, x', y') + k(x, y, y', x')$ & $k(x, y, x', y') \times k(x, y, y', x')$ & $k( \min(x, y), \max(x,y),$ \\
% $+ k(y, x, x', y') + k(y, x, y', x')$ & $\times k(y, x, x', y') \times k(y, x, y', x')$ & $\min(x', y'), \max(x',y') )$
$\begin{array}{r@{}l@{}}
k(x, y, x', y') & {}+ k(x, y, y', x') 
% \\ {}+ k(y, x, x', y') & {}+ k(y, x, y', x')
\end{array}$
&
$\begin{array}{r@{}l@{}}
k(x, y, x', y') & {}\times k(x, y, y', x') \\
% {}\times k(y, x, x', y') & {}\times k(y, x, y', x')
\end{array}$
&
$\begin{array}{r@{}l@{}}
k( &{}\min(x, y), \max(x,y), \\
&{}\min(x', y'), \max(x',y') )
\end{array}$
\end{tabular}
\caption[Three ways to introduce symmetry]{An illustration of three methods of introducing symmetry.
Left:  The additive method.
Center: The product method.
Right: The representer method.
%The additive method has half the marginal variance away from $y = x$, but the min method introduces a non-differentiable seam along $y = x$.
All three methods introduce a different type of nonstationarity.
}
\label{fig:add_vs_min}
\end{figure}


%\paragraph{Spherical Symmetry}

%We can also enforce that a function expresses the symmetries obeyed by $n-spheres$ by simply transforming a set of $n - 1$ coordinates by:
%
%\begin{align}
%x_1 & = \cos(\phi_1) \nonumber \\
%x_2 & = \sin(\phi_1) \cos(\phi_2) \nonumber \\
%x_3 & = \sin(\phi_1) \sin(\phi_2) \cos(\phi_3) \nonumber \\
%& \vdots \nonumber \\
%x_{n-1} & = \sin(\phi_1) \cdots \sin(\phi_{n-2}) \cos(\phi_{n-1}) \nonumber \\
%x_n & = \sin(\phi_1) \cdots \sin(\phi_{n-2}) \sin(\phi_{n-1})
%\end{align}

%\cite{flanders1989}

%\subsection{Parametric embeddings}

%In general, we can always enforce the symmetries obeyed by a given surface by finding a parametric embedding to that surface.  However, it is not clear how to do this in general without introducing unnecessary 


%It is possible to construct

%Because then one dimension would be special.


\subsection{Example: Computing Molecular Energies}

\begin{figure}
\begin{center}
\begin{tabular}{cc}
Function on M\"{o}bius strip & \\
\includegraphics[width=0.3\columnwidth, height=0.3\columnwidth, clip=true,trim=3cm 2cm 2cm 2cm]{\topologyfiguresdir/mobius_field} & 
  \begin{tikzpicture}

%\pgfmathsetmacro{\r}{3cm}
%\pgfmathsetmacro{\ho}{70}
%\pgfmathsetmacro{\ht}{30}
\newcommand{\radius}{3}
\newcommand{\hone}{120}
\newcommand{\htwo}{70}
\newcommand{\hthree}{30}

	\coordinate (O) at (0, 0);
	\coordinate (left) at ({\radius*cos(\hone)}, {\radius*sin(\hone)});
	\coordinate (right) at ({\radius*cos(\htwo)}, {\radius*sin(\htwo)});
	\coordinate (zero) at ({\radius*cos(\hthree)}, {\radius*sin(\hthree)});

	\draw[fill] (left) circle (2pt);
	\draw (left) node[below, left] {H};
	
	\draw[fill] (right) circle (2pt);
	\draw (right) node[right] {H};

	\draw[fill] (zero) circle (2pt);
	\draw (zero) node[right] {H};

	\draw[fill] (O) circle (3pt);
	\draw (O) node[below] {C};

	\draw (left) -- (O);
	\draw (right) -- (O);
	\draw (zero) -- (O);

	\begin{scope}
	\path[clip] (O) -- (right) -- (zero);
	\fill[red, opacity=0.5, draw=black] (O) circle (2);
	\node at ($(O)+(50:1.6)$) {$\theta_1$};	
	\end{scope}
	
	\begin{scope}
	\path[clip] (O) -- (left) -- (right);
	\fill[green, opacity=0.5, draw=black] (O) circle (1.8);
	\node at ($(O)+(90:1.4)$) {$\theta_2$};	
	\end{scope}	
  \end{tikzpicture}
\end{tabular}
\end{center}
\caption[The energy of a molecular configuration obeys the same symmetries as a M\"{o}bius strip]{An example of a function expressing the same symmetries as a M\"{o}bius strip in two of its arguments.  The energy of a molecular configuration $f(\theta_1, \theta_2)$ depends only on the relative angles between atoms, and because each atom is indistiguishable, is invariant to permuting the atoms. }
\label{fig:molecule}
\end{figure}

Figure \ref{fig:molecule} gives one example of a function which obeys the same symmetries as a M\"{o}bius strip, in some subsets of its arguments.



\subsection{Example: Translation Invariance in Images}

Most models of images are invariant to spatial translations [cite convolution nets].  Similarily, most models of sounds are also invariant to translation through time.

Note that this sort of translational invariance is completely distinct from the stationarity properties of kernels used in Gaussian process priors.  A stationary kernel implies that the prior is invariant to translations of the entire training and test set.

We are discussing here a discretized input space (into pixels or the audio equivalent), where the input vectors have one dimension for every pixel.  We are interested in creating priors on functions that are invariant to shifting a signal along its pixels:
%
\begin{align}
f \Bigg( \raisebox{-2.5ex}{ \includegraphics[width=1cm]{\topologyfiguresdir/grid2} } \Bigg) 
= f \Bigg( \raisebox{-2.5ex}{ \includegraphics[width=1cm]{\topologyfiguresdir/grid3} } \Bigg)
\end{align}

Translational invariance in this setting is equivalent to symmetries between dimensions in the input space.

This prior can be achieved in one dimension by using the following kernel transformation:
%
\begin{align}
k \left( (x_1, x_2, \dots, x_D ), (x_1', x_2', \dots, x_D' ) \right) & = %\nonumber \\
\sum_{i=1}^D \prod_{j=1}^D k( x_j, x_{ i + j \textnormal{mod $D$} }' )
\end{align}
%
Edge effects can be handled either by wrapping the image around, or by padding it with zeros.

\paragraph{Convolution} The resulting kernel could be called a \emph{discrete convolution kernel}.  For an image with $R, C$ rows and columns, it can also be written as:
%
\begin{align}
k_{\textnormal{conv}} \left( (x_{11}, x_{12}, \dots, x_{RC} ), (x_{11}', x_{12}', \dots, x_{RC}' ) \right) = % \nonumber \\
%\sum_{i=-L}^L \sum_{j=-L}^L k\left( (x_{1+i,1+j}, x_{1+i,2+j} , \dots, x_{R+i,C+j} ), (x_{1+i,1+j}', x_{1+i,2+j}' , \dots, x_{R+i,C+j}' ) \right)
\sum_{i=-L}^L \sum_{j=-L}^L k\left( \vx, T_{ij}(\vx') \right)
\end{align}
%
where $T_{ij}(\vx)$ is the operator which replaces each $x_{mn}$ with $x_{m+i, n+j}$.  Thus we are simply defining the covariance between two images to be the sum of all covariances between all relative translations of the two images.
%We can also normalize the kernel by pre-multiplying it with $\sqrt{k_{\textnormal{conv}}(\vx, \vx) k_{\textnormal{conv}}(\vx', \vx') }$.

%$f \left( \begin{tikzpicture} \draw[step=0.1,black,thin] (0.3,0.3) grid (1,1); \end{tikzpicture} \right)$



%Is there a pathology of the additive construction that appears in the limit?

%\subsection{Max-pooling}
%What we'd really like to do is a max-pooling operation.  However, in general, a kernel which is the max of other kernels is not PSD [put counterexample here?].  Is the max over co-ordinate switching PSD?

%\section{Related Work}
%\label{sec:related_work}

%\paragraph{Invariances in Gaussian processes}
%\cite{Invariances13} show that, for Gaussian processes, with probability one, $f(\vx) = f(T(\vx))$ if and only if $k(x, x') = k(x, T(x'))$.





\section{Generating Topological Manifolds}

In this section, we give a geometric illustration of the symmetries encoded by \gp{}s with different sorts of kernels.
The language of models of functions exhibiting symmetries, can be used to create a prior on surfaces, by warping a latent surface $\vx$ to an observed surface $\vy = \vf(\vx)$.
The distribution on $\vf$ allows us to put mass on 

%If we set the input $\vx$ to be a 2-dimensional plane, intro 3 dimensions using a \gp{} encoding certain symmetries, the resulting surfaces will correspon
First create a mesh in 2d.  Then draw 3 independent functions from a GP prior with the relevant symmetries encoded in the kernel.  Then, map the 2d points making up the mesh through those 3 functions to get the 3D  coordinates of each point on the mesh.

This is similar in spirit to the GP-LVM model \cite{lawrence2005probabilistic}, which learns an embedding of the data into a low-dimensional space, and constructs a fixed kernel structure over that space.
%The GP-LVM will be discussed in further detail in chapter ...

\subsection{Surfaces, Cylinders and Torii}

\begin{figure}
\begin{tabular}{ccc}
Manifold $( \SE_1 \times \SE_2 )$  & Cylinder $( \SE_1 \times \Per_2 )$ & Toroid $( \Per_1 \times \Per_2 )$\\
\includegraphics[width=0.33\columnwidth]{\topologyfiguresdir/manifold} &
\includegraphics[width=0.33\columnwidth]{\topologyfiguresdir/cylinder} &
\includegraphics[width=0.33\columnwidth]{\topologyfiguresdir/torus} \\
\end{tabular}
\caption[Generating 2D manifolds with different topological structures]{
Generating 2D manifolds with different topological structures.
By enforcing that the functions mapping from $\mathbb{R}^2$ to $\mathbb{R}^3$ obey the appropriate symmetries, the surfaces created have the corresponding topologies, ignoring self-intersections.
}
\label{fig:gen_surf}
\end{figure}

Figure \ref{fig:gen_surf} shows...


\subsection{M\"{o}bius Strips}

A prior on functions on M\"{o}bius strips can be achieved by enforcing the symmetries:
%
\begin{align}
f(x, y) & = f( x, y + \tau_y) \\
f(x, y) & = f( x + \tau_x, y)  \\
f(x, y) & = f( y, x )
\end{align}

If we imagine moving along the edge of a M\"{o}bius strip, that is equivalent to moving along a diagonal in the function generated.
Figure \ref{fig:mobius} shows this.
%
\begin{figure}
\begin{tabular}[t]{c|c}
%\begin{columns}
\centering
M\"{o}bius manifold & 
Sudanese M\"{o}bius strip \\
%$( \Per_1 \times \Per_2 )$ and $f(x,y) = f(y,x)$ & generated parametrically\\
$ \Per(x_1, x_1') \times \Per(x_2, x_2') + \Per(x_1, x_2') \times \Per(x_2, x_1')$ & generated parametrically\\
%$ + \Per(x_1, x_2') \times \Per(x_2, x_1')$ & \\
%\includegraphics[width=0.45\columnwidth, height=0.45\columnwidth, clip=true,trim=2cm 2cm 2cm 1cm]{\topologyfiguresdir/mobius_regression} \\
\includegraphics[width=0.3\columnwidth]{\topologyfiguresdir/mobius} &
%\includegraphics[width=0.33\columnwidth]{\topologyfiguresdir/mobius} 
%\begin{minipage}{0.33\columnwidth}
%{\begin{tabular}[t]{p{.3\columnwidth}}
%\includegraphics[width=0.3\columnwidth,clip=true,trim=0cm 0cm 10cm 0cm]{\topologyfiguresdir/sudanese-wikipedia}
%\\
%\includegraphics[width=0.3\columnwidth,clip=true,trim=10.55cm 0cm 0cm 0cm]{\topologyfiguresdir/sudanese-wikipedia}
%\end{tabular}}
%\end{minipage}
\raisebox{1cm}{\includegraphics[width=0.4\columnwidth,clip=true,trim=0cm 0cm 10cm 0cm]{\topologyfiguresdir/sudanese-wikipedia}}
\end{tabular}
\caption[Generating M\"{o}bius strips]{Generating M\"{o}bius strips.
Left: By enforcing that the functions mapping from $\mathbb{R}^2$ to $\mathbb{R}^3$ obey the appropriate symmetries, surfaces sampled from the prior have topology corresponding to a M\"{o}bius strip.
M\"{o}bius strips generated this way do not have the familiar shape of a circular flat surface with a half-twist; rather they tend to look like \emph{Sudanese} M\"{o}bius strips \citep{sudanese1984}, whose edge has a circular shape.
Right: A Sudanese projection of a M\"{o}bius strip.
Image adapted from \citep{sudanesepict}.
}
\label{fig:mobius}
\end{figure}
%
The second example is doesn't resemble a typical M\"{o}bius strip because the edge of the mobius strip is in a geometric circle.  This kind of embedding is resembles the Sudanese M\"{o}bius strip [cite].

Another classic example of a function living on a Mobius strip is the auditory quality of 2-note intervals.  The harmony of a pair of notes is periodic (over octaves) for each note, and the 






\section{Kernels on Discrete Data}

Kernels can be defined over all types of data structures: Text, images, matrices, and even kernels . Coming up with a kernel on a new type of data used to be an easy way to get a NIPS paper.

\subsection{Categorical Variables}

There is a simple way to do GP regression over categorical variables. Simply represent your categorical variable as a by a one-of-k encoding. This means that if your number ranges from 1 to 5, represent that as 5 different data dimensions, only one of which is on at a time. 

Then, simply put a product of SE kernels on those dimensions. This is the same as putting one SE ARD kernel on all of them. The lengthscale hyperparameter will now encode whether, when that coding is active, the rest of the function changes. If you notice that the estimated lengthscales for your categorical variables is short, your model is saying that it's not sharing any information between data of different categories. 







\iffalse

\section{Worked example: building a structured kernel for a time-series}

%\subsection{Modeling multiple periodicities}

\begin{figure}[h]
\begin{tabular}{ccc}
Long-term trend & Weekly periodicity &Yearly periodicity \\
\includegraphics[width=0.31\columnwidth]{\examplefigsdir/births-component-1} &
\includegraphics[width=0.31\columnwidth]{\examplefigsdir/births-component-3} & 
\includegraphics[width=0.31\columnwidth]{\examplefigsdir/births-component-2-zoom} 
\end{tabular}
\caption[Composite model of births data]{A composite \gp{} model of births data. (blue)}
\label{fig:quebec-decomp}
\end{figure}





\iffalse
\begin{figure}
\renewcommand{\tabcolsep}{1mm}
\def \incpic#1{\includegraphics[width=0.200\columnwidth]{../figures/worked-example/births-#1}}
\begin{tabular}{*{5}{c}}
 & {Long-term} & {Weekly} & {Yearly} & {Short-term} \\ 
 \rotatebox{90}{{Long-term}} & \incpic{Long-term-Long-term} & \incpic{Long-term-Weekly} & \incpic{Long-term-Yearly} & \incpic{Long-term-Short-term} \\ 
 \rotatebox{90}{{Weekly}} & \incpic{Weekly-Long-term} & \incpic{Weekly-Weekly} & \incpic{Weekly-Yearly} & \incpic{Weekly-Short-term} \\ 
 \rotatebox{90}{{Yearly}} & \incpic{Yearly-Long-term} & \incpic{Yearly-Weekly} & \incpic{Yearly-Yearly} & \incpic{Yearly-Short-term} \\ 
 \rotatebox{90}{{Short-term}} & \incpic{Short-term-Long-term} & \incpic{Short-term-Weekly} & \incpic{Short-term-Yearly} & \incpic{Short-term-Short-term} \\ 
 \end{tabular}
\caption[Two-way interactions in births data]{Two-way interactions in births data}
\label{fig:quebec-decomp}
\end{figure}
\fi
%\subsection{Incoportating discrete covariates}

%\subsection{Breaking down the predictions, examining different parts of the model}
\fi



\section{Why Assume Zero Mean?}

In literature, as well as in practice, it is common to construct \gp{} priors with a zero mean function.
This might seem strange, since it is presumably a good place to put prior information, or if we are comparing models, to express  since marginalizing over an unknown mean function can be equivalently expressed as a different \gp{} with zero-mean, and another term added to the kernel.

\paragraph{A known mean function can be moved into the covariance function}
Specifically, if we wish to model an unknown function $f(\vx)$ with known mean $m(\vx)$, (with unknown magnitude $c \sim \Nt{0}{\sigma^2_c}$), we can equivalently express this model using another \gp{} with zero mean:
%$related modelquivalently (\vx) a(\vx)$ with $f \sim \GPt{0}{k(\vx,\vx')}$, then this 
%
\begin{align}
f \sim \GPt{c m(\vx)}{k(\vx,\vx')}, \quad c \sim \Nt{0}{\sigma^2_c}
\iff f \sim \GPt{ \vzero }{ c^2 m(\vx) m(\vx') + k(\vx,\vx')}
\end{align}

%This correspondence means that, b
By moving the mean function into the covariance function, we get the same model, but we can integrate over the magnitude of the mean function at no additional cost.
This is one advantage of moving as much structure as possible into the covariance function.
%In fact, we can view \gp{} regression as simply implicitly integrating over the magnitudes of (possibly uncountably many) different mean functions all summed together.
%TODO: provide a link to the GPs as neural nets discussion.

\paragraph{An unknown mean function can be moved into the covariance function}

If we wish to express our ignorance about the mean function, one way would be by putting a \gp{} prior on it.
%
\begin{align}
m \sim \GPt{\vzero}{k_1(\vx,\vx')}, \quad
f \sim \GPt{m(\vx)}{k_2(\vx,\vx')}
\iff 
f \sim \GPt{\vzero}{k_1(\vx,\vx') + k_2(\vx,\vx')}
\end{align}


\section{Why Not Learn the Mean Function Instead?}
One might ask: besides integrating over the magnitude, what is the advantage of moving the mean function into the covariance function?
After all, mean functions are certainly more interpretable than a posterior distribution over functions.

Instead of searching over a large class of covariance functions, which seems strange and unnatural, we might consider simply searching over a large class of structured mean functions, assuming a simple \iid noise model.
This is the approach taken by practically every other regression technique: neural networks, decision trees, boosting, \etc.
If we could integrate over a wide class of possible mean functions, we would have a very powerful learning an inference method.
The problem faced by all of these methods is the well-known problem \emph{overfitting}.
If we are forced to choose just a single function with which to make predictions, we must carefully control the flexibility of the model we learn, generally prefering ``simple'' functions, or to choose a function from a restricted set.

If, on the other hand, we are allowed to keep in mind many possible explanations of the data, \emph{there is no need to penalize complexity}. [cite Occam's razor paper?]
The power of putting structure into the covariance function is that doing so allows us to implictly integrate over many functions, maintaining a posterior distribution over infinitely many functions, instead of choosing just one.
In fact, each of the functions being considered can be infinitely complex, without causing any form of overfitting.
For example, each of the samples shown in figure \ref{fig:gp-post} varies randomly over the whole real line, never repeating, each one requiring an infinite amount of information to describe.
Choosing the one function which best fits the data will almost certainly cause overfitting.
However, if we integrate over many such functions, we will end up with a posterior putting mass on only those functions which are compatible with the data.
In other words, the parts of the function that we can determine from the data will be predicted with certainty, but the parts which are still uncertain will give rise to a wide range of predictions.

To repeat: \emph{there is no need to assume that the function being modeled is simple, or to prefer simple explanations} in order to avoid overfitting, if we integrate over many possible explanations rather than choosing just the one.

In Chapter~\ref{ch:grammar}, we will compare a system which estimates a parametric covariance function against one which estimates a parametric mean function.


\section{Signal versus Noise}

In most derivations of Gaussian processes, the model is given as
%
\begin{align}
y = f(\inputVar) + \epsilon, \quad \textnormal{where} \quad \epsilon \simiid \Nt{0}{\sigma^2_{\textnormal{noise}}}
\end{align}

However, $\epsilon$ can equivalently be thought of as another Gaussian process, and so this model can be written as $y(\inputVar) \sim \GP\left(0, k + \delta \right)$.  The lack of a hard distinction between the noise model and the signal model raises the larger question:  Which part of a model represents signal, and which represents noise?

We believe that it depends on what you want to do with the model - there is no hard distinction between signal and noise in general.

For example: often, we don't care about the short-term variations in a function, and only in the long-term trend.
However, in many other cases, we with to de-trend our data to see more clearly how much a particular part of the signal deviated from normal.

%\subsubsection{Student's $t$ processes}

%One shortcoming of the



\section{Learning Hyperparameters}


\section{Learning Structure}
In some situations, we might not know \emph{a priori} whether a particular structure or symmetry is present in the function we are trying to model.
%Again, the fact that we can compare marginal likelihoods in \gp{}s means that we can 
%Because \gp{}s let us build models both with and without certain symmetries, 
By building kernels with and without such structure, we can compute the marginal likelihoods of the corresponding \gp{} models.
The quantities represent the relative amount of evidence that the data provide for each of these possibilities, providing the assumptions of the model are correect.
%To do so, we simple need to compare the marginal likelihood of the data
%We demonstrate that marginal likeihood an be used to automatically search over such structures.



\section{Conclusion}

We've seen that kernels are a flexible and powerful language for building models of different types of functions.
However, for a given problem, it can difficult to specify an appropriate kernel, even after looking at the data.
Rather than choosing one kernel \emph{a priori}, one should at least try out a few different kernels.
However, it might be difficult to enumerate all plausible kernels, and tedious to search over them.
%In fact, choosing the kernel can be considered one of the main difficulties in doing inference.

Analogously, we usually don't expect to simply guess the best value of some parameter.
Rather, we specify a search space and an objective, and ask the computer to the search this space for us. 
In the next chapter, we'll see how to perform such a search over the discrete space of kernel expressions.


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


