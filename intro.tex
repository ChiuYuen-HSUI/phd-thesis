\input{common/header.tex}
\inbpdocument

\chapter{Introduction}
\label{ch:intro}


\begin{quotation}
``I only work on intractable nonparametrics - Gaussian processes don't count.''

\hspace*{\fill} Sinead Williamson, personal communication
\end{quotation}


\section{Regression}

% These two paras written with James L and Zoubin
The general problem of regression consists of learning a function $f$ mapping from some input space $\mathcal{X}$ to some output space $\mathcal{Y}$.
We would like an expressive language which can represent both simple parametric forms of $f$ such as linear, polynomial, \etc and also complex nonparametric functions specified in terms of properties such as smoothness, periodicity, \etc~ 
Fortunately, Gaussian processes (\gp{}s) provide a very general and analytically tractable way of capturing both simple and complex functions. 

\section{Gaussian process models}


Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks\cite{rasmussen38gaussian}.  The kind of structure which can be captured by a GP model is mainly determined by its \emph{kernel}: the covariance function.  One of the main difficulties in specifying a Gaussian process model is in choosing a kernel which can represent the structure present in the data.  For small to medium-sized datasets, the kernel has a large impact on modeling efficacy.

Gaussian processes are distributions over functions such that any finite subset of function evaluations, $(f(x_1), f(x_2), \ldots f(x_N))$, have a joint Gaussian distribution \citep{rasmussen38gaussian}.
A \gp{} is completely specified by its mean function, $\mu(x)=\mathbb{E}(f(x))$ and kernel (or covariance) function $\kernel(x,x') = \Cov(f(x),f(x'))$.
It is common practice to assume zero mean, since marginalizing over an unknown mean function can be equivalently expressed as a zero-mean \gp{} with a new kernel.
The structure of the kernel captures high-level properties of the unknown function, $f$, which in turn determines how the model generalizes or extrapolates to new data.
We can therefore define a language of regression models by specifying a language of kernels.

\subsection{Useful properties of Gaussian process models}

\begin{itemize}
\item {\bf Tractable inference} Given a kernel function, the posterior distribution can be computed exactly in closed form.  This is a rare property for nonparametric models to have.
\item {\bf Expressivity} by choosing different covariance functions, we can express a very wide range of modeling assumptions.
\item {\bf Integration over hypotheses} the fact that a \gp{} posterior lets us exactly integrate over a wide range of hypotheses means that overfitting is less of an issue than in comparable model classes - for example, neural nets.
\item {\bf Marginal likelihood} A side benefit of being able to integrate over all hyoptheses is that we compute the \emph{marginal likelihood} of the data given the model.  This gives us a principled way of comparing different Gaussian process models.
\item {\bf Closed-form posterior} The posterior predictive distribution of a \gp{} is another \gp{}.  This means that \gp{}s can easily be composed with other models or decision procedures.  For example, \NA{Carl's reinforcement learning work.}
\end{itemize}


\begin{figure}[t]
\begin{centering}
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-0} & 
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-1} \\
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-2} & 
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-3}
\end{tabular}
\end{centering}
\caption[One-dimensional Gaussian process posterior]{A visual representation of a one-dimensional Gaussian process posterior.
Red isocountours show the marginal density at each input location.
Coloured lines are samples from the posterior.}
\label{fig:gp-post}
\end{figure}

Figure \ref{fig:gp-post} shows a Gaussian process posterior.  Typically, it's rendered with the mean and +- 2SD, but there's nothing special about mean.





\subsection{Why assume zero-mean?}

It is common practice to assume zero mean, since marginalizing over an unknown mean function can be equivalently expressed as a zero-mean \gp{} with a new kernel.





\section{Latent Variable Models}

Besides being useful for modeling functions, a simple extension allows \gp{}s to be useful for general density modeling.  



Unfortunately, this extension causes many of the useful properties of the \gp{} not to hold.

The \gplvm{} can also be thought of as a method for modeling the covariance matrix between all rows of $Y$ using a number of parameters which grows linearly with $N$.


\begin{figure}[t]
\begin{centering}
\includegraphics[width=\textwidth]{\gplvmfiguresdir/gplvm_1d_draw_9}
\end{centering}
\caption[One-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.  Bottom: density and samples from a 1D Gaussian, specifying the distribution $p(\vX)$ in the latent space.  Top Right: A function drawn from a \gp{} prior.  Left: A nonparametric density defined by warping the latent density through the function drawn from a \gp{} prior.}  
\label{fig:oned-gplvm}
\end{figure}



\begin{figure}
\begin{centering}
{\begin{tabular}{cccc}
\phantom{h} & Latent space $p(\vX)$ & & Observed space $p(\vY)$ \\
& \fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-latent-crop}} &
\raisebox{7em}{$\overset{\mathlarger{f(x)}}{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\rightarrow}}}}}}}}$} &
%\raisebox{7em}{\Huge $\overset{\textnormal{\Large{GP}}}\rightarrow$} &
\fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-draw-crop}}
\end{tabular}}
\end{centering}
\caption[Two-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.  Left:  Isocontours and samples from a 2D Gaussian, specifying the distribution $p(\vX)$ in the latent space.  Right: Density and samples from a nonparametric density defined by warping the latent density through a function drawn from a GP prior.}  
\label{fig:twod-gplvm}
\end{figure}



\section{Outline}

This thesis aims to present a set of related results about how the probabilistic nature of Gaussian process models allows them to be easily extended or composed with other models.
Furthermore, the fact that the marginal likelihood is often available (or easily approximable) means that we can evaluate how much evidence the data provides for one structure over another.


\paragraph{Chapter \ref{ch:kernels}} describes, in detail, the ways in which different sorts of structure can be introduced into a \gp{} model through the kernel.

\paragraph{Chapter \ref{ch:grammar}} shows how to construct a general, open-ended language over kernels - which implies a corresponding language over models.
Given a wide variety of structures, plus the ability to evaluate the suitability of each one, it's straightforward to automatically search over models.

\paragraph{Chapter \ref{ch:description}} shows that, for the particular language of models constructed in chapter \ref{ch:grammar}, it's relatively easy to automatically generate english-language descriptions of the models discovered.
Combined with 


\section{Contributions}

My doctorate was made possible, and enjoyable by the company of the many co-authors I was fortunate to work with.
In this section, I detail the novel contributions of this thesis, and attempt to give proper credit to my tireless co-authors.

\paragraph{Grammar on topologies}
Chapter \ref{ch:kernels} contains a section on describes, in detail, the ways in which different sorts of structure can be introduced into a \gp{} model through the kernel.

\paragraph{Deep kernels}
Section \ref{sec:deep_kernels}

The research upon which \paragraph{chapter \ref{ch:grammar}} is based was done in collaboration with James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani.
Specifically, myself, James Lloyd and Roger Grosse wrote the

\paragraph{Chapter \ref{ch:description}} was written in collaboration with James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani.
Specifically, James Lloyd wrote most of the code to automatically generate reports, and ran all of the experiments.
The idea of the correspondence between kernels and adjectives grew out of many extended discussions between myself and James.
The text was written mainly by myself and James Lloyd, with many helpful contributions and suggestions from Roger Grosse, Zoubin Ghaharamani, and Josh Tenenbaum.


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


