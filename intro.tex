\input{common/header.tex}
\inbpdocument

\chapter{Introduction}
\label{ch:intro}


\begin{quotation}
``I only work on intractable nonparametrics - Gaussian processes don't count.''

\hspace*{\fill} Sinead Williamson, personal communication
\end{quotation}


\section{Regression}

% These two paras written with James L and Zoubin
The general problem of regression consists of learning a function $f$ mapping from some input space $\mathcal{X}$ to some output space $\mathcal{Y}$.
We would like an expressive language which can represent both simple parametric forms of $f$ such as linear, polynomial, \etc and also complex nonparametric functions specified in terms of properties such as smoothness, periodicity, \etc~ 
Fortunately, Gaussian processes (\gp{}s) provide a very general and analytically tractable way of capturing both simple and complex functions. 

\section{Gaussian process models}


Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks\cite{rasmussen38gaussian}.  The kind of structure which can be captured by a GP model is mainly determined by its \emph{kernel}: the covariance function.  One of the main difficulties in specifying a Gaussian process model is in choosing a kernel which can represent the structure present in the data.  For small to medium-sized datasets, the kernel has a large impact on modeling efficacy.

Gaussian processes are distributions over functions such that any finite subset of function evaluations, $(f(x_1), f(x_2), \ldots f(x_N))$, have a joint Gaussian distribution \citep{rasmussen38gaussian}.
A \gp{} is completely specified by its mean function, $\mu(x)=\mathbb{E}(f(x))$ and kernel (or covariance) function $\kernel(x,x') = \Cov(f(x),f(x'))$.
It is common practice to assume zero mean, since marginalizing over an unknown mean function can be equivalently expressed as a zero-mean \gp{} with a new kernel.
The structure of the kernel captures high-level properties of the unknown function, $f$, which in turn determines how the model generalizes or extrapolates to new data.
We can therefore define a language of regression models by specifying a language of kernels.

\subsection{Useful properties of Gaussian process models}

\begin{itemize}
\item {\bf Tractable inference} Given a kernel function, the posterior distribution can be computed exactly in closed form.  This is a rare property for nonparametric models to have.
\item {\bf Expressivity} by choosing different covariance functions, we can express a very wide range of modeling assumptions.
\item {\bf Integration over hypotheses} the fact that a \gp{} posterior lets us exactly integrate over a wide range of hypotheses means that overfitting is less of an issue than in comparable model classes - for example, neural nets.
\item {\bf Marginal likelihood} A side benefit of being able to integrate over all hyoptheses is that we compute the \emph{marginal likelihood} of the data given the model.  This gives us a principled way of comparing different Gaussian process models.
\item {\bf Closed-form posterior} The posterior predictive distribution of a \gp{} is another \gp{}.  This means that \gp{}s can easily be composed with other models or decision procedures.  For example, \NA{Carl's reinforcement learning work.}
\end{itemize}


\begin{figure}[t]
\begin{centering}
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-0} & 
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-1} \\
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-2} & 
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-3}
\end{tabular}
\end{centering}
\caption[One-dimensional Gaussian process posterior]{A visual representation of a one-dimensional Gaussian process posterior.
Red isocountours show the marginal density at each input location.
Coloured lines are samples from the posterior.}
\label{fig:gp-post}
\end{figure}

Figure \ref{fig:gp-post} shows a Gaussian process posterior.  Typically, it's rendered with the mean and +- 2SD, but there's nothing special about mean.





\subsection{Why assume zero-mean?}

It is common practice to assume zero mean, since marginalizing over an unknown mean function can be equivalently expressed as a zero-mean \gp{} with a new kernel.





\section{Latent Variable Models}

Besides being useful for modeling functions, a simple extension allows \gp{}s to be useful for general density modeling.  



Unfortunately, this extension causes many of the useful properties of the \gp{} not to hold.

The \gplvm{} can also be thought of as a method for modeling the covariance matrix between all rows of $Y$ using a number of parameters which grows linearly with $N$.


\begin{figure}[t]
\begin{centering}
\includegraphics[width=\textwidth]{\gplvmfiguresdir/gplvm_1d_draw_9}
\end{centering}
\caption[One-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.  Bottom: density and samples from a 1D Gaussian, specifying the distribution $p(\vX)$ in the latent space.  Top Right: A function drawn from a \gp{} prior.  Left: A nonparametric density defined by warping the latent density through the function drawn from a \gp{} prior.}  
\label{fig:oned-gplvm}
\end{figure}



\begin{figure}
\begin{centering}
{\begin{tabular}{cccc}
\phantom{h} & Latent space $p(\vX)$ & & Observed space $p(\vY)$ \\
& \fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-latent-crop}} &
\raisebox{7em}{$\overset{\mathlarger{f(x)}}{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\rightarrow}}}}}}}}$} &
%\raisebox{7em}{\Huge $\overset{\textnormal{\Large{GP}}}\rightarrow$} &
\fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-draw-crop}}
\end{tabular}}
\end{centering}
\caption[Two-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.  Left:  Isocontours and samples from a 2D Gaussian, specifying the distribution $p(\vX)$ in the latent space.  Right: Density and samples from a nonparametric density defined by warping the latent density through a function drawn from a GP prior.}  
\label{fig:twod-gplvm}
\end{figure}




\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


