\input{common/header.tex}
\inbpdocument

\chapter{Introduction}
\label{ch:intro}


%\begin{quotation}
%``I only work on intractable nonparametrics - Gaussian processes don't count.'' \\
%\hspace*{\fill} Sinead Williamson, personal communication
%\end{quotation}


\begin{quotation}
``All models are wrong, but yours are stupid too.'' \\
\hspace*{\fill} \citet{mlhipster}
\end{quotation}

Prediction, extrapolation, and induction can all be expressed as learning a function from data.
A general recipe for learning from data is to perform \emph{inference} - to choose a hyopthesis or to weight a set of hypotheses, based on how compatible they are with the data.
To do inference, we start with a weighted set of hypotheses - a \emph{model}.
To be able to learn a wide variety of types of functions, we'd like to have an expressive language of models of functions, which can represent both simple parametric functions, such as linear or polynomial, and also complex nonparametric functions specified in terms of properties such as smoothness or periodicity.
Fortunately, Gaussian processes (\gp{}s) provide a very general and analytically tractable way of learning many different classes of functions.
This chapter will introduce the basic properties of \gp{}s.

Once we have a rich enough language for expressing functions, the remaining question becomes:
Which particular model will work well on my problem?
What sort of structure should I put in my model?
The next chapter will describe the many types of functions that we know how to model using \gp{}s.




\section{Gaussian Process Models}

%Gaussian processes are a flexible and tractable prior over functions, useful for solving regression and classification tasks \citep{rasmussen38gaussian}.
Gaussian processes are a simple and general class of nonparametric models of functions.
%
\begin{figure}[t]
\begin{centering}
\begin{tabular}{cc}
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-0} & 
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-1} \\
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-2} & 
\includegraphics[width=0.48\textwidth]{\introfigsdir/fuzzy-3}
\end{tabular}
\end{centering}
\caption[One-dimensional Gaussian process posterior]
{A visual representation of a one-dimensional Gaussian process posterior.
Different shades of red correspond to deciles of the predictive density at each input location.
Coloured lines show samples from the process.
Top left: A \gp{} not conditioned on any datapoints.
The remaining plots show the posterior after conditioning on different amounts of data.
}
\label{fig:gp-post}
\end{figure}
%
%Figure \ref{fig:gp-post} shows a Gaussian process distribution, as it is conditioned on more and more observations.
%Typically, it's rendered with the mean and +- 2SD, but there's nothing special about mean.
%
To be precise, a \gp{} is any distribution over functions such that any finite subset of function evaluations $f(\vx_1), f(\vx_2), \ldots f(\vx_N)$, have a joint Gaussian distribution \citep{rasmussen38gaussian}.
A \gp{} model, before conditioning on data, is completely specified by its mean function,
%
\begin{align}
%\mathbb{E}(f(x)) = \mu(x)
\expectargs{}{f(\vx)} = \mu(\vx)
\end{align}
%
and its covariance function, also called the \emph{kernel}:
%
\begin{align}
\Cov(f(\vx),f(\vx')) = \kernel(\vx,\vx')
\end{align}
%
It is common practice to assume zero mean, since marginalizing over an unknown mean function can be equivalently expressed as a zero-mean \gp{} with a new kernel, or by subtracting the mean function from the data.

After accounting for the mean, the kind of structure which can be captured by a \gp{} model is entirely determined by its kernel, which in turn determines how the model generalizes, or extrapolates to new data.

There are many possible choices of covariance function, and we can specify a wide range of models just by specifying the kernel.
Examples of commonly used models not usually cast as \gp{}s are linear regression, splines, 
%some forms of generalized additive models,
and Kalman filters.
However, these model classes barely scratch the surface of the wide variety of possible \gp{} models.
%In fact, any symmetric positive-definite function can be used to construct a \gp{} model.
%The benefit to this flexibility is that 
%We can therefore define a language of regression models by specifying a language of kernels.
%
One of the main difficulties in using \gp{}s is constructing a kernel which represents the structure present in the data.


Gaussian processes can be seen as a dual representation of Bayesian linear regression. \NA{Cite}
By moving into this dual space, we pay a price in computational complexity, but gain the ability to model functions to any desired level of detail.
Crucially, the marginal likelihood allows us to automatically discover the appropriate amount of detail to use, by Bayesian Occam's razor \citep{rasmussen2001occam,mackay2003information}.

To be concrete, here's the marginal likelihood under a \gp{} of observing a set of function values $\left[ f(\vx_1), f(\vx_2), \ldots f(\vx_N)  \right] = \vf(\vX)$ at locations given by the rows of $\vX$:
%
\begin{align}
p(\funcval | \vX, \mu(\cdot), k(\cdot, \cdot)) & = \N{\funcval}{\mu(\vX)}{k(\vX, \vX)} \nonumber\\
& = (2\pi)^{-\frac{N}{2}}|k(\vX, \vX)|^{-\frac{1}{2}}\, \exp \left\{ -\frac{1}{2} \left( \funcval - \boldsymbol\mu(\vX) \right)\tra k(\vX, \vX)\inv \left(\funcval - \boldsymbol\mu(\vX) \right) \right\}
\label{eq:gp_marg_lik}
\end{align}
%
This Gaussian likelihood is referred to as the \emph{marginal} likelihood because it implicitly integrates over all possible functions values $\vf( \bar \vX)$, where $\bar \vX$ is the set of all locations where we don't have any observations.
However, even though we don't need to consider any other locations when computing the likelihood, we can still ask the model which function values are likely to occur at other locations, given that the observations we've seen.
The predictive distribution at a test point $\testpoint$ has a simple form:
%
\begin{align}
p( f(\testpoint) | \funcval, \vX, \mu(\cdot), k(\cdot, \cdot)) 
= \mathcal{N} \big( f(\testpoint) | & \mu(\testpoint) + k(\testpoint, \vX) k(\vX, \vX)\inv \left( \funcval - \mu(\vX) \right)  \nonumber \\
& k(\testpoint, \testpoint) - k(\testpoint, \vX) k(\vX, \vX)\inv k(\vX, \testpoint) \big)
\label{eq:predictive}
\end{align}
%
These equations may look complex, but only require a few matrix operations to evaluate.

Since the marginal likelihood depends on the kernel, we can select the form of the covariance function, or its parameters, by maximum likelihood, or through inference in a Bayesian model.

Sampling from a \gp{} is also straightforward: a sample from a \gp{} is just a single sample from a single multivariate Normal distribution \eqref{eq:predictive}.
Figure~\ref{fig:gp-post} shows prior and posterior samples from a \gp{}.


%[explain how that works]
% Insert neural net stuff here?



\subsection{Useful Properties of Gaussian Processes}

\begin{itemize}

\item {\bf Analytic inference}
Given a kernel function and some observations, the predictive posterior distribution can be computed exactly in closed form.  This is a rare property for nonparametric models to have.

\item {\bf Expressivity}
By choosing different covariance functions, we can express a very wide range of modeling assumptions.

\item {\bf Integration over hypotheses}
The fact that a \gp{} posterior lets us exactly integrate over a wide range of hypotheses means that overfitting is less of an issue than in comparable model classes.
It also removes the need for sophisticated optimization schemes.
%
In contrast, much of the neural network literature is devoted to techniques for regularization and optimization.

\item {\bf Marginal likelihood}
A side benefit of being able to integrate over all hyoptheses is that we compute the \emph{marginal likelihood} of the data given the model.
This gives us a principled way of comparing different Gaussian process models.

\item {\bf Closed-form posterior}
The posterior predictive distribution of a \gp{} is another \gp{}.
This means that \gp{}s can easily be composed with other models or decision procedures.
For example, in reinforcement learning applications, 

\item {\bf Easy to Analyze}
%\subsection{Why stick to such a limited model class?}
It may seem unsatisfying to restrict ourselves to a limited model class.
Shouldn't we instead learn to use some more flexible model class, such as the set of all computible functions?
The answer is: simple models can be used as well-understood building blocks for constructing more interesting models in diverse settings.

Consider linear models.
Although they form an extremely limited model class, they are fast, simple, and easy to analyze.
This makes them easy to incorporate into other models or procedures.
Linear models may seem like a hopelessly simple model class, but they're arguably the most useful modeling tools in existence.

\end{itemize}




\subsection{Limitations of Gaussian Processes}

\begin{itemize}

\item {\bf Slow inference}
Computing the matrix inverse in \eqref{eq:gp_marg_lik} and \eqref{eq:predictive} takes $\mathcal{O}(N^3)$ time.
%and $\mathcal{O}(N^2)$ memory.  
Fortunately, this problem can be addressed by approximate inference schemes, and most \gp{} software packages implement several of these.

\item {\bf Light tails}
We may wish to use non-Gaussian noise models, for instance in order to be robust to outliers, or to perform classification, or some other form of structured prediction.
Using non-Gaussian noise models requires approximate inference schemes; fortunately, mature software packages exist which can automatically perform approximate inference for a wide variety of likelihoods.
%\item {\bf Symmetric}
%The predictive distribution of a \gp{} is always symmetric about its mean function.
%This means that \gp{} models are inappropriate for modeling non-negative functions, such as likelihoods.
%However, this problem can be addressed by simply exponentiating a \gp{} model, giving rise to a \emph{log-Gaussian process}.

\item {\bf The need to choose a kernel}
In practice, the extreme flexibility of \gp{} models means that we are also faced with the difficult task of choosing a kernel.
In fact, 
%because the model only depends on the inputs through the kernel, 
choosing a useful kernel is equivalent to the problem of learning good features for the data.
Typically, human experts choose from among a small set of standard kernels.
In this thesis, we hope to go some way towards automating the construction and selection of useful kernels.
\end{itemize}





\section{Outline and Contributions of Thesis}

This thesis presents a set of related results about how the probabilistic nature of Gaussian process models allows them to be easily extended or composed with other models.
Furthermore, the fact that the marginal likelihood is often available (or easily approximable) means that we can evaluate how much evidence the data provides for one structure over another.

\paragraph{Chapter \ref{ch:kernels}} contains a wide-ranging overview of many of the types of structured priors on functions that can be easily expressed by constructing appropriate covariance functions.
%describes, in detail, the ways in which different sorts of structure can be introduced into a \gp{} model through the kernel.
%TODO: Expand this description.
For example, in chapter~\ref{ch:kernels}, we'll see how \gp{}s can be combined with latent variable models to produce models of nonparametric manifolds.
By introducing structure into the kernels of those \gp{}s, we can create manifolds with diverse topological structures, such as cylinders, torii and M\"obius strips.

Given a wide variety of structures, plus the ability to evaluate the suitability of each one, it's straightforward to automatically search over models.
{\bf Chapter \ref{ch:grammar}} shows how to construct a general, open-ended language over kernels - which implies a corresponding language over models.
In chapter~\ref{ch:grammar}, we'll see how the marginal likelihood can guide us into automatically building structured models, and how capturing structure allows us to extrapolate, rather than simply interpolating.

Another benefit of using a relatively simple model class is that the resulting models are easy to understand.
{\bf Chapter~\ref{ch:description}} demonstrates how easy-to-understand the resulting models are, by demonstrating a simple system which automatically describes the structure discovered in a dataset by a search over \gp{} models.
This system automatically generates reports with graphs and english-language descriptions of \gp{} models.
Chapter \ref{ch:description} shows that, for the particular language of models constructed in chapter \ref{ch:grammar}, it's relatively easy to automatically generate english-language descriptions of the models discovered.
Augmented with interpretable plots decomposing the predictive posterior, we demonstrate how to automatically generate useful analyses of time-series.
Combined with the automatic model search developed in chapter \ref{ch:grammar}, this system represents the beginnings of an ``automatic statistician''.
We discuss the advantages and potential pitfalls of automating the modeling process in this way.

{\bf Chapter \ref{ch:additive}} examines the model class obtained by performing dropout in \gp{}s, finding them to have equivalent covariance to \emph{additive Gaussian processes}, a model summing over exponentially-many \gp{} models, each depending on a different subset of the input variables.  An polynomial-time algorithm for doing inference in this model class is given, and the resulting model class is characterized and related to existing model classes.

{\bf Chapter \ref{ch:warped}} develops an extension of the \gplvm{} in which the latent distribution is a mixture of Gaussians.  This model gives rise to a Bayesian clustering model in the clusters have nonparametric shapes.  Like the density manifolds learned by the \gplvm{}, the shapes of the clusters learned by the \iwmm{} follow the contours of the data density.

Besides having a dual representation as linear regression, \gp{}s can also be derived as the limit of an infinitely-wide neural network.
As an example of using \gp{}s as a simple-to-understand building block, {\bf Chapter \ref{ch:deep-limits}} analyzes deep network models by characterizing the prior over functions obtained by composing \gp{} priors to form \emph{deep Gaussian processes}.
%, and relates them to existing neep neural network architecures.
We find that, as the number of layers in such models increases, the amount of information retained about the original input diminshes to a single degree of freedom.
We show that a simple change to the network architecture fixes this pathology.


\section{Attribution}

This thesis was made possible (and enjoyable to produce) by the substantial contributions of the many co-authors I was fortunate to work with.
In this section, I %detail the novel contributions of this thesis, and 
attempt to give proper credit to my tireless co-authors.

\paragraph{Structure through kernels}
%Chapter \ref{ch:kernels} contains a wide-ranging overview of many of the types of structured priors on functions that can be easily expressed by constructing appropriate covariance functions.
Section \ref{sec:deep_kernels} of chapter \ref{ch:kernels}, describing how symmetries in the kernel of a \gplvm{} give rise to priors on manifolds with interesting topologies, is based on a collaboration with David Reshef, Roger Grosse, Josh Tenenbaum, and Zoubin Ghahramani.

\paragraph{Structure Search}
The research upon which Chapter \ref{ch:grammar} is based was done in collaboration with James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, and Zoubin Ghahramani, and was published in \citep{DuvLloGroetal13}, where James Lloyd was joint first author.
Myself, James Lloyd and Roger Grosse jointly developed the idea of searching through a grammar-based language of \gp{} models, inspired by \citet{grosse2012exploiting}, and wrote the first versions of the code together.
James Lloyd ran almost all of the experiments.
 Carl Rasmussen, Zoubin Ghahramani and Josh Tenenbaum provided many conceptual insights, as well as suggestions about how the resulting procedure could be most fruitfully applied.

\paragraph{Automatic Statistician} The work appearing in chapter \ref{ch:description} was written in collaboration with James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum, Zoubin Ghahramani, and was published in \citep{LloDuvGroetal14}.
The idea of the correspondence between kernels and adjectives grew out of discussions between James and myself.
James Lloyd wrote most of the code to automatically generate reports, and ran all of the experiments.
The text was written mainly by myself, James Lloyd, and Zoubin Ghahramani, with many helpful contributions and suggestions from Roger Grosse and Josh Tenenbaum.

\paragraph{Additive Gaussian processes}
The work in chapter \ref{ch:additive} discussing additive \gp{}s was done in collaboration with Hannes Nickisch and Carl Rasmussen, who developed a richly parameterized kernel which efficiently sums all possible products of input dimensions.
My role in the project was to examine the properties of the resulting model, clarify the connections to existing methods, and to create all figures and run all experiments.
This work was previously published in \citep{duvenaud2011additive11}.

\paragraph{Warped Mixtures}
The work comprising the bulk of chapter \ref{ch:warped} was done in collaboration with Tomoharu Iwata and Zoubin Ghahramani, and appeared in \citep{IwaDuvGha12}.
Specifically, the main idea was borne out of a conversation between Tomo and myself, and together we wrote almost all of the code together as well as the paper.
Tomo ran most of the experiments.
Zoubin Ghahramani provided initial guidance, as well as many helpful suggestions throughout the project.


\paragraph{Deep Gaussian Processes}
The ideas contained in chapter \ref{ch:deep-limits} were developed through discussions with Oren Rippel, Ryan Adams and Zoubin Ghahramani, and appear in \citep{DuvRipAdaGha14}.  The derivations, experiments and writing were done mainly by myself, with many helpful suggestions by my co-authors.

\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


