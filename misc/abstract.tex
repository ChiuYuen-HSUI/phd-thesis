% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
This thesis develops a method for automatically constructing, visualizing and describing a large class of models, useful for forecasting and finding structure in domains such as time series, geological formations, and physical dynamics.
These models, based on Gaussian processes, can capture many types of statistical structure, such as periodicity, changepoints, additivity, and symmetries.
Such structure can be encoded through \emph{kernels}, which have historically been hand-chosen by experts.
We show how to automate this task, creating a system that explores an open-ended space of models and reports the structures discovered.


To automatically construct Gaussian process models, we search over sums and products of kernels, maximizing the approximate marginal likelihood.
We show how any model in this class can be automatically decomposed into qualitatively different parts, and how 
%every possible 
each component can be visualized and described through text.
%We show how such models can be automatically decomposed, visualized, and described.
%Combining these results, we present a procedure that takes in a dataset and outputs an automatically constructed model, along with a detailed report with plots and automatically generated text that illustrate the qualitatively different, and sometimes novel, types of structure discovered in the data.
We combine these results into a procedure that, given a dataset, automatically constructs a model along with a detailed report containing plots and generated text that illustrate the structure discovered in the data.

The introductory chapters contain a tutorial showing how to express many types of structure through kernels, and how adding and multiplying different kernels combines their properties.
Examples also show how symmetric kernels can produce priors over topological manifolds such as cylinders, toruses, and M\"{o}bius strips, as well as their higher-dimensional generalizations.




%Models can automatically be constructed by searching over sums and products of kernels, maximizing the approximate marginal likelihood of the data.
%We show how such models can be automatically decomposed into qualitatively different types of structure, how these components can be automatically visualized, and finally how they can be automatically described using text.

%Later chapters present a procedure which, given a dataset, outputs an automatically-constructed model, along with a detailed report.
%These reports contain plots and automatically generated text that illustrate the qualitatively different, and sometimes novel, types of structure discovered in the data.
%Models are automatically constructed by searching over sums and products of kernels, maximizing approximate marginal likelihood.

%We show how such models can be automatically decomposed, visualized, and described.
%Combining these results, we present a procedure which takes in a dataset and outputs an automatically-constructed model, along with a detailed report.
%These reports contain plots and automatically generated text that illustrate the qualitatively different, and sometimes novel, types of structure discovered in the data.

%To automatically construct Gaussian process models, we search over sums and products of kernels, maximizing approximate marginal likelihood.
%This model-building procedure combines the properties of existing kernels to produce possibly novel statistical structures.
%We show how such models can be decomposed automatically into qualitatively different parts, and how each component can automatically be visualized and described through text.
%Combining these results, we present a procedure which takes in a dataset and outputs an automatically-constructed model and a detailed report.
%These reports contain plots and text illustrating each of the types of structure discovered in the data.


This thesis also explores several extensions to Gaussian process models.
First, building on existing work that relates Gaussian processes and neural nets, we analyze natural extensions of these models to \emph{deep kernels} and \emph{deep Gaussian processes}.
%Second, we examine models which efficiently sum over functions of all combinations of input variables, connecting this model class to the regularization method of \emph{dropout}.
Second, we examine \emph{additive Gaussian processes}, showing their relation to the regularization method of \emph{dropout}.
Third, we combine Gaussian processes with the Dirichlet process to produce the \emph{warped mixture model}: a Bayesian clustering model having nonparametric cluster shapes, and a corresponding latent space in which each cluster has an interpretable parametric form.
\end{abstract}
