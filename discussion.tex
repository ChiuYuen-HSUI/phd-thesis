\input{common/header.tex}
\inbpdocument

\chapter{Discussion}
\label{ch:discussion}


\section{Why Assume Zero Mean?}

In literature, as well as in practice, it is common to construct \gp{} priors with a zero mean function.
This might seem strange, since it is presumably a good place to put prior information, or if we are comparing models, to express  since marginalizing over an unknown mean function can be equivalently expressed as a different \gp{} with zero-mean, and another term added to the kernel.


\subsection{Marginalizing Out the Mean Function}

Additivity also helps us to avoid explicitly representing the mean function of a \gp{}, in two ways.
First of all, a known mean function can be moved into the covariance function.
Specifically, if we wish to model an unknown function $f(\vx)$ with known mean $m(\vx)$, where the mean has unknown magnitude $c \sim \Nt{0}{\sigma^2_c}$, we can equivalently express this model using another \gp{} with zero mean:
%$related modelquivalently (\vx) a(\vx)$ with $f \sim \GPt{0}{k(\vx,\vx')}$, then this 
%
\begin{align}
f \sim \GPt{c m(\vx)}{k(\vx,\vx')}, \quad c \sim \Nt{0}{\sigma^2_c}
\iff f \sim \GPt{ \vzero }{ c^2 m(\vx) m(\vx') + k(\vx,\vx')}
\end{align}

By moving the mean function into the covariance function, we get the same model as before, but we can integrate over the magnitude $c$ of the mean function at no additional cost.

Second, a mean function with an unknown form can also be expressed through an extra term in the covariance function.
Specifically, if express our ignorance about the mean function through a \gp{} prior, then that's the same model as a \gp{} whose kernel is a sum of two terms:
%
\begin{align}
m \sim \GPt{\vzero}{k_a(\vx,\vx')}, \quad
f \sim \GPt{m(\vx)}{k_b(\vx,\vx')}
\iff 
f \sim \GPt{\vzero}{k_a(\vx,\vx') + k_b(\vx,\vx')}
\end{align}



\paragraph{A known mean function can be moved into the covariance function}
Specifically, if we wish to model an unknown function $f(\vx)$ with known mean $m(\vx)$, (with unknown magnitude $c \sim \Nt{0}{\sigma^2_c}$), we can equivalently express this model using another \gp{} with zero mean:
%$related modelquivalently (\vx) a(\vx)$ with $f \sim \GPt{0}{k(\vx,\vx')}$, then this 
%
\begin{align}
f \sim \GPt{c m(\vx)}{k(\vx,\vx')}, \quad c \sim \Nt{0}{\sigma^2_c}
\iff f \sim \GPt{ \vzero }{ c^2 m(\vx) m(\vx') + k(\vx,\vx')}
\end{align}

%This correspondence means that, b
By moving the mean function into the covariance function, we get the same model, but we can integrate over the magnitude of the mean function at no additional cost.
This is one advantage of moving as much structure as possible into the covariance function.
%In fact, we can view \gp{} regression as simply implicitly integrating over the magnitudes of (possibly uncountably many) different mean functions all summed together.
%TODO: provide a link to the GPs as neural nets discussion.

\paragraph{An unknown mean function can be moved into the covariance function}

If we wish to express our ignorance about the mean function, one way would be by putting a \gp{} prior on it.
%
\begin{align}
m \sim \GPt{\vzero}{k_1(\vx,\vx')}, \quad
f \sim \GPt{m(\vx)}{k_2(\vx,\vx')}
\iff 
f \sim \GPt{\vzero}{k_1(\vx,\vx') + k_2(\vx,\vx')}
\end{align}


\section{Why Not Learn the Mean Function Instead?}
One might ask: besides integrating over the magnitude, what is the advantage of moving the mean function into the covariance function?
After all, mean functions are certainly more interpretable than a posterior distribution over functions.

Instead of searching over a large class of covariance functions, which seems strange and unnatural, we might consider simply searching over a large class of structured mean functions, assuming a simple \iid noise model.
This is the approach taken by practically every other regression technique: neural networks, decision trees, boosting, \etc.
If we could integrate over a wide class of possible mean functions, we would have a very powerful learning an inference method.
The problem faced by all of these methods is the well-known problem \emph{overfitting}.
If we are forced to choose just a single function with which to make predictions, we must carefully control the flexibility of the model we learn, generally prefering ``simple'' functions, or to choose a function from a restricted set.

If, on the other hand, we are allowed to keep in mind many possible explanations of the data, \emph{there is no need to penalize complexity}. [cite Occam's razor paper?]
The power of putting structure into the covariance function is that doing so allows us to implictly integrate over many functions, maintaining a posterior distribution over infinitely many functions, instead of choosing just one.
In fact, each of the functions being considered can be infinitely complex, without causing any form of overfitting.
For example, each of the samples shown in figure \ref{fig:gp-post} varies randomly over the whole real line, never repeating, each one requiring an infinite amount of information to describe.
Choosing the one function which best fits the data will almost certainly cause overfitting.
However, if we integrate over many such functions, we will end up with a posterior putting mass on only those functions which are compatible with the data.
In other words, the parts of the function that we can determine from the data will be predicted with certainty, but the parts which are still uncertain will give rise to a wide range of predictions.

To repeat: \emph{there is no need to assume that the function being modeled is simple, or to prefer simple explanations} in order to avoid overfitting, if we integrate over many possible explanations rather than choosing just the one.

In Chapter~\ref{ch:grammar}, we will compare a system which estimates a parametric covariance function against one which estimates a parametric mean function.


\section{Signal versus Noise}

In most derivations of Gaussian processes, the model is given as
%
\begin{align}
y = f(\inputVar) + \epsilon, \quad \textnormal{where} \quad \epsilon \simiid \Nt{0}{\sigma^2_{\textnormal{noise}}}
\end{align}

However, $\epsilon$ can equivalently be thought of as another Gaussian process, and so this model can be written as $y(\inputVar) \sim \GP\left(0, k + \delta \right)$.  The lack of a hard distinction between the noise model and the signal model raises the larger question:  Which part of a model represents signal, and which represents noise?

We believe that it depends on what you want to do with the model - there is no hard distinction between signal and noise in general.

For example: often, we don't care about the short-term variations in a function, and only in the long-term trend.
However, in many other cases, we with to de-trend our data to see more clearly how much a particular part of the signal deviated from normal.

%\subsubsection{Student's $t$ processes}

%One shortcoming of the



%\section{Why does everyone use squared-exp?}
%In some instances, using a 'default' kernel yields acceptable performance.
%Many frequentist methods assume a characteristic kernel, such as the squared-exp.
%This choice is motivated by the fact that, in the limit of infinite data, and a shrinking lengthscale, the estimate of the function will converge asymptotically to the truth. [citation needed]



%\section{Why haven't structured kernels been built for SVMs?}

%Because without marginal likelihood to tell you which structure is present in your data, it's not clear how to choose which kernel to use without cross-validation.


%\section{Automating Statistics}

%Automating the process of statistical modeling would have a tremendous impact on fields that currently rely on expert statisticians, machine learning researchers, and data scientists.
%While fitting simple models (such as linear regression) is largely automated by standard software packages, there has been little work on the automatic construction of flexible but interpretable models. 


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}

