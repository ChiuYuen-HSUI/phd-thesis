\input{common/header.tex}
\inbpdocument

\chapter{Conclusions and Discussion}
\label{ch:discussion}


\section{Summary of Contributions}

The main contribution of this thesis is to show how to automate the construction of interpretable nonparametric models using Gaussian processes.
This was done in several parts.
First, \cref{ch:kernels} systematically described many different kernel construction techniques, and demonstrated properties of the corresponding \gp{} priors.
Next, \cref{ch:grammar} showed how to automatically search over an open-ended space of \gp{} models, and that those models could be automatically decomposed into diverse parts showing the structure found in the data.
\Cref{ch:description} showed that the effect each part of a kernel can be described modularly, allowing automatically written text to be included in detailed reports describing \gp{} models.
An example report is included in \cref{ch:example-solar}.
Together, these chapters describe the beginnings of an ``automatic statistician'', capable of the sort of model construction and description currently performed only by experts.

The second half of this thesis develops several extensions to Gaussian processes that can automatically determine modeling choices that were previously set by trial and error or cross-validation.
\Cref{ch:deep-limits} contains theorems and visualizations characterizing deep Gaussian processes, relates them to existing deep neural networks, and derives novel deep kernels.
\Cref{ch:additive} investigates additive \gp{}s, a family of structured models consisting of sums of functions of all combinations of input variables, which are shown to have the same covariance as a \gp{} using dropout regularization.
\Cref{ch:warped} extends the \gp{} latent variable model into a Bayesian clustering model which automatically infers nonparametric shape of each cluster, as well as the number of clusters.



\section{Structured versus Unstructured \sgp{} Models}

One question left unanswered by this thesis is when to prefer the structured kernel based models described \cref{ch:kernels,ch:grammar,ch:additive} to the relatively unstructured deep \gp{} models described in \cref{ch:deep-limits}.

The warped mixture model of \cref{ch:warped} represents a compromise between these two approaches, combining a discrete clustering model with an unstructured warping function.
However, the results of \citet{damianou2012deep} suggest that clustering can be automatically accomplished by a sufficiently deep, unstructured \gp{}.

\subsubsection{Difficulty of Optimization}
The discrete nature of the search over composite kernel structures can be seen as a blessing and a curse.
Certainly, a mixed discrete and continuous optimization requires more complex procedures compared to the continuous-only optimization possible in deep \gp{}s.

However, the discrete nature of the space of composite kernels offers the possibility of learning heuristics to suggest which types of structure to add.
For example, finding periodic structure or growing variance in the residuals of a model suggests adding periodic or linear components to the kernel, respectively.
It is not clear whether such heuristics can easily be found for optimizing the variational parameters of a deep \gp{}.

\subsubsection{Extrapolation}
Another question is whether, and how, an equally rich inductive bias can be encoded into relatively unstructured models such as deep \gp{}s.
As an example, consider the problem of extrapolating a periodic function.
A deep \gp{} could learn a latent representation similar to that of the periodic kernel, projecting into a basis eqeuivalent to $[\sin(x), \cos(x)]$ in the first hidden layer.
However, to extrapolate a periodic function, the $\sin$ and $\cos$ functions would have to continue to repeat beyond the range of the training data, which would not happen if each layer assumed only local smoothness.

One obvious possibility is to marry the two approaches, learning deep \gp{}s with structured kernels.
However, we may lose some of the advantages of interpretability by this approach.

Another point to consider is that, in high dimensions, the line between interpolation and extrapolation is blurred, and that learning a suitable representation of the data manifold may be sufficient for most purposes.

\subsubsection{Ease of Interpretation}
\cref{ch:description} showed that composite kernels allow automatic visualization and desription of low-dimensional structure.
On the other hand, \citet{damianou2012deep} showed that deep \gplvm{}s allow summarization of high-dimensional structure through showing samples from the posterior, examining the dimension of each latent layer, visualizing latent coordinates, or examining how the predictive distribution changes as one moves in the latent space.

\section{Automating Machine Learning and Statistics}

It seems clear that, one way or another, large parts of the existing practice of model-building will eventually be automated.
%The machine learning community has so far focused largely on producing efficient inference strategies for powerful model classes, which is sufficient for improving predictive performance.
Historically, the statistics community has put much more emphasis on the interpretabilty and meaning of models than the machine learning community, which has focused more on predictive performance.
To automate the practice of statistics, developing model-description procedures for powerful model classes seems like the direction with the most low-hanging fruit.




\iffalse
\section{Why Assume Zero Mean?}

\section{Why Not Learn the Mean Function Instead?}
One might ask: besides integrating over the magnitude, what is the advantage of moving the mean function into the covariance function?
After all, mean functions are certainly more interpretable than a posterior distribution over functions.

Instead of searching over a large class of covariance functions, which seems strange and unnatural, we might consider simply searching over a large class of structured mean functions, assuming a simple \iid noise model.
This is the approach taken by practically every other regression technique: neural networks, decision trees, boosting, \etc.
If we could integrate over a wide class of possible mean functions, we would have a very powerful learning an inference method.
The problem faced by all of these methods is the well-known problem \emph{overfitting}.
If we are forced to choose just a single function with which to make predictions, we must carefully control the flexibility of the model we learn, generally preferring ``simple'' functions, or to choose a function from a restricted set.

If, on the other hand, we are allowed to keep in mind many possible explanations of the data, \emph{there is no need to penalize complexity}. [cite Occam's razor paper?]
The power of putting structure into the covariance function is that doing so allows us to implicitly integrate over many functions, maintaining a posterior distribution over infinitely many functions, instead of choosing just one.
In fact, each of the functions being considered can be infinitely complex, without causing any form of overfitting.
For example, each of the samples shown in \cref{fig:gp-post} varies randomly over the whole real line, never repeating, each one requiring an infinite amount of information to describe.
Choosing the one function which best fits the data will almost certainly cause overfitting.
However, if we integrate over many such functions, we will end up with a posterior putting mass on only those functions which are compatible with the data.
In other words, the parts of the function that we can determine from the data will be predicted with certainty, but the parts which are still uncertain will give rise to a wide range of predictions.

To repeat: \emph{there is no need to assume that the function being modeled is simple, or to prefer simple explanations} in order to avoid overfitting, if we integrate over many possible explanations rather than choosing just the one.

In Chapter~\ref{ch:grammar}, we will compare a system which estimates a parametric covariance function against one which estimates a parametric mean function.


\section{Signal versus Noise}

In most derivations of Gaussian processes, the model is given as
%
\begin{align}
y = f(\inputVar) + \epsilon, \quad \textnormal{where} \quad \epsilon \simiid \Nt{0}{\sigma^2_{\textnormal{noise}}}
\end{align}

However, $\epsilon$ can equivalently be thought of as another Gaussian process, and so this model can be written as $y(\inputVar) \sim \GP\left(0, k + \delta \right)$.  The lack of a hard distinction between the noise model and the signal model raises the larger question:  Which part of a model represents signal, and which represents noise?

We believe that it depends on what you want to do with the model - there is no hard distinction between signal and noise in general.

For example: often, we do not care about the short-term variations in a function, and only in the long-term trend.
However, in many other cases, we wish to de-trend our data to see more clearly how much a particular part of the signal deviated from normal.

\fi



%\section{Automating Statistics}

%Automating the process of statistical modeling would have a tremendous impact on fields that currently rely on expert statisticians, machine learning researchers, and data scientists.
%While fitting simple models (such as linear regression) is largely automated by standard software packages, there has been little work on the automatic construction of flexible but interpretable models. 


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}

