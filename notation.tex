\input{common/header.tex}
\inbpdocument

\chapter*{Notation}
\label{ch:notation}
\addcontentsline{toc}{chapter}{Notation}

%Throughout this thesis we use Roman letters in place of greek letters wherever possible.

Unbolded $x$ represents a single number, $\vx$ represents a vector, and $\vX$ represents a matrix.
A individual element of a vector will be denoted with a subscript and without boldface.
For example, the $i$th element of a vector $\vx$ is denoted as $x_i$.
A bold lower-case number with an index such as $\vx_j$ represents a particular row of matrix $\vX$.

\vspace{1cm}

\begin{tabular}{lm{12cm}}
Symbol \quad     & Description \\
\hline
$\feat(\vx)$       & A feature vector. \\
$\mathcal{O}(\cdot)$ & The big-O asymptotic complexity of an algorithm. \\
$A \otimes B$ & The Kronecker product of matrices $A$ and $B$. \\
%$\vecf$ & A function represented as an infinite-dimensional vector. \\
$\kSE$ & The squared-exponential kernel, also known as the radial-basis function (RBF) kernel, or the Gaussian kernel. \\
$\kRQ$ & The rational-quadratic kernel. \\
$\kPer$ & The periodic kernel. \\
$\kLin$ & The linear kernel. \\
$\kWN$ & The white-noise kernel. \\
$\kC$ & The constant kernel. \\
$\vsigma$ & The changepoint kernel, $\vsigma(x, x') = \sigma(x) \sigma(x')$, where $\sigma(x)$ is a sigmoidal function such as the logistic function. \\
$k_1 + k_2$ & Addition of kernels, shorthand for $k_1(x,x') + k_2(\vx, \vx')$ \\
$k_1 \times k_2$& Multiplication of kernels, shorthand for $k_1(\vx, \vx') \times k_2(\vx, \vx')$ \\
$k(\vX, \vX)$ & the Gram matrix, whose $i,j$th element is given by $k(\vx_i, \vx_j)$. \\
$\vK$ & Shorthand for the Gram matrix $k(\vX, \vX)$ \\
$\vf(\vX)$ & A vector of function values, whose $i$th element is given by $f(\vx_i)$.
\end{tabular}



\outbpdocument{
}


