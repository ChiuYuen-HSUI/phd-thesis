\input{common/header.tex}
\inbpdocument

\chapter{Warped Mixture Models}  % Or: extensions of the GP-LVM
\label{ch:warped}

\begin{quotation}
``What, exactly, is a cluster?''

\hspace*{\fill} - Bernhard Sch\"{o}lkopf, personal communication
\end{quotation}

Previous chapters showed how the probabilistic nature of \gp{}s lets us automatically include an appropriate amount of structure, and the right kind, when building models of functions.
In this chapter, we'll show that we can also take advantage of this property when we compose \gp{}s with other models.
Besides simply learning the right amount of complexity of the function modeled by a \gp{}, we can automatically trade off complexity between the \gp{} and the other parts of the model.

We'll consider a simple example: a Gaussian mixture model, warped by a draw from a \gp{}.
This model will produce clusters with arbitrary shapes, depending on the warping.
More generalliy, this model lets us automatically infer the number, dimension, and shape of a set of nonlinear manifolds, and summarize those manifolds in a low-dimensional latent space.
We call the proposed model the {\it infinite warped mixture model} (\iwmm{}).
\Cref{fig:generative} shows a set of manifolds and datapoints sampled from the prior defined by this model.



%The possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters (or density manifolds) describing the data.  
%The number of manifolds, as well as the shape and dimension of each manifold is automatically inferred.
%We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function.  
%We show that our model is effective for density estimation, performs better than infinite Gaussian mixture models at recovering the true number of clusters, and produces interpretable summaries of high-dimensional datasets.



The work comprising the bulk of this chapter was done in collaboration with Tomoharu Iwata and Zoubin Ghahramani, and appeared in \citep{IwaDuvGha12}.
Specifically, the main idea was borne out of a conversation between Tomo and myself, and together we wrote almost all of the code together as well as the paper.
Tomo ran most of the experiments.
Zoubin Ghahramani provided guidance and many helpful suggestions throughout the project.


%We introduce a flexible class of mixture models for clustering and density estimation. Our model 
%describes clusters as nonlinear manifolds, automatically infers the number of clusters, produces a potentially low-dimensional latent representation, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model 
%which analytically integrates out both the mixture parameters and the warping function.  We show that our model is effective for density estimation, and performs better than infinite Gaussian mixture models at discovering meaningful clusters.

%Joint work with Tomoharu Iwata 



%Probabilistic mixture models are often used for clustering.
%However, if the mixture components are parametric (e.g. Gaussian), then the clustering obtained can be heavily dependent on how well each actual cluster can be modeled by a Gaussian.
%For example, a heavy tailed or curved cluster may need many components to model it.
%Thus, although mixture models are widely used for probabilistic clustering, their assumptions are generally inappropriate if the primary goal is to discover clusters in data.
%Dirichlet process mixture models can alleviate the problem of an unknown number of clusters, but this does not address the problem that real clusters may not be well matched by any parametric density.

%\section{Introduction}


%In this chapter, we propose a nonparametric Bayesian model that can find non-linearly separable clusters with complex shapes.
%The proposed model assumes that each observation has coordinates in a latent space, and is generated by warping the latent coordinates via a nonlinear function from the latent space to the observed space.
%By this warping, complex shapes in the observed space can be modeled by simpler shapes in the latent space.

%In the latent space, we assume an infinite Gaussian mixture model~\citep{rasmussen2000infinite}, which allows us to automatically infer the number of clusters.
%
%By using infinite mixture models 
%based on Dirichlet processes~\citep{ferguson1973bayesian},
%we can automatically infer the number of components from the data.
%For the prior on the nonlinear mapping function, we use a \gp{}.
%Gaussian processes~\citep{rasmussen38gaussian}, which enable us to flexibly infer the nonlinear warping function from the data.
%, when the observed and latent spaces are both two-dimensional.

%The proposed model is nonparametric Bayesian in two senses: it uses a Dirichlet process to model the number of mixture components, and Gaussian processes to model the shape of each component.
%To our knowledge this is the first probabilistic generative model for clustering with flexible nonparametric component densities.

%This chapter has the standard structure of a Bayesian nonparametric machine learning paper.
%First, we describe the model in \cref{sec:gplvm,sec:iwmm-definition}.
%Then we derive a Markov chain Monte Carlo (\mcmc{}) inference procedure for the \iwmm{} in \cref{sec:iwmm-inference}.
%\Cref{sec:iwmm-related-work} reviews related work, and \cref{sec:iwmm-experiments} demonstrates the new abilities of the model.

%In particular, we sample the cluster assignments using Gibbs sampling, sample the latent coordinates using Hamiltonian Monte Carlo, and analytically integrate out both the mixture parameters (weights, means and covariance matrices), and the nonlinear warping function. 

%In particular, we iteratively infer the infinite Gaussian mixture model in the latent space by using Gibbs sampling, the latent coordinates and nonlinear function by using the Hamiltonian Monte Carlo.





\section{The Gaussian Process Latent Variable Model}
\label{sec:gplvm}

Besides being useful for modeling functions, a simple extension allows \gp{}s to be useful for general density modeling, at the cost of inference becoming non-analytic.
The \gplvm{} can also be thought of as a method for modeling the covariance between rows of $Y$, using a number of parameters which grows only linearly with $N$.


\begin{figure}[t]
\begin{centering}
\includegraphics[width=\textwidth]{\gplvmfiguresdir/gplvm_1d_draw_9}
\end{centering}
\caption[One-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model (\gplvm). 
\emph{Bottom:} the density of a set of samples from a 1D Gaussian, specifying the distribution $p(\vX)$ in the latent space.
\emph{Top Right:} A function drawn from a \gp{} prior.
\emph{Left:} A nonparametric density defined by warping the latent density through the sampled function.} 
\label{fig:oned-gplvm}
\end{figure}



\begin{figure}
\begin{centering}
{\begin{tabular}{cccc}
\phantom{h} & Latent space $p(\vX)$ & & Observed space $p(\vY)$ \\
& \fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-latent-crop}} &
\raisebox{7em}{$\overset{\mathlarger{f(x)}}{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\rightarrow}}}}}}}}$} &
%\raisebox{7em}{\Huge $\overset{\textnormal{\Large{GP}}}\rightarrow$} &
\fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-draw-crop}}
\end{tabular}}
\end{centering}
\caption[Two-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.
\emph{Left:}  Isocontours and samples from a 2D Gaussian, specifying the distribution $p(\vX)$ in the latent space. 
\emph{Right:} Density and samples from a nonparametric density defined by warping the latent density through a function drawn from a \gp{} prior.}  
\label{fig:twod-gplvm}
\end{figure}



The \iwmm{} can be viewed as an extension to the \gplvm{}, a probabilistic model of nonlinear manifolds.
% which can be viewed as a special case of the \iwmm{}.
%The \iwmm{} can be viewed as an extension of the Gaussian process latent variable model (GPLVM)~\citep{lawrence2004gaussian}.  
%The \gplvm{} is a probabilistic model of nonlinear manifolds.
%While not typically thought of as a density model, the \gplvm{} does in fact define a posterior density over observations~\citep{nickisch2010gaussian}.
The \gplvm{} smoothly warps a Gaussian density into a more complicated distribution, using a draw from a \gp{}.
Usually, we think of the Gaussian density as living in a latent space having $Q$ dimensions, and the warped density living in the observed space having $D$ dimensions.

A standard definition of the \gplvm{} is as follows:
%
\begin{align}
\textnormal{latent coordinates } \vX = (\vx_1, \vx_2, \ldots, \vx_{N})\tra & \simiid \N{\vx}{0}{\vI_Q} \\
\textnormal{warping functions } \vf = (f_1, f_2, \ldots, f_D)\tra & \simiid \GPt{0}{\kSE \kernplus \kWN} \\
\textnormal{observed datapoints } \vY = (\vy_1, \vy_2, \ldots, \vy_N)\tra & = \vf(\vX)
\end{align}


%Suppose that we have a set of observations
%$\vY = (\vy_1,\ldots,\vy_N)\tra$,
%where $\vy_n \in {\mathbb R}^D$,
%and they are associated with a set of latent coordinates
%$\vX = (\vx_1,\ldots,\vx_{N})\tra$,
%where $\vx_n \in {\mathbb R}^Q$.
%The \gplvm{} assumes that observations are generated by mapping the latent coordinates through a set of smooth functions, over which Gaussian process priors are placed.
Under the \gplvm{}, the probability of observations given the latent coordinates integrating out the mapping functions, is simply a product of multivariate normals:
\begin{align}
p(\vY | \vX, \vtheta) 
& = \prod_{d=1}^D p(\vY_{:,d} | \vX, \vtheta) = \prod_{d=1}^D \N{\vY_{:,d}}{0}{\vK_\vtheta} \\ 
& = (2 \pi)^{-\frac{DN}{2}}  |\vK|^{-\frac{D}{2}} \exp \left( -\frac{1}{2} {\rm tr}( \vY\tra \vK\inv \vY) \right),
\label{eq:py_x}
\end{align}
%where $\vK$ is the $N \times N$ covariance matrix defined 
%by the kernel function $k(\vx_{n},\vx_{m})$,
where $\vtheta$ are the kernel parameters and $\vK$ is the Gram matrix $k(\vX, \vX)$.
%In this chapter, we use an \kSE + \kWN kernel.
% with an additive noise term:
%\begin{align}
%k(\vx_{n},\vx_{m}) &= \alpha \exp\left( - \frac{1}{2 \ell^2}(\vx_n - \vx_m)\tra (\vx_n - \vx_m) \right) 
%+ \delta_{nm} \beta\inv.
%\end{align}
%where $\alpha$ is 
%$\ell$ is the length scale,
%and $\beta$ is the variance of the additive noise.
%This likelihood is simply the product of $D$ independent Gaussian process likelihoods, one for each output dimension.

Typically, the \gplvm{} is used for dimensionality reduction or visualization, and the latent coordinates are set by maximizing \eqref{eq:py_x}.
In that setting, the Gaussian prior density on $\vx$ is essentially a regularizer which keeps the latent coordinates from spreading arbitrarily far apart.  
In contrast, we'll integrate out the latent coordinates, and place a more flexible parameterization on $p(\vx)$ than a single isotropic Gaussian.
Just as the \gplvm{} can be viewed as a manifold learning algorithm, the \iwmm{} can be viewed as learning a set of manifolds, one for each cluster.





\section{The Infinite Warped Mixture Model}
\label{sec:iwmm-definition}

\begin{figure}
\centering
\tabcolsep=0.1em
{\begin{tabular}{ccc}
\fbox{\includegraphics[trim=2em 0em 0em 0em, clip, height=13.5em,width=13.5em]{\warpedfiguresdir/iwmm_latent_N200_seed2155_darker}} &
\raisebox{5em}{$\rightarrow$} &
\fbox{\includegraphics[trim=8em 6em 6em 2em, clip, height=13.5em,width=13.5em]{\warpedfiguresdir/iwmm_N200_seed2155_points1000000}}\\
Latent space & & Observed space \\
\end{tabular}}
\caption[A draw from the infinite warped mixture model prior]{
A sample from the \iwmm{} prior.
\emph{Left:} In the latent space, a mixture distribution is sampled from a Dirichlet process mixture of Gaussians.
\emph{Right:} The latent mixture is smoothly warped to produce non-Gaussian manifolds in the observed space.}
\label{fig:generative}
\end{figure}


In this section, we define in detail the infinite warped mixture model (\iwmm{}).
Just like the \gplvm{}, the \iwmm{} assumes a smooth nonlinear mapping from a latent density to an observed density.
The only difference is that the \iwmm{} assumes that the latent density is an infinite mixture of Gaussians~\citep{rasmussen2000infinite}:
\begin{align}
p(\vx ) = \sum_{c=1}^{\infty} \lambda_c \, {\cal N}(\vx|\bm{\mu}_{c},\vR_{c}\inv)
\end{align}
where $\lambda_{c}$, $\bm{\mu}_{c}$ and $\vR_{c}$ is the mixture weight, mean, and precision matrix of the $c^{\text{\tiny th}}$ mixture component.
%To ensure a conjugate likelihood, we place Gaussian-Wishart priors on the Gaussian parameters.

The \iwmm{} can be seen as a generalization of either the \gplvm{} or the infinite Gaussian mixture model (\iGMM{}).
To be precise, the \iwmm{} with a single fixed spherical Gaussian density on the latent coordinates corresponds to the \gplvm{}, while the \iwmm{} with fixed direct mapping function $f_{d}(\vx)=x_{d}$ and 
$Q=D$ corresponds to the \iGMM{}.

The need for a clustering model that doesn't have a fixed set of cluster shapes is motivated by fact that
a mixture of Gaussians fit to a single non-Gaussian cluster (such as one that is curved or heavy-tailed) will report that the data contains many Gaussian clusters.
If we care about getting the number of clusters right, then we need a flexible model of cluster shapes.






%
%\begin{figure}[t!]
%\centering
%\includegraphics[height=12em]{\warpedfiguresdir/iWMM}
%\caption[Graphical model of the infinite warped mixture model]{A graphical model representation of the infinite warped mixture model, where the shaded and unshaded nodes indicate observed and latent variables, respectively, and plates indicate repetition.}
%\label{fig:graphical}
%\end{figure}
%
%Figure~\ref{fig:graphical} shows the graphical model representation of the proposed model.
%Here, we assume a Gaussian for the mixture component, although we could in principle use other distributions such as Student's t-distribution or the Laplace distribution.



%The \iwmm{} offers attractive properties that do not exist in other probabilistic models; principally, the ability to model clusters with nonparametric densities, and to infer a separate dimension for manifold.
% todo: move this sentence somewhere else.





\section{Inference}
\label{sec:iwmm-inference}

As discussed in \cref{sec:useful-properties}, one of the main advantages of \gp{} priors is that, given inputs $\vX$, outputs $\vY$ and kernel parameters $\vtheta$, we can analytically integrate over functions mapping $\vX$ to $\vY$.
However, if we introduce uncertainty about any of these parameters, or add extra latent variables, then there's usually no analytic method to integrate out these extra parameters.
In this section, we'll outline how we can infer all the parameters in the \iwmm{} given only a set of observations $\vY$.
Details can be found in \cref{sec:iwmm-inference-details}.

By placing conjugate priors on the parameters of the Gaussian mixture components, we can analytically integrate out the cluster shapes, given the assignments of points to clusters.
The only remaining variables to infer are the latent points $\vX$, the cluster assignments $\vZ$, and the kernel parameters $\vtheta$.
%In particular, we'll alternate collapsed Gibbs sampling of $\vZ$ and Hamiltonian Monte Carlo sampling of $\vX$.
%
%Given $\vZ$, we can calculate the gradient of the un-normalized posterior distribution of $\vX$, integrating over warping functions.
%This gradient allows us to sample $\vX$ using Hamiltonian Monte Carlo (\HMC{}).
%
We can obtain samples from their posterior 
%$p(\vX,\vZ|\vY,\bm{\theta},\vS,\nu,\vu,r,\eta)$ 
$p(\vX,\vZ, \vtheta | \vY)$ 
by iterating two steps:
\begin{enumerate}
\item
Given the latent points $\vX$, we sample the discrete cluster memberships $\vZ$ using collapsed Gibbs sampling, integrating out the mixture parameters \eqref{eq:gibbs}.
%The conditional probability of $\vZ$ given $\vX$ is given by \eqref{eq:gibbs}.
%For each observation $n=1,\cdots,N$,
%sample the component assignment $z_{n}$ by collapsed Gibbs sampling 
\item 
Given the cluster assignments $\vZ$, we sample the continuous latent coordinates $\vX$ and kernel parameters $\vtheta$ using Hamiltonian Monte Carlo.
\end{enumerate}

The complexity of each iteration of \HMC{} is dominated by the $\mathcal{O}(N^3)$ computation of $\vK\inv$.
This complexity could be improved by making use of an inducing-point approximation such as \citep{quinonero2005unifying,snelson2006sparse}.



\subsubsection{Posterior Predictive Density}

One disadvantage of this model class is that its predictive density has no closed form.
To approximate the predictive density, we sample latent points from the posterior on the latent density, and map them through warpings drawn from the corresponding posterior density.
The Gaussian noise added to each observation means that each sample adds a Gaussian to the Monte Carlo estimate of the predictive density.
Details can be found in \cref{sec:iwmm-predictive-density}.
This procedure was used to generate the plots of posterior density in \cref{fig:generative,fig:warping,fig:posterior}.





\section{Related Work}
\label{sec:iwmm-related-work}

The \gplvm{} is effective as a nonlinear latent variable model in a wide variety of applications ~\citep{lawrence2004gaussian,salzmann2008local,lawrence2009non}.
The latent positions $\vX$ in the \gplvm{} are typically obtained by maximum a posteriori estimation or variational Bayesian inference~\citep{titsias2010bayesian}, placing a single fixed spherical Gaussian prior on $\vx$.
A prior which penalizes a high-dimensional latent space was introduced by \citet{geiger2009rank}, in which the latent variables and their intrinsic dimensionality are simultaneously optimized.
The \iwmm{} can also infer the intrinsic dimensionality of nonlinear manifolds:
inferring the Gaussian covariance for each latent cluster allows the variance of irrelevant dimensions to become small.  
Because each latent cluster has a different set of parameters, the effective dimension of each cluster can vary, allowing manifolds of differing dimension in the observed space.
This ability is demonstrated in figure \ref{fig:warping}b.

The \iwmm{} can also be viewed as a generalization of the mixture of probabilistic principle component analyzers~\citep{tipping1999mixtures}, or mixture of factor analyzers~\citep{ghahramani2000variational}, where the linear mapping of the mixtures is generalized to a nonlinear mapping by Gaussian processes, and number of components is infinite.% infinite number of components by Dirichlet processes.

There exist non-probabilistic clustering methods which can find clusters with complex shapes, such as spectral clustering~\citep{ng2002spectral} and nonlinear manifold clustering~\citep{cao2006nonlinear,elhamifar2011sparse}.
Spectral clustering finds clusters by first forming a similarity graph, then finding a low-dimensional latent representation using the graph, and finally, clustering the latent coordinates via k-means.
The performance of spectral clustering depends on parameters which are usually set manually, such as the number of clusters, the number of neighbors, and the variance parameter used for constructing the similarity graph.
In contrast, the \iwmm{} infers such parameters automatically.
One of the main advantages of the \iwmm{} over these methods is that there is no need to construct a similarity graph.
%The \iwmm{} can be viewed as a probabilistic generative model for spectral clustering, where the both methods find clusters in a nonlinearly mapped low-dimensional space.  

The kernel Gaussian mixture model~\citep{wang2003kernel} can also find non-Gaussian shaped clusters.
This model estimates a \GMM{} in the implicit high-dimensional feature space defined by the kernel mapping of the observed space.
However, the kernel GMM uses a fixed nonlinear mapping function, with no guarantee that the latent points will be well-modeled by a \GMM{}.
%and the data points mapped by a fixed function are not guaranteed 
%to be distributed according to a GMM.
In contrast, the \iwmm{} infers the mapping function such that the latent co-ordinates will be well-modeled by a mixture of Gaussians.

%The infinite mixtures of Gaussian process experts~\citep{rasmussen2002infinite}
%uses Dirichlet process and Gaussian process priors for regression problems.
%The mixture of ppca
%ppca \citep{tipping1999probabilistic}
%mixture of ppca \citep{tipping1999mixtures}
%one Gaussian process function




\section{Experimental Results}
\section{sec:iwmm-experiments}

\subsection{Clustering Faces}

We first examined our model's ability to model images without pre-processing.
We constructed a dataset consisting of 50 greyscale 32x32 pixel images of two individuals from the UMIST faces dataset \citep{umistfaces}.
Both series of images capture a person turning his head to the right.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\columnwidth]{\warpedfiguresdir/faces2}
\caption[Latent clusters of face images]{A sample from the 2-dimensional latent space when modeling a series of 32x32 face images.
Our model correctly discovers that the data consists of two separate manifolds, both approximately one-dimensional, which share the same head-turning structure.}
\label{fig:faces}
\end{figure}

Figure \ref{fig:faces} shows a sample from the posterior over the latent coordinates, as well as the density model.  The model has recovered three relevant, interpretable features of the dataset.
First, that there are two distinct faces.
Second, that each set of images lies approximately along a smooth one-dimensional manifold.
Third, that the two manifolds share roughly the same structure: the front-facing images of both individuals lie close to one another, as do the side-facing images.


\subsection{Synthetic Datasets}
Next, we demonstrate the proposed model on the four synthetic datasets shown in Figure~\ref{fig:warping}.
%The top row of Figure~\ref{fig:warping} shows the two-dimensional observed data points (unlabeled) and the non-Gaussian clusters inferred by the \iwmm{}.
% The bottom row shows the latent coordinates and Gaussian components in the latent space inferred by the \iwmm{}.
% Each plot shows a single sample from the Markov chain.
%
\def\inclatentpic#1{\fbox{\includegraphics[width=0.22\columnwidth, height=0.2\columnwidth]{\warpedfiguresdir/#1}}}
\begin{figure}[ht!]
\centering
{\tabcolsep=0.3em
\begin{tabular}{cccc}
\multicolumn{4}{c}{Observed space} \\
\inclatentpic{spiral2_x3_observed_coordinates_epoch5000} &
\inclatentpic{halfcircles_N100K3_x3_observed_coordinates_epoch5000} &
\inclatentpic{circles_N50K2_x3_observed_coordinates_epoch5000} &
\inclatentpic{pinwheel_N50K5_x3_observed_coordinates_epoch5000} \\
$\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ \\ 
\inclatentpic{spiral2_x_latent_coordinates_epoch5000} &
\inclatentpic{halfcircles_N100K3_x_latent_coordinates_epoch5000} &
\inclatentpic{circles_N50K2_x_latent_coordinates_epoch5000} &
\inclatentpic{pinwheel_N50K5_x_latent_coordinates_epoch5000} \\
\multicolumn{4}{c}{Latent space} \\
(a) 2-curve & (b) 3-semi & (c) 2-circle & (d) Pinwheel \\
\end{tabular}}
\caption[Recovering clusters on synthetic data]{
Top row: The observed, unlabeled data points, and the clusters inferred by the \iwmm{}.
Bottom row: Latent coordinates and Gaussian components, shown for a single sample from the posterior.
Each point in the latent space corresponds to a point in the observed space. This figure is best viewed in color.}
\label{fig:warping}
\end{figure}
%
None of these four datasets can be appropriately clustered by Gaussian mixture models (GMM).
For example, consider the 2-curve data shown in Figure~\ref{fig:warping} (a), where 100 data points lie in one of two curved lines in a two-dimensional observed space.
%as shown at the top row of .
A GMM with two components cannot separate the two curved lines, while a GMM with many components could separate the two lines only by breaking each line into many clusters. 
In contrast, with the \iwmm{}, the two non-Gaussian-shaped clusters in the observed space were represented by two Gaussian-shaped clusters in the latent space, as shown at the bottom row of Figure~\ref{fig:warping} (a).
The \iwmm{} separated the two curved lines by nonlinearly warping two Gaussians from the latent space to the observed space.

Figure \ref{fig:warping} (c) shows an interesting manifold learning challenge: a dataset consisting of two circles.
The outer circle is modeled in the latent space by a Gaussian with effectively one degree of freedom.
This linear topology fits the outer circle in the observed space by bending the two ends until they overlap.
In contrast, the sampler fails to discover the 1D topology of the inner circle, modeling it with a 2D manifold instead.
This example demonstrates that each cluster in the \iwmm{} manifold can have a different effective dimension.


\def\incwarpmixpic#1{\fbox{\includegraphics[width=0.22\columnwidth, height=0.2\columnwidth]{\warpedfiguresdir/#1}}}
\begin{figure}[ht!]
\centering
{\tabcolsep=0.3em
\begin{tabular}{cccc}
\incwarpmixpic{spiral2all_o_latent_coordinates_epoch1}&
\incwarpmixpic{spiral2all_o_latent_coordinates_epoch500} & 
\incwarpmixpic{spiral2all_o_latent_coordinates_epoch1800}&
\incwarpmixpic{spiral2all_o_latent_coordinates_epoch3000}\\
(a) Initialization & (b) Iteration 500 & (c) Iteration 1800 & (d) Iteration 3000 \\
\end{tabular}}
\caption[A visualization of a sampler for the \siwmm{}]{The inferred infinite GMMs over iterations in the two-dimensional latent space with the \siwmm{} using the 2-curve data. Labels indicate the number of iterations of the sampler, and the color of each point represents its ordering in the observed coordinates.}
\label{fig:infer}
\end{figure}



\subsection{Mixing}

An interesting side-effect of learning the number of latent clusters is that this added flexibility can help the sampler escape local minima, helping the sampler to mix properly.
Figure~\ref{fig:infer} shows the samples of the latent coordinates and clusters of the \iwmm{} over time, when modeling the 2-curve data.
\ref{fig:infer}(a) shows the latent coordinates initialized at the observed coordinates, starting with one latent component.
At the 500th iteration \ref{fig:infer}(b), each curved line is modeled by two components.
At the 1800th iteration \ref{fig:infer}(c), the left curved line is modeled by a single component.
At the 3000th iteration \ref{fig:infer}(d), the right curved line is also modeled by a single component, and the dataset is appropriately clustered.
This configuration was relatively stable, and a similar state was found at the 5000th iteration.
%In this way, the \iwmm{} can find latent coordinates
%by flexibly changing the number of components.



\subsection{Density Estimation}

\begin{figure}[ht!]
\centering
\begin{tabular}{cc}
(a) \iwmm{} & (b) \gplvm{} \\
\includegraphics[width=0.4\columnwidth]{\warpedfiguresdir/result_spiral2_ydistribution} &
\includegraphics[width=0.4\columnwidth]{\warpedfiguresdir/result_spiral2all_gplvm_ydistribution}
\end{tabular}
\caption[Comparing density estimates of between the \sgplvm{} and the \siwmm{}]{
\emph{Left:} The posterior density in the observed space with the 2-curve data inferred by the \iwmm{}.
\emph{Right:} The posterior inferred by the \iwmm{} with one component, a model equivalent to the \gplvm{}.}
\label{fig:posterior}
\end{figure}

\Cref{fig:posterior}(a) shows the posterior density in the observed space inferred by the \iwmm{} on the 2-curve data, computed using 1000 samples from the Markov chain.
The two separate manifolds of high density implied by the two curved lines was recovered by the \iwmm{}.  
Note also that the density along the manifold varies with the density of data shown in \Cref{fig:warping}(a).  
%The warped densities output by our model are actually a richer representation than simply a set of manifolds, since they also define a density at each point in the observed space.
%The density computed using multiple samples from MCMC is clearer than the density computed from a single sample shown at the bottom of Figure~\ref{fig:warping} (a).

This result can be compared to a special case of our model, which uses only a single Gaussian to model the latent coordinates instead of an infinite GMM.
\Cref{fig:posterior}(b) shows that the single-cluster variant of the \iwmm{} posterior is forced to place significant density connecting the two clusters, since it has to reproduce the observed density manifold by warping a single Gaussian.


\subsection{Visualization}
Next, we briefly investigate the potential of the \iwmm{} for low-dimensional visualization of data.
%
\def\incdensitypic#1{\fbox{\includegraphics[width=0.22\columnwidth, height=0.2\columnwidth]{\warpedfiguresdir/#1}}}
\begin{figure}[ht!]
\centering
{\tabcolsep=0.3em
\begin{tabular}{cccc}
\incdensitypic{spiral2all_o_latent_coordinates} &
\incdensitypic{spiral2all_wm2_o_latent_coordinates} &
\incdensitypic{spiral2_gplvm2_o_latent_coordinates} &
\incdensitypic{spiral2_fixedgaussgplvm_o_latent_coordinates}\\
(a) \iwmm{} & (b) \iwmm{} ($C=1$) &
(c) \gplvm{} & (d) Bayesian \gplvm{}
\end{tabular}}
\caption[Comparison of latent coordinate estimates]{Latent coordinates of the 2-curve data, estimated by four different methods.
% by (a) \iwmm{}, (b) \iwmm{} ($C=1$), (c) \gplvm{}, and (d) Bayesian \gplvm{}.
}
\label{fig:latent}
\end{figure}
%
Figure~\ref{fig:latent} (a) shows the latent coordinates obtained by averaging over 1000 samples from the posterior of the \iwmm{}.
Because rotating the latent coordinates does not change their probability, averaging may not be an adequate way to summarize the posterior.
However, we show this result in order to show the characteristics of latent coordinates obtained by the \iwmm{}.
The estimated latent coordinates are clearly separated, and they form two straight lines.
This result indicates that in some cases, the \iwmm{} can recover the topology of the data before it has been warped into a manifold.
For comparison, Figure~\ref{fig:latent} (b) shows the latent coordinates 
estimated by the \iwmm{} when forced to use a single cluster: the latent coordinates lie in two sections of a single straight line.
Figure~\ref{fig:latent} (c) and (d) show the latent coordinates 
estimated by the \gplvm{} when optimizing or integrating out the latent coordinates, respectively.  
Recall that the \iwmm{} ($C=1$) is a more flexible model than the \gplvm{}, since the \gplvm{} enforces a spherical covariance in the latent space.
These methods did not unfold the two curved lines, since the effective dimension of their latent representation is fixed beforehand.
%because in these models, no force to favor forming a low-dimensional manifold.
In contrast, the \iwmm{} effectively formed a low-dimensional representation in the latent space. 

Regardless of the dimension of the latent space, the \iwmm{} will tend to model each cluster with as low-dimensional a Gaussian as possible. 
This is because, if the data in a cluster can be made to lie in a low-dimensional plane, a narrowly-shaped Gaussian will assign the latent coordinates much higher likelihood than a spherical Gaussian.

\subsection{Clustering Performance}
We more formally evaluated the density estimation and clustering performance of the proposed model using four real datasets: iris, glass, wine and vowel, obtained from LIBSVM multi-class datasets \citep{chang2011libsvm}, in addition to the four synthetic datasets shown above: 2-curve, 3-semi, 2-circle and Pinwheel~\citep{adams2009archipelago}.
The statistics of these datasets are summarized in Table~\ref{tab:statistics}.
%
\begin{table}[ht!]
\centering
\caption[Datasets used for evaluation of the \siwmm{}]
{Statistics of the datasets used for evaluation.}
\label{tab:statistics}
\begin{tabular}{rrrrrrrrr}
\hline
 & 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
\hline
samples: $N$ & 100 & 300 & 100 & 250 & 150 & 214 & 178 & 528 \\
dimension: $D$ & 2 & 2 & 2 & 2 & 4 & 9 & 13 & 10 \\
num. clusters: $C$ & 2 & 3 & 2 & 5 & 3 & 7 & 3 & 11 \\
\hline
\end{tabular}
\end{table}
%
In each experiment, we show the results of ten-fold cross-validation.
Results in bold are not significantly different from the best performing method in each column according to a paired t-test.
%In each experiment, 10\% of the data was used for testing.
%
%
%We used two evaluation measurements: test point likelihood and Rand index
%for evaluating in terms of density estimation performance
%and clustering performance, respectively.
% the Gaussian-Wishart prior is quite different in these cases, placing more mass on higher-dimensional manifolds when the dimension of the latent space is high.  Placing a hyper-prior allowing the concentration parameter $\eta$ to vary may shrink these differences.

\begin{table}[ht!]
\centering
\caption[Clustering performance comparison]
{Average Rand index for evaluating clustering performance.}
\label{tab:rand}
\begin{tabular}{lrrrrrrrr}
\hline
 & 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
\hline
iGMM & $0.52$ & $0.79$ & $0.83$ & $0.81$ & $0.78$ & $0.60$ & $0.72$ & $\mathbf{0.76}$ \\
iWMM(Q=2) & $\mathbf{0.86}$ & $\mathbf{0.99}$ & $\mathbf{0.89}$ & $\mathbf{0.94}$ & $\mathbf{0.81}$ & $\mathbf{0.65}$ & $0.65$ & $0.50$ \\
iWMM(Q=D) & $\mathbf{0.86}$ & $\mathbf{0.99}$ & $\mathbf{0.89}$ & $\mathbf{0.94}$ & $0.77$ & $0.62$ & $\mathbf{0.77}$ & $\mathbf{0.76}$ \\
\hline
\end{tabular}
\end{table}
%
Table \ref{tab:rand} compares the clustering performance of the \iwmm{} with the \iGMM{}, quantified by the Rand index~\citep{rand1971objective}, which measures the correspondence between inferred clusters and true clusters.
The \iGMM{} is another probabilistic generative model commonly used for clustering, which can be seen as a special case of the \iwmm{} in which the Gaussian clusters are not warped.  
These experiments demonstrate the extent to which nonparametric cluster shapes allow a mixture model to recover more meaningful clusters.
%When the latent dimensionality was set to two ($Q=2$), clustering performance was degraded.% did not work well
%because information was lost by reducing the dimensionality.
%The sometimes large differences between performance in the $D = 2$ case and the $D = Q$ case may be attributed to the fact that when the latent dimension is high, it requires many samples from from the latent distribution to produce an accurate estimate of the posterior density at the test locations.  A potential solution to this difficulty in inference may be to use a warping with back-constraints \citep{Lawrence06localdistance}, allowing us to more directly evaluate the density at a given point in the observed space.
%TODO: cite


Table \ref{tab:likelihood} lists average test log likelihood, comparing the proposed models with kernel density estimation (\KDE{}),
%the manifold Parzen window (MPW)~\citep{vincent2002manifold} and 
and the infinite Gaussian mixture model (\iGMM{}).
In \KDE{}, the kernel width is estimated by maximizing the leave-one-out log densities.
%With WM and \iwmm{}, we set the dimensionality of the latent space
%at that of the observed space $Q=D$ or $Q=2$.
Since the manifold on which the observed data lies can be at most $D$-dimensional, we set the latent dimension $Q$ equal to the observed dimension $D$ in \iwmm{}s.
We also include the $Q = 2$ case in an attempt to characterize how much modeling power is lost by forcing the latent representation to be visualizable. 
%We include the $Q=2$ case in order to attempt to quantify the 
%The proposed models achieved higher test log likelihoods than the KDE, MPW
The proposed models achieved high test likelihoods compared with the \KDE{} and the \iGMM{}.
%The \iwmm{} achieved better performance than WM for each latent dimensionality,
%except on the wine dataset.
% This result indicates that it is important to assume an infinite mixture model in the latent space for density estimation.%, and that even a fully Bayesian version of the \gplvm{} is not necessarily a good density model.

\begin{table}[ht!]
\centering
\caption[Predictive log-likelihood comparison]
{Average test log-likelihood for evaluating density estimation performance.}
\label{tab:likelihood}
\begin{tabular}{lrrrrrrrr}
\hline
& 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
\hline 
KDE & $-2.47$ & $-0.38$ & $-1.92$ & $-1.47$ & $\mathbf{-1.87}$ & $1.26$ & $-2.73$ & $\mathbf{6.06}$ \\
iGMM & $-3.28$ & $-2.26$ & $-2.21$ & $-2.12$ & $-1.91$ & $3.00$ & $\mathbf{-1.87}$ & $-0.67$ \\
\iwmm{}(Q=2) & $\mathbf{-0.90}$ & $\mathbf{-0.18}$ & $\mathbf{-1.02}$ & $\mathbf{-0.79}$ & $\mathbf{-1.88}$ & $\mathbf{5.76}$ & $\mathbf{-1.96}$ & $\mathbf{5.91}$ \\
\iwmm{}(Q=D) & $\mathbf{-0.90}$ & $\mathbf{-0.18}$ & $\mathbf{-1.02}$ & $\mathbf{-0.79}$ & $\mathbf{-1.71}$ & $\mathbf{5.70}$ & $-3.14$ & $-0.35$ \\
\hline
\end{tabular}
\end{table}

\subsubsection{Source Code}
Code to reproduce all the above experiments is available at \url{github.com/duvenaud/warped-mixtures}.


\section{Conclusion}

In this chapter, we introduced a simple generative model of non-Gaussian density manifolds which can infer nonlinearly separable clusters, low-dimensional representations of varying dimension per cluster, and density estimates which smoothly follow data contours.
We then introduced an efficient sampler for this model which integrates out both the cluster parameters and the warping function exactly.
We further demonstrated that allowing non-parametric cluster shapes improves clustering performance over the Dirichlet process Mixture of Gaussians.

Many methods have been proposed which can perform some combination of clustering, manifold learning, density estimation and visualization.
We demonstrated that a simple but flexible probabilistic generative model can perform well at all these tasks.

Since the proposed model is generative, it can be used for density estimation as well as clustering.
It can also be extended to handle missing data, integrate with other probabilistic models, and use other families of distributions for the latent components.


\section{Future Work}

%One design decision which may be worth revisiting is the use the same warping for all mixture components.  It would be simple, and typically faster, to use a separate set of GPs to model the warping of each latent cluster.  

\subsubsection{More Sophisticated Latent Density Models}
The Dirichlet process mixture of Gaussians in the latent space of our model could easily be replaced by a more sophisticated density model, such as a hierarchical Dirichlet process~\citep{teh2006hierarchical}, or a Dirichlet diffusion tree~\citep{neal2003density}.
Another straightforward extension of our model would be making inference more scalable by using sparse Gaussian processes~\citep{quinonero2005unifying,snelson2006sparse} or more advanced Hamiltonian Monte Carlo methods~\citep{zhang2011quasi}.
An interesting but more complex extension of the \iwmm{} would be a semi-supervised version of the model.
The \iwmm{} could allow label propagation along regions of high density in the latent space, even if the individual points in those regions are stretched far apart along low-dimensional manifolds in the observed space.
Another natural extension would be to allow a separate warping for each cluster, producing a mixture of warped Gaussians, rather than a warped mixture of Gaussians.% which would also improve inference speed.


\subsubsection{Learning the Topology of Data Manifolds}

Some datasets naturally live on manifolds which aren't simply connected.
For example, motion capture data or video of a person walking in a circle naturally lives on a torus, with one coordinate specifying the phase of the person's step, and another specifying how far around the circle they've walked.

%What if we were to use the structured kernels of \cref{ch:kernels,ch:grammar} to specify the prior on warpings?
As shown in \cref{sec:topological-manifolds}, using structured kernels to specify the warping of a latent space gives rise to interesting topologies on the observed density manifold.
If a suitable method for computing the marginal likelihood of a \gplvm{} is available, an automatic search similar to that of \cref{ch:grammar} would be possible, automatically finding the topology of the data manifold.

% Todo:  Cite paper from Jeff's talk that suggests using DP mixtures to estimate the number of clusters.


%This model is 

%Table \ref{tab:count} shows the inferred number of components
%by the iGMM and \iwmm{}.
%With the synthetic data sets, 
%the number of components inferred by the \iwmm{} was 
%
%\begin{table*}[t!]
%\centering
%\caption{Average inferred number of components.}
%\label{tab:count}
%\begin{tabular}{lrrrrrrrr}
%\hline
% & 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
%\hline
%iGMM & 2.7 & 4.5 & 3.8 & 4.2 & 2.0 & 2.3 & 2.7 & 5.0 \\ 
%iWMM($Q=2$) & 2.2 & 3.0 & 3.2 & 4.6 & 2.6 & 3.7 & 3.1 & 2.8 \\
%iWMM($Q=D$) & 2.2 & 3.0 & 3.2 & 4.6 & 2.0 & 2.8 & 5.9 & 5.1 \\ 
%\hline
%true & 2 & 3 & 2 & 5 & 3 & 7 & 3 & 11 \\
%\hline
%\end{tabular}
%\end{table*}
%


%In contrast to the many somewhat ad-hoc clustering, manifold learning, density estimation, and visualization methods introduced recently, we show that a simple generative model can perform well at all of these tasks.

%We introduced a nonparametric Bayesian clustering method capable of inferring nonlinearly separable clusters.
%In the experiments, we demonstrated that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters.


%\subsection*{Acknowledgements}
%The authors would like to thank Dominique Perrault-Joncas, Carl Edward Rasmussen, and Ryan Prescott Adams for helpful discussions.
%\pagebreak

%\section{Open Questions}

%\subsubsection{How can we extend this model to handle heavy-tailed clusters?}


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


