\input{common/header.tex}
\inbpdocument

\chapter{Warped Mixture Models}  % Or: extensions of the GP-LVM
\label{ch:warped}


A mixture of Gaussians fit to a single curved or heavy-tailed cluster will report that the data contains many clusters.  
To produce more appropriate clusterings, we introduce a model which warps a latent mixture of Gaussians to produce nonparametric cluster shapes.  
The possibly low-dimensional latent mixture model allows us to summarize the properties of the high-dimensional clusters (or density manifolds) describing the data.  
The number of manifolds, as well as the shape and dimension of each manifold is automatically inferred.
We derive a simple inference scheme for this model which analytically integrates out both the mixture parameters and the warping function.  
We show that our model is effective for density estimation, performs better than infinite Gaussian mixture models at recovering the true number of clusters, and produces interpretable summaries of high-dimensional datasets.
%We introduce a flexible class of mixture models for clustering and density estimation. Our model 
%describes clusters as nonlinear manifolds, automatically infers the number of clusters, produces a potentially low-dimensional latent representation, and produces a density estimate. Our approach makes use of two tools from Bayesian nonparametrics: a Dirichlet process mixture model to allow an unbounded number of clusters, and a Gaussian process warping function to allow each cluster to have a complex shape. We derive a simple inference scheme for this model 
%which analytically integrates out both the mixture parameters and the warping function.  We show that our model is effective for density estimation, and performs better than infinite Gaussian mixture models at discovering meaningful clusters.

%Joint work with Tomoharu Iwata 



Probabilistic mixture models are often used for clustering.
However, if the mixture components are parametric (e.g. Gaussian), then the clustering obtained can be heavily dependent on how well each actual cluster can be modeled by a Gaussian.
For example, a heavy tailed or curved cluster may need many components to model it.
Thus, although mixture models are widely used for probabilistic clustering, their assumptions are generally inappropriate if the primary goal is to discover clusters in data.
Dirichlet process mixture models can alleviate the problem of an unknown number of clusters, but this does not address the problem that real clusters may not be well matched by any parametric density.

\begin{figure}
\centering
\tabcolsep=0.1em
{\begin{tabular}{ccc}
\fbox{\includegraphics[trim=2em 0em 0em 0em, clip, height=10.5em,width=10.5em]{\warpedfiguresdir/iwmm_latent_N200_seed2155_darker}} &
\raisebox{5em}{$\rightarrow$} &
\fbox{\includegraphics[trim=8em 6em 6em 2em, clip, height=10.5em,width=10.5em]{\warpedfiguresdir/iwmm_N200_seed2155_points1000000}}\\
Latent space & & Observed space \\
\end{tabular}}
\caption[A draw from the infinite warped mixture model prior]{
A sample from the iWMM prior.  Left: In the latent space, a mixture distribution is sampled from a Dirichlet process mixture of Gaussians.  Right:  The latent mixture is smoothly warped to produce non-Gaussian manifolds in the observed space.}
\label{fig:generative}
\end{figure}

In this paper, we propose a nonparametric Bayesian model that can find nonlinearly separable clusters with complex shapes.
The proposed model assumes that each observation has coordinates in a latent space, and is generated by warping the latent coordinates via a nonlinear function from the latent space to the observed space.
By this warping, complex shapes in the observed space can be modeled by simpler shapes in the latent space.
In the latent space, we assume an infinite Gaussian mixture model~\cite{rasmussen2000infinite}, which allows us to automatically infer the number of clusters.
%
%By using infinite mixture models 
%based on Dirichlet processes~\cite{ferguson1973bayesian},
%we can automatically infer the number of components from the data.
For the prior on the nonlinear mapping function, we use Gaussian processes~\cite{rasmussen38gaussian}, which enable us to flexibly infer the nonlinear warping function from the data.
We call the proposed model the {\it infinite warped mixture model} (iWMM).
Figure \ref{fig:generative} shows a set of manifolds and datapoints sampled from the prior defined by this model.%, when the observed and latent spaces are both two-dimensional.

%The proposed model is nonparametric Bayesian in two senses: it uses a Dirichlet process to model the number of mixture components, and Gaussian processes to model the shape of each component.
To our knowledge this is the first probabilistic generative model for clustering with flexible nonparametric component densities.
Since the proposed model is generative, it can be used for density estimation as well as clustering. It can also be extended to handle missing data, integrate with other probabilistic models, and use other families of distributions for the latent components.

We derive an inference procedure for the iWMM based on Markov chain Monte Carlo (MCMC).  In particular, we sample the cluster assignments using Gibbs sampling, sample the latent coordinates using hybrid Monte Carlo, and analytically integrate out both the mixture parameters (weights, means and covariance matrices), and the nonlinear warping function. 

%In particular, we iteratively infer the infinite Gaussian mixture model in the latent space by using Gibbs sampling, the latent coordinates and nonlinear function by using the hybrid Monte Carlo.





\section{Gaussian Process Latent Variable Model}


Besides being useful for modeling functions, a simple extension allows \gp{}s to be useful for general density modeling.  



Unfortunately, this extension causes many of the useful properties of the \gp{} not to hold.

The \gplvm{} can also be thought of as a method for modeling the covariance matrix between all rows of $Y$ using a number of parameters which grows linearly with $N$.


\begin{figure}[t]
\begin{centering}
\includegraphics[width=\textwidth]{\gplvmfiguresdir/gplvm_1d_draw_9}
\end{centering}
\caption[One-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.  Bottom: density and samples from a 1D Gaussian, specifying the distribution $p(\vX)$ in the latent space.  Top Right: A function drawn from a \gp{} prior.  Left: A nonparametric density defined by warping the latent density through the function drawn from a \gp{} prior.}  
\label{fig:oned-gplvm}
\end{figure}



\begin{figure}
\begin{centering}
{\begin{tabular}{cccc}
\phantom{h} & Latent space $p(\vX)$ & & Observed space $p(\vY)$ \\
& \fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-latent-crop}} &
\raisebox{7em}{$\overset{\mathlarger{f(x)}}{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\mathlarger{\rightarrow}}}}}}}}$} &
%\raisebox{7em}{\Huge $\overset{\textnormal{\Large{GP}}}\rightarrow$} &
\fbox{\includegraphics[height=15em,width=15em]{\gplvmfiguresdir/gplvm-draw-crop}}
\end{tabular}}
\end{centering}
\caption[Two-dimensional Gaussian process latent variable model]{A visual representation of the Gaussian process latent variable model.  Left:  Isocontours and samples from a 2D Gaussian, specifying the distribution $p(\vX)$ in the latent space.  Right: Density and samples from a nonparametric density defined by warping the latent density through a function drawn from a GP prior.}  
\label{fig:twod-gplvm}
\end{figure}



In this section, we give a brief introduction to the \gplvm{}, which can be viewed as a special case of the iWMM.
%The iWMM can be viewed as an extension of the Gaussian process latent variable model (GPLVM)~\cite{lawrence2004gaussian}.  
The GPLVM is a probabilistic model of nonlinear manifolds.
While not typically thought of as a density model, the \gplvm{} does in fact define a posterior density over observations~\cite{nickisch2010gaussian}.
It does this by smoothly warping a single, isotropic Gaussian density in the latent space into a more complicated distribution in the observed space.  


Suppose that we have a set of observations
$\vY = (\vy_{1},\cdots,\vy_{N})^{\top}$,
where $\vy_{n}\in{\mathbb R}^{D}$,
and they are associated with a set of latent coordinates
$\vX = (\vx_{1},\cdots,\vx_{N})^{\top}$,
where $\vx_{n}\in{\mathbb R}^{Q}$.
The GPLVM assumes that observations are generated by mapping the latent coordinates through a set of smooth functions, over which Gaussian process priors are placed.
Under the \gplvm{}, the probability of observations given the latent coordinates, integrating out the mapping functions, is
\begin{align}
p(\vY | \vX,\bm{\theta})  = (2 \pi)^{-\frac{DN}{2}}  |\vK|^{-\frac{D}{2}} \exp \left( -\frac{1}{2} {\rm tr}( \vY^{\top} \vK^{-1} \vY) \right),
\label{eq:py_x}
\end{align}
where $\vK$ is the $N \times N$ covariance matrix defined 
by the kernel function $k(\vx_{n},\vx_{m})$,
and $\bm{\theta}$ is the kernel hyperparameter vector.
In this paper, we use an RBF kernel with an additive noise term:
\begin{align}
k(\vx_{n},\vx_{m}) &= \alpha \exp\left( - \frac{1}{2 \ell^2}(\vx_n - \vx_m)^{\top} (\vx_n - \vx_m) \right) \nonumber \\
&+ \delta_{nm} \beta^{-1}.
\end{align}
%where $\alpha$ is 
%$\ell$ is the length scale,
%and $\beta$ is the variance of the additive noise.
This likelihood is simply the product of $D$ independent Gaussian process likelihoods, one for each output dimension.

Typically, the \gplvm{} is used for dimensionality reduction or visualization, and the latent coordinates are determined by maximizing the posterior probability of the latent coordinates, while integrating out the warping function.  
In that setting, the Gaussian prior density on $\vx$ is essentially a regularizer which keeps the latent coordinates from spreading arbitrarily far apart.  
In contrast, we instead integrate out the latent coordinates as well as the warping function, and place a more flexible parameterization on $p(\vx)$ than a single isotropic Gaussian.

Just as the GPLVM can be viewed as a manifold learning algorithm, the iWMM can be viewed as learning a set of manifolds, one for each cluster.





\section{Infinite Warped Mixture Model}

In this section, we define in detail the infinite warped mixture model (\iwmm{}).
In the same way as the \gplvm{}, the \iwmm{} assumes a set of latent coordinates and a smooth, nonlinear mapping from the latent space to the observed space.
In addition, the \iwmm{} assumes that the latent coordinates
are generated from a Dirichlet process mixture model.
In particular, we use the following infinite Gaussian mixture model,
\begin{align}
p(\vx|\{\lambda_{c},\bm{\mu}_{c},\vR_{c}\}) = \sum_{c=1}^{\infty} \lambda_{c} {\cal N}(\vx|\bm{\mu}_{c},\vR_{c}^{-1}),
\end{align}
where $\lambda_{c}$, $\bm{\mu}_{c}$ and $\vR_{c}$ is the mixture weight, 
mean, and precision matrix of the $c^{\text{\tiny th}}$ mixture component.
We place Gaussian-Wishart priors on the Gaussian parameters
$\{\bm{\mu}_{c},\vR_{c}\}$,
\begin{align}
p(\bm{\mu}_{c},\vR_{c})
= {\cal N}(\bm{\mu}_{c}|\vu,(r\vR_{c})^{-1})
{\cal W}(\vR_{c}|\vS^{-1},\nu),
\end{align}
where $\vu$ is the mean of $\bm{\mu}_{c}$, 
$r$ is the relative precision of $\bm{\mu}_{c}$, 
$\vS^{-1}$ is the scale matrix for $\vR_{c}$, 
and $\nu$ is the number of degrees of freedom for $\vR_{c}$.
The Wishart distribution is defined as follows:
\begin{align}
{\cal W}(\vR|\vS^{-1},\nu)
=\frac{1}{G}|\vR|^{\frac{\nu-Q-1}{2}}\exp\left(-\frac{1}{2}{\rm tr}(\vS\vR)\right),
\end{align}
where $G$ is the normalizing constant.
Because we use conjugate Gaussian-Wishart priors 
for the parameters of the Gaussian mixture components, we can analytically integrate out those parameters, given the assignments of points to components.
Let $z_{n}$ be the latent assignment of the $n^{\text{\tiny th}}$ point.
The probability of latent coordinates $\vX$
given latent assignments $\vZ=(z_{1},\cdots,z_{N})$ is obtained 
by integrating out the Gaussian parameters $\{\bm{\mu}_{c},\vR_{c}\}$
as follows:
\begin{align}
p(\vX|\vZ,\vS,\nu,r) &= \prod_{c=1}^{\infty}
\pi^{-\frac{N_{c}Q}{2}}\frac{r^{Q/2}|\vS|^{\nu/2}}{r_{c}^{Q/2}|\vS_{c}|^{\nu_{c}/2}}
\nonumber\\
&\times \prod_{q=1}^{Q}\frac{\Gamma(\frac{\nu_{c}+1-q}{2})}{\Gamma(\frac{\nu+1-q}{2})},
\label{eq:px_z}
\end{align}
where
$N_{c}$ is the number of data points assigned to the $c^{\text{\tiny th}}$ component,
$\Gamma(\cdot)$ is Gamma function,
and
\begin{align}
r_{c}=r+N_{c}, \hspace{2em}
\nu_{c}=\nu+N_{c}, 
\nonumber
\end{align}
\begin{align}
\vu_{c}=\frac{r\vu+\sum_{n:z_{n}=c}\vx_{n}}{r+N_{c}}, 
\nonumber
\end{align}
\begin{align}
\vS_{c}=\vS+\sum_{n:z_{n}=c}\vx_{n}\vx_{n}^{\top}+r\vu\vu^{\top}
-r_{c}\vu_{c}\vu_{c}^{\top},
\end{align}
are the posterior Gaussian-Wishart parameters of the $c^{\text{\tiny th}}$ component.
We use a Dirichlet process with concentration parameter $\eta$
for infinite mixture modeling~\cite{maceachern1998estimating} 
in the latent space.
Then, the probability of $\vZ$ is given as follows:
\begin{align}
p(\vZ|\eta) = 
\frac{\eta^{C}\prod_{c=1}^{C}(N_{c}-1)!}
{\eta(\eta+1)\cdots(\eta+N-1)},
\label{eq:pz}
\end{align}
where $C$ is the number of components for which $N_{c}>0$.
The joint distribution is given by
\begin{align}
\lefteqn{p(\vY,\vX,\vZ|\bm{\theta},\bm{S},\nu,\vu,r,\eta)}
\nonumber \\
& = p(\vY|\vX,\bm{\theta})
p(\vX|\vZ,\bm{S},\nu,\vu,r)p(\vZ|\eta),
\label{eq:joint}
\end{align}
where factors in the right hand side can be calculated by
(\ref{eq:py_x}), (\ref{eq:px_z}) and (\ref{eq:pz}), respectively.

In summary, the infinite warped mixture model generates observations $\vY$ according to the following generative process:
\begin{enumerate}
\item Draw mixture weights $\bm{\lambda} \sim {\rm GEM}(\eta)$
\item For each component $c=1,\cdots,\infty$
\begin{enumerate}
\item Draw precision $\vR_{c} \sim {\cal W}(\vS^{-1},\nu)$
\item Draw mean $\bm{\mu}_{c} \sim {\cal N}(\vu,(r\vR_{c})^{-1})$
\end{enumerate}
\item For each observed dimension $d=1,\cdots,D$
\begin{enumerate}
\item Draw function $f_{d}(\vx) \sim {\rm GP}(m(\vx),k(\vx,\vx'))$
\end{enumerate}
\item For each observation $n=1,\cdots,N$
\begin{enumerate}
\item Draw latent assignment $z_{n} \sim {\rm Mult}(\bm{\lambda})$
\item Draw latent coordinates $\vx_{n} \sim {\cal N}(\bm{\mu}_{z_{n}},\vR_{z_{n}}^{-1})$
\item For each observed dimension $d=1,\cdots,D$
\begin{enumerate}
\item Draw feature $y_{nd} \sim {\cal N}(f_{d}(\vx_{n}),\beta^{-1})$
\end{enumerate}
\end{enumerate}
\end{enumerate}
Here, ${\rm GEM}(\eta)$ is the stick-breaking process \cite{sethuraman94}
that generates mixture weights for a Dirichlet process with parameter $\eta$,
%${\rm Mult}(\cdot)$ represents a multinomial distribution,
${\rm Mult}(\bm{\lambda})$ represents a multinomial distribution with parameter $\bm{\lambda}$,
$m(\vx)$ is the mean function of the Gaussian process,
and $\vx,\vx'\in{\mathbb R}^{Q}$.
%
\begin{figure}[t!]
\centering
\includegraphics[height=12em]{\warpedfiguresdir/iWMM}
\caption[Graphical model of the infinite warped mixture model]{A graphical model representation of the infinite warped mixture model, where the shaded and unshaded nodes indicate observed and latent variables, respectively, and plates indicate repetition.}
\label{fig:graphical}
\end{figure}
%
Figure~\ref{fig:graphical} shows the graphical model representation
of the proposed model.
Here, we assume a Gaussian for the mixture component,
although we could in principle use other distributions
such as Student's t-distribution or the Laplace distribution.

The iWMM can be seen as a generalization of either the GPLVM or the infinite Gaussian mixture model (iGMM).
To be precise, the iWMM with a single fixed spherical Gaussian density on the latent coordinates corresponds to the GPLVM, while the iWMM with fixed direct mapping function $f_{d}(\vx)=x_{d}$ and 
$Q=D$ corresponds to the iGMM.

The iWMM offers attractive properties that do not exist in other probabilistic models; principally, the ability to model clusters with nonparametric densities, and to infer a seperate dimension for manifold.
% todo: move this sentence somewhere else.





\section{Inference}

We infer the posterior distribution of the latent coordinates $\vX$ and cluster assignments $\vZ$ using Markov chain Monte Carlo (MCMC).
In particular, we alternate collapsed Gibbs sampling of $\vZ$, and hybrid Monte Carlo sampling of $\vX$.
Given $\vX$,
%the proposed model is simply a standard infinite mixture model with conjugate priors. Therefore, 
we can efficiently sample $\vZ$ using collapsed Gibbs sampling, integrating out the mixture parameters.
Given $\vZ$, we can calculate the gradient of the unnormalized posterior distribution of $\vX$, integrating over warping functions.
This gradient allows us to sample $\vX$ using hybrid Monte Carlo.

First, we explain collapsed Gibbs sampling for $\vZ$.
Given a sample of $\vX$, $p(\vZ|\vX,\vS,\nu,\vu,r,\eta)$ does not depend on $\vY$.  This lets resample cluster assignments, integrating out the iGMM likelihood in close form.
Given the current state of all but one latent component $z_{n}$, a new value for $z_{n}$ is sampled from the following probability:
\begin{align}
\lefteqn{p(z_{n}=c|\vX,\vZ_{\setminus n},\bm{S},\nu,\vu,r,\eta)}
\nonumber \\
&\propto\!
\left\{
\begin{array}{ll}
\!\!N_{c\setminus n}\cdot p(\vx_{n}|\vX_{c\setminus n},\bm{S},\nu,\vu,r) & \text{{\small existing components}}\\
\!\!\eta\cdot p(\vx_{n}|\bm{S},\nu,\vu,r) & \text{{\small a new component}}
\end{array}
\right.
\label{eq:gibbs}
\end{align}
where $\vX_{c}=\{\vx_{n}|z_{n}=c\}$ 
is the set of latent coordinates assigned to the $c^{\text{\tiny th}}$ component,
and $\setminus n$ represents the value or set
when excluding the $n^{\text{\tiny th}}$ data point.
We can analytically calculate $p(\vx_{n}|\vX_{c\setminus n},\bm{S},\nu,\vu,r)$
as follows:
\begin{align}
\lefteqn{p(\vx_{n}|\vX_{c\setminus n},\bm{S},\nu,\vu,r)}
\nonumber \\
&=\pi^{-\frac{N_{c\setminus n}Q}{2}}\frac{r_{c\setminus n}^{Q/2}|\vS_{c\setminus n}|^{\nu_{c\setminus n}/2}}{r_{c\setminus n}'^{Q/2}|\vS_{c\setminus n}'|^{\nu_{c\setminus n}'/2}}\prod_{d=1}^{Q}\frac{\Gamma(\frac{\nu_{c\setminus n}'+1-d}{2})}{\Gamma(\frac{\nu_{c\setminus n}+1-d}{2})},
\end{align}
where 
$r_{c}'$, $\nu_{c}'$, $\vu_{c}'$ and $\vS_{c}'$
represent the posterior Gaussian-Wishart parameters of the $c^{\text{\tiny th}}$ component
when the $n^{\text{\tiny th}}$ data point is assigned to the $c^{\text{\tiny th}}$ component.
We can efficiently calculate the determinant by using the
rank one Cholesky update.
In the same way, we can analytically calculate 
the likelihood for a new component $p(\vx_{n}|\bm{S},\nu,\vu,r)$.

Hybrid Monte Carlo (HMC) sampling of $\vX$ from posterior 
${p(\vX|\vZ,\vY,\bm{\theta},\bm{S},\nu,\vu,r)}$,
requires computing the gradient of the log of the unnormalized posterior
${\log p(\vY|\vX,\bm{\theta})+\log p(\vX|\vZ,\bm{S},\nu,\vu,r)}$. 
The first term of the gradient can be calculated by
\begin{align}
\pderiv{\log p(\vY | \vX,\bm{\theta})}{\vK} = -\frac{1}{2}D\vK^{-1}+\frac{1}{2}\vK^{-1} \vY \vY^{T} \vK^{-1}, 
\end{align}
and
\begin{align}
\lefteqn{\pderiv{k(\vx_{n},\vx_{m})}{\vx_n}}
\nonumber \\
&= - \frac{\alpha}{\ell^2} \exp \left( - \frac{1}{2 \ell^2} (\vx_n - \vx_m)^{\top} (\vx_n - \vx_m) \right) (\vx_n - \vx_m),
\end{align}
using the chain rule.
The second term can be calculated as follows:
\begin{align}
\frac{\partial \log p(\vX|\vZ,\bm{S},\nu,\vu,r)}{\partial \vx_{n}} 
= -\nu_{z_{n}}\bm{S}_{z_{n}}^{-1}(\vx_{n}-\vu_{z_{n}}).
\end{align}
We also infer kernel hyperparameters $\bm{\theta}=\{\alpha,\beta,\ell\}$ via HMC, using the gradient of the log unnormalized posterior with respect to the kernel hyperparameters.
The complexity of each iteration of HMC is dominated by the $\mathcal{O}(N^3)$ computation of $\vK\inv$
\footnote{This complexity could be improved by making use of an inducing point approximation such as \cite{quinonero2005unifying,snelson2006sparse}}.


In summary, we obtain samples from the posterior $p(\vX,\vZ|\vY,\bm{\theta},\vS,\nu,\vu,r,\eta)$ 
by iterating the following procedures:
\begin{enumerate}
\item For each observation $n=1,\cdots,N$,
%\begin{enumerate}
sample the component assignment $z_{n}$ by collapsed Gibbs sampling (\ref{eq:gibbs}).
%\end{enumerate}
\item Sample latent coordinates $\vX$ and kernel parameters $\bm{\theta}$ using hybrid Monte Carlo.
\end{enumerate}

%scalability
%complexity
%$O(DN^{3})$?

\subsection{Posterior Predictive Density}

In the GP-LVM, the predictive density of at test point $y^\star$ is usually computed by finding the point $x^\star$ which which is most likely to be mapped to $y^\star$, then using the density of $p(x^\star)$ and the Jacobian of the warping at that point to approximately compute the density at $y^\star$.  When inference is done by simply optimizing the location of the latent points, this estimation method simply requires solving a single optimization for each $y^\star$.  

For our model, we use approximate integration to estimate $p(y^\star)$.  This is done for two reasons: First, multiple latent points (possibly from different clusters) can map to the same observed point, meaning the standard method can underestimate $p(y^\star)$.  Second, because we do not optimize the latent coordinates but rather sample them, we would need to perform optimizations for each $p(y^\star)$ seperately for each sample.  Our method gives estimates for all $p(\vy^\star)$ at once, but may not be accurate in very high dimensions.

The posterior density in the observed space given the training data is simply:
\begin{align}
\lefteqn{p(\vy_{*}|\vY)}\nonumber \\
&=\int \!\!\! \int p(\vy_{*},\vx_{*},\vX|\vY)d\vx_{*}d\vX \nonumber\\
&=\int\!\!\! \int p(\vy_{*}|\vx_{*},\vX,\vY)p(\vx_{*}|\vX,\vY)p(\vX|\vY)d\vx_{*}d\vX.
\label{eq:density}
\end{align}
We approximate $p(\vX|\vY)$ using the samples from the Gibbs and hybrid Monte Carlo samplers.
We approximate $p(\vx_{*}|\vX,\vY)$ by sampling points from the latent mixture and warping them, using the following procedure:
\begin{enumerate}
\item Draw latent assignment\\
%$z_{*} \sim {\rm Mult}(N_{1},\cdots,N_{C},\eta)$
$z_{*} \sim {\rm Mult}(\frac{N_{1}}{N+\eta},\cdots,\frac{N_{C}}{N+\eta},\frac{\eta}{N+\eta})$
\item Draw precision matrix\\
$\vR_{*} \sim {\cal W}(\vS^{-1}_{z_{*}},\nu_{z_{*}})$
\item Draw mean\\
$\bm{\mu}_{*} \sim {\cal N}(\vu_{z_{*}},(r_{z_{*}}\vR_{*})^{-1})$
\item Draw latent coordinates\\
$\vx_{*} \sim {\cal N}(\bm{\mu}_{*},\vR_{*}^{-1})$
\end{enumerate}
When a new component $C+1$ is assigned to $z_{*}$, 
the prior Gaussian-Wishart distribution is used for sampling in steps 2 and 3.
The first factor of (\ref{eq:density}) can be calculated by
\begin{align}
\lefteqn{p(\vy_{*}|\vx_{*},\vX,\vY)}\nonumber\\
&= {\cal N}(\vk_{*}^{\top}\vK^{-1}\vY,k(\vx_{*},\vx_{*})-\vk_{*}^{\top}\vK^{-1}\vk_{*}),
\end{align}
where
$\vk_{*}=(k(\vx_{*},\vx_{1}),\cdots,k(\vx_{*},\vx_{N}))^{\top}$.
%We can approximate the posterior density in the observed space
%given the training data by
%$p(y_{*}|\vY) \approx \frac{1}{S}\sum_{s=1}^{S}p(y_{*}|\vY,\vX^{(s)},\vZ^{(s)})%$,
%where $\vX^{(s)}$, $\vZ^{(s)}$ are the $s^{\text{\tiny th}}$ samples from 
%the Gibbs and hybrid Monte Carlo samplers,
%and $S$ is the number of samples.
%Given a sample $\{ \vX^{(s)}, \vZ^{(s)}\}_{s=1}^{S}$,
%we can draw a sample from the posterior $p(y_{*}|\vX,\vY,\vZ)$
%using the following procedure:
%\begin{enumerate}
%\item Draw latent assignment\\
%%$z_{*} \sim {\rm Mult}(N_{1},\cdots,N_{C},\eta)$
%$z_{*} \sim {\rm Mult}(\frac{N_{1}}{N+\eta},\cdots,\frac{N_{C}}{N+\eta},\frac{\%eta}{N+\eta})$
%\item Draw precision matrix\\
%$\vR_{*} \sim {\cal W}(\vS^{-1}_{z_{*}},\nu_{z_{*}})$
%\item Draw mean\\
%$\bm{\mu}_{*} \sim {\cal N}(\vu_{z_{*}},(r_{z_{*}}\vR_{*})^{-1})$
%\item Draw latent coordinates\\
%$\vx_{*} \sim {\cal N}(\bm{\mu}_{*},\vR_{*}^{-1})$
%\item Draw observation\\
%$\vy_{*} \sim {\cal N}(\vk_{*}^{\top}\vK^{-1}\vY,(k(\vx_{*},\vx_{*})-\vk_{*}^{\%top}\vK^{-1}\vk_{*})\bm{I})$
%\end{enumerate}
%Here, 
%$\vk_{*}=(k(\vx_{*},\vx_{1}),\cdots,k(\vx_{*},\vx_{N}))^{\top}$.
%Because our model only depends on exponential family distributions such as multinomial, Wishart and Gaussian, we can draw samples directly from the posterior.  
Each step of this procedure is exact, and since the observations $\vy_{*}$ are conditionally normally distributed, each one adds a smooth contribution to the empirical Monte Carlo estimate of the posterior density, as opposed to a collection of point masses.  This procedure was used to generate the plots of posterior density in figures \ref{fig:generative}, \ref{fig:warping}, and \ref{fig:posterior}.

\section{Related work}

The GPLVM is effective as a nonlinear latent variable model in a wide variety of applications ~\cite{lawrence2004gaussian,salzmann2008local,lawrence2009non}.
The latent positions $\vX$ in the GPLVM are typically obtained by maximum a posteriori estimation or variational Bayesian inference~\cite{titsias2010bayesian}, placing a single fixed spherical Gaussian prior on $\vx$.
A prior which penalizes a high-dimensional latent space is introduced by ~\cite{geiger2009rank}, in which the latent variables and their intrinsic dimensionality are simultaneously optimized.
The iWMM can also infer the intrinsic dimensionality of nonlinear manifolds:
inferring the Gaussian covariance for each latent cluster allows the variance of irrelevant dimensions to become small.  
Because each latent cluster has a different set of parameters, the effective dimension of each cluster can vary, allowing manifolds of different dimension in the observed space.  This ability is demonstrated in figure \ref{fig:warping}b.

The iWMM can also be viewed as a generalization of the mixture of probabilistic principle component analyzers~\cite{tipping1999mixtures}, or mixture of factor analyzers~\cite{ghahramani2000variational}, where the linear mapping of the mixtures is generalized to a nonlinear mapping by Gaussian processes, and number of components is infinite.% infinite number of components by Dirichlet processes.

There exist non-probabilistic clustering methods which can find clusters with complex shapes, such as spectral clustering~\cite{ng2002spectral} and nonlinear manifold clustering~\cite{cao2006nonlinear,elhamifar2011sparse}.
Spectral clustering finds clusters by first forming a similarity graph, then finding a low-dimensional latent representation using the graph, and finally, clustering the latent coordinates via k-means.
The performance of spectral clustering depends on parameters which are usually set manually, such as the number of clusters, the number of neighbors, and the variance parameter used for constructing the similarity graph.
In contrast, the iWMM infers such parameters automatically.  One of the main advantages of the iWMM over these methods is that there is no need to construct a similarity graph.
%The iWMM can be viewed as a probabilistic generative model for spectral clustering, where the both methods find clusters in a nonlinearly mapped low-dimensional space.  

The kernel Gaussian mixture model~\cite{wang2003kernel} can also find non-Gaussian shaped clusters.
This model estimates a GMM in the implicit high-dimensional feature space defined by the kernel mapping of the observed space.
However, the kernel GMM uses a fixed nonlinear mapping function, with no guarantee that the latent points will be well-modeled by a GMM.  
%and the data points mapped by a fixed function are not guaranteed 
%to be distributed according to a GMM.
In contrast, the iWMM infers the mapping function such that the latent co-ordinates will be well-modeled by a mixture of Gaussians.

%The infinite mixtures of Gaussian process experts~\cite{rasmussen2002infinite}
%uses Dirichlet process and Gaussian process priors for regression problems.
%The mixture of ppca
%ppca \cite{tipping1999probabilistic}
%mixture of ppca \cite{tipping1999mixtures}
%one Gaussian process function

\section{Experimental results}


\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{\warpedfiguresdir/faces2}
\caption[Latent clusters of face images]{A sample from the 2-dimensional latent space when modeling a series of 32x32 face images.  Our model correctly discovers that the data consists of two seperate manifolds, both approximately one-dimensional, which share the same head-turning structure.}
\label{fig:faces}
\end{figure}

\subsection{Clustering Faces}

We first examined our model's ability to model images without pre-processing.  We constructed a dataset consisting of 50 greyscale 32x32 pixel images of two individuals from the UMIST faces dataset \cite{umistfaces}.  Both series of images capture a person turning his head to the right.  Figure \ref{fig:faces} shows a sample from the posterior over the latent coordinates and density model.  The model has recovered three relevant, interpretable features of the dataset.  First, that there are two distinct faces.  Second, that each set of images lies approximately along a smooth one-dimensional manifold.  Third, that the two manifolds share roughly the same structure: the front-facing images of both individuals lie close to one another, as do the side-facing images.

%
\begin{figure*}%[t!]
\centering
{\tabcolsep=0.3em
\begin{tabular}{cccc}
\multicolumn{4}{c}{Observed space} \\
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2_x3_observed_coordinates_epoch5000}}
&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/halfcircles_N100K3_x3_observed_coordinates_epoch5000}}
&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/circles_N50K2_x3_observed_coordinates_epoch5000}}
&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/pinwheel_N50K5_x3_observed_coordinates_epoch5000}}\\
$\uparrow$ & $\uparrow$ & $\uparrow$ & $\uparrow$ \\ 
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2_x_latent_coordinates_epoch5000}}
&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/halfcircles_N100K3_x_latent_coordinates_epoch5000}}
&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/circles_N50K2_x_latent_coordinates_epoch5000}}
&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/pinwheel_N50K5_x_latent_coordinates_epoch5000}}
\\
\multicolumn{4}{c}{Latent space} \\
(a) 2-curve & (b) 3-semi & (c) 2-circle & (d) Pinwheel \\
\end{tabular}}
\caption[Recovering clusters on synthetic data]{
Top row: The observed, unlabeled data points, and the clusters inferred by the iWMM.
Bottom row: Latent coordinates and Gaussian components, shown for a single sample from the posterior.
Each point in the latent space corresponds to a point in the observed space. This figure is best viewed in color.}
\label{fig:warping}
\end{figure*}

\subsection{Synthetic Datasets}
Next, we demonstrate the proposed model on the four synthetic datasets shown in Figure~\ref{fig:warping}.
%The top row of Figure~\ref{fig:warping} shows the two-dimensional observed data points (unlabeled) and the non-Gaussian clusters inferred by the iWMM.
% The bottom row shows the latent coordinates and Gaussian components in the latent space inferred by the iWMM.
% Each plot shows a single sample from the Markov chain.
None of these four datasets can be appropriately clustered by Gaussian mixture models (GMM).
For example, consider the 2-curve data shown in Figure~\ref{fig:warping} (a), where 100 data points lie in one of two curved lines in a two-dimensional observed space.
%as shown at the top row of .
A GMM with two components cannot separate the two curved lines, while a GMM with many components could separate the two lines only by breaking each line into many clusters. 
In contrast, with the iWMM, the two non-Gaussian-shaped clusters in the observed space were represented by two Gaussian-shaped clusters in the latent space, as shown at the bottom row of Figure~\ref{fig:warping} (a).
The iWMM separated the two curved lines by nonlinearly warping two Gaussians from the latent space to the observed space.

Figure \ref{fig:warping} (c) shows an interesting manifold learning challenge: a dataset consisting of two circles.  The outer circle is modeled in the latent space by a Gaussian with effectively one degree of freedom.  This linear topology fits the outer circle in the observed space by bending the two ends until they overlap.  In contrast, the sampler fails to discover the 1D topology of the inner circle, modeling it with a 2D manifold instead.  This example demonstrates that each cluster in the iWMM manifold can have a different effective dimension.

\begin{figure*}[t!]
\centering
{\tabcolsep=0.3em
\begin{tabular}{cccc}
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2all_o_latent_coordinates_epoch1}}&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2all_o_latent_coordinates_epoch500}} & 
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2all_o_latent_coordinates_epoch1800}}&
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2all_o_latent_coordinates_epoch3000}}\\
(a) 1 & (b) 500 & (c) 1800 & (d) 3000 \\
\end{tabular}}
\caption[A visualization of a sampler for the iWMM]{The inferred infinite GMMs over iterations in the two-dimensional latent space with the iWMM using the 2-curve data. Labels indicate the number of iterations of the sampler, and the color of each point represents its ordering in the observed coordinates.}
\label{fig:infer}
\end{figure*}

\subsection{Mixing}

An interesting side-effect of learning the number of latent clusters is that this added flexibility can help the sampler escape local minima, helping the sampler to mix properly.
Figure~\ref{fig:infer} shows the samples of the latent coordinates and clusters of the iWMM over time, when modeling the 2-curve data.
\ref{fig:infer}(a) shows the latent coordinates initialized at the observed coordinates, starting with one latent component.
At the 500th iteration \ref{fig:infer}(b), each curved line is modeled by two components.
At the 1800th iteration \ref{fig:infer}(c), the left curved line is modeled by a single component.
At the 3000th iteration \ref{fig:infer}(d), the right curved line is also modeled by a single component, and the dataset is appropriately clustered.
This configuration was relatively stable, and a similar state was found at the 5000th iteration.
%In this way, the iWMM can find latent coordinates
%by flexibly changing the number of components.


\begin{figure}%[t!]
\centering
\begin{tabular}{cc}
\includegraphics[height=10em,width=10em]{\warpedfiguresdir/result_spiral2_ydistribution}&
\includegraphics[height=10em,width=10em]{\warpedfiguresdir/result_spiral2all_gplvm_ydistribution}
\\
(a) iWMM & (b) iWMM ($C=1$) \\
\end{tabular}
\caption[Comparing density estimates of between the GP-LVM and the iWMM]{The posterior density in the observed space with the 2-curve data inferred by the iWMM (a), and that inferred by the iWMM with one component (b).}
\label{fig:posterior}
\end{figure}

\subsection{Density Estimation}
Figure~\ref{fig:posterior} (a) shows the posterior density in the observed space inferred by the iWMM on the 2-curve data, computed using 1000 samples from the Markov chain.
The two separate manifolds of high density implied by the two curved lines was recovered by the iWMM.  
Note also that the density along the manifold varies with the density of data shown in Figure \ref{fig:warping} (a).  
%The warped densities output by our model are actually a richer representation than simply a set of manifolds, since they also define a density at each point in the observed space.
%The density computed using multiple samples from MCMC is clearer than the density computed from a single sample shown at the bottom of Figure~\ref{fig:warping} (a).
%
This result can be compared to a special case of our model, which uses only a single Gaussian to model the latent coordinates instead of an infinite GMM.
Figure~\ref{fig:posterior} (b) shows that the result of the iWMM with $C=1$,
where posterior is forced to place significant density 
connecting the two clusters.
%;we simply call this the {\it warped model} (WM).%
Figure~\ref{fig:posterior} (b) shows that the single-cluster variant of the iWMM posterior is forced to place significant density connecting the two clusters.


\begin{figure*}%[t!]
\centering
{\tabcolsep=0.3em
\begin{tabular}{cccc}
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2all_o_latent_coordinates}} &
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2all_wm2_o_latent_coordinates}} &
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2_gplvm2_o_latent_coordinates}} &
\fbox{\includegraphics[height=11em,width=11em]{\warpedfiguresdir/spiral2_fixedgaussgplvm_o_latent_coordinates}}\\
(a) iWMM & (b) iWMM ($C=1$) &
(c) GPLVM & (d) BGPLVM \\
\end{tabular}}
\caption[Copmarison of latent coordinate estimates]{The estimated latent coordinates of the 2-curve data by
(a) iWMM, (b) iWMM ($C=1$), (c) GPLVM, and (d) Bayesian GPLVM.}
\label{fig:latent}
\end{figure*}

\subsection{Visualization}
Next, we briefly investigate the potential of the iWMM for visualization.  Figure~\ref{fig:latent} (a) shows the latent coordinates obtained by averaging over 1000 samples from the posterior of the iWMM.
Because rotating the latent coordinates does not change their probability, averaging may not be an adequate way to summarize the posterior.
However, we show this result in order to 
show the characteristics of latent coordinates obtained by the iWMM.
The estimated latent coordinates are clearly separated, and they form two straight lines.
This result indicates that in some cases, the iWMM can recover the topology of the data before it has been warped into a manifold.
For comparison, Figure~\ref{fig:latent} (b) shows the latent coordinates 
estimated by the iWMM when forced to use a single cluster: the latent coordinates lie in two sections of a single straight line.
Figure~\ref{fig:latent} (c) and (d) show the latent coordinates 
estimated by the GPLVM when optimizing or integrating out the latent coordinates, respectively.  
Recall that the iWMM ($C=1$) is a more flexible model than the GPLVM, since the GPLVM enforces a spherical covariance in the latent space.
These methods did not unfold the two curved lines, since the effective dimension of their latent representation is fixed beforehand.
%because in these models, no force to favor forming a low-dimensional manifold.
In contrast, the iWMM effectively formed a low-dimensional representation in the latent space. 

Regardless of the dimension of the latent space, the iWMM will tend to model each cluster with as low-dimensional a Gaussian as possible. 
This is because, if the data in a cluster can be made to lie in a low-dimensional plane, a narrowly-shaped Gaussian will assign the latent coordinates much higher likelihood than a spherical Gaussian.

\begin{table*}[ht!]
\centering
\caption[Datasets used for evaluation of the iWMM]
{The statistics of datasets used for evaluation.}
\label{tab:statistics}
\begin{tabular}{rrrrrrrrr}
\hline
 & 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
\hline
number of samples: $N$ & 100 & 300 & 100 & 250 & 150 & 214 & 178 & 528 \\
observed dimensionality: $D$ & 2 & 2 & 2 & 2 & 4 & 9 & 13 & 10 \\
number of clusters: $C$ & 2 & 3 & 2 & 5 & 3 & 7 & 3 & 11 \\
\hline
\end{tabular}
\end{table*}

\subsection{Clustering Performance}
We more formally evaluated the density estimation and clustering performance of the proposed model using four real datasets: iris, glass, wine and vowel,
 obtained from LIBSVM multi-class datasets \cite{chang2011libsvm}, in addition to the four synthetic datasets shown above: 2-curve, 3-semi, 2-circle and Pinwheel~\cite{adams2009archipelago}.
The statistics of these datasets are summarized in Table~\ref{tab:statistics}.
In each experiment, we show the results of ten-fold cross-validation.
Results in bold are not significantly different from the best performing method in each column according to a paired t-test.
%In each experiment, 10\% of the data was used for testing.
%
%
%We used two evaluation measurements: test point likelihood and Rand index
%for evaluating in terms of density estimation performance
%and clustering performance, respectively.
% the Gaussian-Wishart prior is quite different in these cases, placing more mass on higher-dimensional manifolds when the dimension of the latent space is high.  Placing a hyper-prior allowing the concentration parameter $\eta$ to vary may shrink these differences.

\begin{table*}[ht!]
\centering
\caption[Clustering performance comparison]
{Average Rand index for evaluating clustering performance.}
\label{tab:rand}
\begin{tabular}{lrrrrrrrr}
\hline
 & 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
\hline
iGMM & $0.52$ & $0.79$ & $0.83$ & $0.81$ & $0.78$ & $0.60$ & $0.72$ & $\mathbf{0.76}$ \\
iWMM(Q=2) & $\mathbf{0.86}$ & $\mathbf{0.99}$ & $\mathbf{0.89}$ & $\mathbf{0.94}$ & $\mathbf{0.81}$ & $\mathbf{0.65}$ & $0.65$ & $0.50$ \\
iWMM(Q=D) & $\mathbf{0.86}$ & $\mathbf{0.99}$ & $\mathbf{0.89}$ & $\mathbf{0.94}$ & $0.77$ & $0.62$ & $\mathbf{0.77}$ & $\mathbf{0.76}$ \\
\hline
\end{tabular}
\end{table*}
%
Table \ref{tab:rand} compares the clustering performance of the iWMM with the iGMM, quantified by the Rand index~\cite{rand1971objective}, which measures the correspondence between inferred clusters and true clusters.
The iGMM is another probabilistic generative model commonly used for clustering, which can be seen as a special case of the iWMM in which the Gaussian clusters are not warped.  
These experiments demonstrate the extent to which nonparametric cluster shapes allow a mixture model to recover more meaningful clusters.
%When the latent dimensionality was set to two ($Q=2$), clustering performance was degraded.% did not work well
%because information was lost by reducing the dimensionality.
%The sometimes large differences between performance in the $D = 2$ case and the $D = Q$ case may be attributed to the fact that when the latent dimension is high, it requires many samples from from the latent distribution to produce an accurate estimate of the posterior density at the test locations.  A potential solution to this difficulty in inference may be to use a warping with back-constraints \cite{Lawrence06localdistance}, allowing us to more directly evaluate the density at a given point in the observed space.
%TODO: cite


Table \ref{tab:likelihood} lists average test log likelihood, comparing the proposed models
with kernel density estimation (KDE),
%the manifold Parzen window (MPW)~\cite{vincent2002manifold} and 
and the infinite Gaussian mixture model (iGMM).
In KDE, the kernel width is estimated by maximizing the leave-one-out log densities.
%With WM and iWMM, we set the dimensionality of the latent space
%at that of the observed space $Q=D$ or $Q=2$.
Since the manifold on which the observed data lies can be at most $D$-dimensional, we set the latent dimension $Q$ equal to the observed dimension $D$ in iWMMs.
We also include the $Q=2$ case in an attempt to characterize how much modeling power is lost by forcing the latent representation to be visualizable. 
%We include the $Q=2$ case in order to attempt to quantify the 
%The proposed models achieved higher test log likelihoods than the KDE, MPW
The proposed models achieved high test log likelihoods compared with the KDE and iGMM.
%The iWMM achieved better performance than WM for each latent dimensionality,
%except on the wine dataset.
% This result indicates that it is important to assume an infinite mixture model in the latent space for density estimation.%, and that even a fully Bayesian version of the GPLVM is not necessarily a good density model.

\begin{table*}[ht!]
\centering
\caption[Predictive log-likelihood comparison]
{Average test log likelihood for evaluating density estimation performance.}
\label{tab:likelihood}
\begin{tabular}{lrrrrrrrr}
\hline
& 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
\hline 
KDE & $-2.47$ & $-0.38$ & $-1.92$ & $-1.47$ & $\mathbf{-1.87}$ & $1.26$ & $-2.73$ & $\mathbf{6.06}$ \\
iGMM & $-3.28$ & $-2.26$ & $-2.21$ & $-2.12$ & $-1.91$ & $3.00$ & $\mathbf{-1.87}$ & $-0.67$ \\
iWMM(Q=2) & $\mathbf{-0.90}$ & $\mathbf{-0.18}$ & $\mathbf{-1.02}$ & $\mathbf{-0.79}$ & $\mathbf{-1.88}$ & $\mathbf{5.76}$ & $\mathbf{-1.96}$ & $\mathbf{5.91}$ \\
iWMM(Q=D) & $\mathbf{-0.90}$ & $\mathbf{-0.18}$ & $\mathbf{-1.02}$ & $\mathbf{-0.79}$ & $\mathbf{-1.71}$ & $\mathbf{5.70}$ & $-3.14$ & $-0.35$ \\
\hline
\end{tabular}
\end{table*}

\subsection{Source code}

Code to reproduce all the above experiments is available at \url{http://github.com/duvenaud/warped-mixtures}.

\section{Future work}

%One design decision which may be worth revisiting is the use the same warping for all mixture components.  It would be simple, and typically faster, to use a separate set of GPs to model the warping of each latent cluster.  
The Dirichlet process mixture of Gaussians in the latent space of our model could easily be replaced by a more sophisticated density model, such as a hierarchical Dirichlet process~\cite{teh2006hierarchical}, or a Dirichlet diffusion tree~\cite{neal2003density}.
Another straightforward extension of our model would be making inference more scalable by using sparse Gaussian processes~\cite{quinonero2005unifying,snelson2006sparse} or more advanced hybrid Monte Carlo methods~\cite{zhang2011quasi}.
An interesting but more complex extension of the iWMM would be a semi-supervised version of the model.
The iWMM could allow label propagation along regions of high density in the latent space, even if those regions were stretched along low-dimensional manifolds in the observed space.  Another natural extension would be to allow a separate warping for each cluster, which would also improve inference speed.

% Todo:  Cite paper from Jeff's talk that suggests using DP mixtures to estimate the number of clusters.


%This model is 

%Table \ref{tab:count} shows the inferred number of components
%by the iGMM and iWMM.
%With the synthetic data sets, 
%the number of components inferred by the iWMM was 
%
%\begin{table*}[t!]
%\centering
%\caption{Average inferred number of components.}
%\label{tab:count}
%\begin{tabular}{lrrrrrrrr}
%\hline
% & 2-curve & 3-semi & 2-circle & Pinwheel & Iris  & Glass  & Wine  & Vowel  \\
%\hline
%iGMM & 2.7 & 4.5 & 3.8 & 4.2 & 2.0 & 2.3 & 2.7 & 5.0 \\ 
%iWMM($Q=2$) & 2.2 & 3.0 & 3.2 & 4.6 & 2.6 & 3.7 & 3.1 & 2.8 \\
%iWMM($Q=D$) & 2.2 & 3.0 & 3.2 & 4.6 & 2.0 & 2.8 & 5.9 & 5.1 \\ 
%\hline
%true & 2 & 3 & 2 & 5 & 3 & 7 & 3 & 11 \\
%\hline
%\end{tabular}
%\end{table*}
%

\section{Conclusion}

In this paper, we introduced a simple generative model of non-Gaussian density manifolds which can infer nonlinearly separable clusters, low-dimensional representations of varying dimension per cluster, and density estimates which smoothly follow data contours.  We then introduced an efficient sampler for this model which integrates out both the cluster parameters and the warping function exactly.  We further demonstrated that allowing non-parametric cluster shapes improves clustering performance over the Dirichlet process Mixture of Gaussians.

Many methods have been proposed which can perform some combination of clustering, manifold learning, density estimation and visualization.
We demonstrated that a simple but flexible probabilistic generative model can perform well at all these tasks.

%In contrast to the many somewhat ad-hoc clustering, manifold learning, density estimation, and visualization methods introduced recently, we show that a simple generative model can perform well at all of these tasks.

%We introduced a nonparametric Bayesian clustering method capable of inferring nonlinearly separable clusters.
%In the experiments, we demonstrated that our model is effective for density estimation, and performs much better than infinite Gaussian mixture models at discovering meaningful clusters.


\subsection*{Acknowledgements}
The authors would like to thank Dominique Perrault-Joncas, Carl Edward Rasmussen, and Ryan Prescott Adams for helpful discussions.
%\pagebreak


\outbpdocument{
\bibliographystyle{plainnat}
\bibliography{references.bib}
}


